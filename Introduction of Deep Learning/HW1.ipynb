{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# Numerical Operations\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# Reading/Writing Data\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "\n",
    "# For Progress Bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "# For plotting learning curve\n",
    "from tensorboardX import SummaryWriter\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def same_seed(seed):\n",
    "    '''Fixes random number generator seeds for reproducibility.'''\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def train_valid_split(data_set, valid_ratio, seed):\n",
    "    '''Split provided training data into training set and validation set'''\n",
    "    valid_set_size = int(valid_ratio * len(data_set))\n",
    "    train_set_size = len(data_set) - valid_set_size\n",
    "    train_set, valid_set = random_split(data_set, [train_set_size, valid_set_size], generator=torch.Generator().manual_seed(seed))\n",
    "    return np.array(train_set), np.array(valid_set)\n",
    "\n",
    "def predict(test_loader, model, device):\n",
    "    model.eval() # Set your model to evaluation mode.\n",
    "    preds = []\n",
    "    for x in tqdm(test_loader):\n",
    "        x = x.to(device)\n",
    "        with torch.no_grad():\n",
    "            pred = model(x)\n",
    "            preds.append(pred.detach().cpu())\n",
    "    preds = torch.cat(preds, dim=0).numpy()\n",
    "    return preds\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class COVID19Dataset(Dataset):\n",
    "    '''\n",
    "    x: Features.\n",
    "    y: Targets, if none, do prediction.\n",
    "    '''\n",
    "    def __init__(self, x, y=None):\n",
    "        if y is None:\n",
    "            self.y = y\n",
    "        else:\n",
    "            self.y = torch.FloatTensor(y)\n",
    "        self.x = torch.FloatTensor(x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.y is None:\n",
    "            return self.x[idx]\n",
    "        else:\n",
    "            return self.x[idx], self.y[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class My_Model(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(My_Model, self).__init__()\n",
    "        # TODO: modify model's structure, be aware of dimensions.\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        x = x.squeeze(1) # (B, 1) -> (B)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "\n",
    "def select_feat(train_data, valid_data, test_data, select_all=True):\n",
    "    '''Selects useful features to perform regression'''\n",
    "    y_train, y_valid = train_data[:,-1], valid_data[:,-1]\n",
    "    raw_x_train, raw_x_valid, raw_x_test = train_data[:,:-1], valid_data[:,:-1], test_data\n",
    "\n",
    "    if select_all:\n",
    "        feat_idx = list(range(raw_x_train.shape[1]))\n",
    "    else:\n",
    "        feat_idx = [0,1,2,3,4] # TODO: Select suitable feature columns.\n",
    "\n",
    "    return raw_x_train[:,feat_idx], raw_x_valid[:,feat_idx], raw_x_test[:,feat_idx], y_train, y_valid"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def trainer(train_loader, valid_loader, model, config, device):\n",
    "\n",
    "    criterion = nn.MSELoss(reduction='mean') # Define your loss function, do not modify this.\n",
    "\n",
    "    # Define your optimization algorithm.\n",
    "    # TODO: Please check https://pytorch.org/docs/stable/optim.html to get more available algorithms.\n",
    "    # TODO: L2 regularization (optimizer(weight decay...) or implement by your self).\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=config['learning_rate'], momentum=0.9)\n",
    "\n",
    "    writer = SummaryWriter() # Writer of tensoboard.\n",
    "\n",
    "    if not os.path.isdir('./models'):\n",
    "        os.mkdir('./models') # Create directory of saving models.\n",
    "\n",
    "    n_epochs, best_loss, step, early_stop_count = config['n_epochs'], math.inf, 0, 0\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train() # Set your model to train mode.\n",
    "        loss_record = []\n",
    "\n",
    "        # tqdm is a package to visualize your training progress.\n",
    "        train_pbar = tqdm(train_loader, position=0, leave=True)\n",
    "\n",
    "        for x, y in train_pbar:\n",
    "            optimizer.zero_grad()               # Set gradient to zero.\n",
    "            x, y = x.to(device), y.to(device)   # Move your data to device.\n",
    "            pred = model(x)\n",
    "            loss = criterion(pred, y)\n",
    "            loss.backward()                     # Compute gradient(backpropagation).\n",
    "            optimizer.step()                    # Update parameters.\n",
    "            step += 1\n",
    "            loss_record.append(loss.detach().item())\n",
    "\n",
    "            # Display current epoch number and loss on tqdm progress bar.\n",
    "            train_pbar.set_description(f'Epoch [{epoch+1}/{n_epochs}]')\n",
    "            train_pbar.set_postfix({'loss': loss.detach().item()})\n",
    "\n",
    "        mean_train_loss = sum(loss_record)/len(loss_record)\n",
    "        writer.add_scalar('Loss/train', mean_train_loss, step)\n",
    "\n",
    "        model.eval() # Set your model to evaluation mode.\n",
    "        loss_record = []\n",
    "        for x, y in valid_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            with torch.no_grad():\n",
    "                pred = model(x)\n",
    "                loss = criterion(pred, y)\n",
    "\n",
    "            loss_record.append(loss.item())\n",
    "\n",
    "        mean_valid_loss = sum(loss_record)/len(loss_record)\n",
    "        print(f'Epoch [{epoch+1}/{n_epochs}]: Train loss: {mean_train_loss:.4f}, Valid loss: {mean_valid_loss:.4f}')\n",
    "        writer.add_scalar('Loss/valid', mean_valid_loss, step)\n",
    "\n",
    "        if mean_valid_loss < best_loss:\n",
    "            best_loss = mean_valid_loss\n",
    "            torch.save(model.state_dict(), config['save_path']) # Save your best model\n",
    "            print('Saving model with loss {:.3f}...'.format(best_loss))\n",
    "            early_stop_count = 0\n",
    "        else:\n",
    "            early_stop_count += 1\n",
    "\n",
    "        if early_stop_count >= config['early_stop']:\n",
    "            print('\\nModel is not improving, so we halt the training session.')\n",
    "            return"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "config = {\n",
    "    'seed': 5201314,      # Your seed number, you can pick your lucky number. :)\n",
    "    'select_all': True,   # Whether to use all features.\n",
    "    'valid_ratio': 0.2,   # validation_size = train_size * valid_ratio\n",
    "    'n_epochs': 3000,     # Number of epochs.\n",
    "    'batch_size': 256,\n",
    "    'learning_rate': 1e-5,\n",
    "    'early_stop': 400,    # If model has not improved for this many consecutive epochs, stop training.\n",
    "    'save_path': './models/model.ckpt'  # Your model will be saved here.\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data size: (2160, 118)\n",
      "valid_data size: (539, 118)\n",
      "test_data size: (1078, 117)\n",
      "number of features: 117\n"
     ]
    }
   ],
   "source": [
    "# Set seed for reproducibility\n",
    "same_seed(config['seed'])\n",
    "\n",
    "\n",
    "# train_data size: 2699 x 118 (id + 37 states + 16 features x 5 days)\n",
    "# test_data size: 1078 x 117 (without last day's positive rate)\n",
    "train_data, test_data = pd.read_csv('./covid.train.csv').values, pd.read_csv('./covid.test.csv').values\n",
    "train_data, valid_data = train_valid_split(train_data, config['valid_ratio'], config['seed'])\n",
    "\n",
    "# Print out the data size.\n",
    "print(f\"\"\"train_data size: {train_data.shape}\n",
    "valid_data size: {valid_data.shape}\n",
    "test_data size: {test_data.shape}\"\"\")\n",
    "\n",
    "# Select features\n",
    "x_train, x_valid, x_test, y_train, y_valid = select_feat(train_data, valid_data, test_data, config['select_all'])\n",
    "\n",
    "# Print out the number of features.\n",
    "print(f'number of features: {x_train.shape[1]}')\n",
    "\n",
    "train_dataset, valid_dataset, test_dataset = COVID19Dataset(x_train, y_train), \\\n",
    "                                            COVID19Dataset(x_valid, y_valid), \\\n",
    "                                            COVID19Dataset(x_test)\n",
    "\n",
    "# Pytorch data loader loads pytorch dataset into batches.\n",
    "train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, pin_memory=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=config['batch_size'], shuffle=True, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False, pin_memory=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/3000]: 100%|██████████| 9/9 [00:00<00:00,  9.58it/s, loss=123] \n",
      "Epoch [2/3000]: 100%|██████████| 9/9 [00:00<00:00, 120.32it/s, loss=72.5]\n",
      "Epoch [3/3000]: 100%|██████████| 9/9 [00:00<00:00, 121.94it/s, loss=49.5]\n",
      "Epoch [4/3000]: 100%|██████████| 9/9 [00:00<00:00, 128.92it/s, loss=50.6]\n",
      "Epoch [5/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=43.9]\n",
      "Epoch [6/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=44.2]\n",
      "Epoch [7/3000]: 100%|██████████| 9/9 [00:00<00:00, 170.27it/s, loss=56.3]\n",
      "Epoch [8/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=54.6]\n",
      "Epoch [9/3000]: 100%|██████████| 9/9 [00:00<00:00, 158.32it/s, loss=44.6]\n",
      "Epoch [10/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=53.3]\n",
      "Epoch [11/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=36.7]\n",
      "Epoch [12/3000]: 100%|██████████| 9/9 [00:00<00:00, 170.26it/s, loss=46.5]\n",
      "Epoch [13/3000]: 100%|██████████| 9/9 [00:00<00:00, 214.85it/s, loss=37.3]\n",
      "Epoch [14/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.17it/s, loss=46.4]\n",
      "Epoch [15/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=40.2]\n",
      "Epoch [16/3000]: 100%|██████████| 9/9 [00:00<00:00, 112.74it/s, loss=40.4]\n",
      "Epoch [17/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.94it/s, loss=44.6]\n",
      "Epoch [18/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=36.3]\n",
      "Epoch [19/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=31.8]\n",
      "Epoch [20/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.54it/s, loss=30.1]\n",
      "Epoch [21/3000]: 100%|██████████| 9/9 [00:00<00:00, 164.08it/s, loss=27.6]\n",
      "Epoch [22/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=31.6]\n",
      "Epoch [23/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.95it/s, loss=32.4]\n",
      "Epoch [24/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=31.6]\n",
      "Epoch [25/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.94it/s, loss=29.5]\n",
      "Epoch [26/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=30.9]\n",
      "Epoch [27/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.17it/s, loss=32.8]\n",
      "Epoch [28/3000]: 100%|██████████| 9/9 [00:00<00:00, 170.26it/s, loss=30.6]\n",
      "Epoch [29/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=23.2]\n",
      "Epoch [30/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.95it/s, loss=27.6]\n",
      "Epoch [31/3000]: 100%|██████████| 9/9 [00:00<00:00, 167.11it/s, loss=20.3]\n",
      "Epoch [32/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=28.2]\n",
      "Epoch [33/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=32.6]\n",
      "Epoch [34/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=18.4]\n",
      "Epoch [35/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=29.3]\n",
      "Epoch [36/3000]: 100%|██████████| 9/9 [00:00<00:00, 164.08it/s, loss=25.9]\n",
      "Epoch [37/3000]: 100%|██████████| 9/9 [00:00<00:00, 209.86it/s, loss=24.4]\n",
      "Epoch [38/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.01it/s, loss=27]\n",
      "Epoch [39/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.53it/s, loss=19.4]\n",
      "Epoch [40/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=19.3]\n",
      "Epoch [41/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=17.4]\n",
      "Epoch [42/3000]: 100%|██████████| 9/9 [00:00<00:00, 209.86it/s, loss=13.8]\n",
      "Epoch [43/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.54it/s, loss=12.9]\n",
      "Epoch [44/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.54it/s, loss=11.3]\n",
      "Epoch [45/3000]: 100%|██████████| 9/9 [00:00<00:00, 170.26it/s, loss=118]\n",
      "Epoch [46/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=120]\n",
      "Epoch [47/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=137]\n",
      "Epoch [48/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=151]\n",
      "Epoch [49/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=130]\n",
      "Epoch [50/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.01it/s, loss=127]\n",
      "Epoch [51/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=120]\n",
      "Epoch [52/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.17it/s, loss=107]\n",
      "Epoch [53/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.54it/s, loss=61.6]\n",
      "Epoch [54/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=20.6]\n",
      "Epoch [55/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=26.2]\n",
      "Epoch [56/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=22.2]\n",
      "Epoch [57/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=9.67]\n",
      "Epoch [58/3000]: 100%|██████████| 9/9 [00:00<00:00, 111.41it/s, loss=11.4]\n",
      "Epoch [59/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.17it/s, loss=15.2]\n",
      "Epoch [60/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=9.49]\n",
      "Epoch [61/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.94it/s, loss=12.1]\n",
      "Epoch [62/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=6.88]\n",
      "Epoch [63/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.94it/s, loss=9.67]\n",
      "Epoch [64/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.94it/s, loss=7.66]\n",
      "Epoch [65/3000]: 100%|██████████| 9/9 [00:00<00:00, 167.11it/s, loss=11.1]\n",
      "Epoch [66/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.52it/s, loss=6.16]\n",
      "Epoch [67/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=18.3]\n",
      "Epoch [68/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=10.9]\n",
      "Epoch [69/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=6.24]\n",
      "Epoch [70/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.54it/s, loss=9.63]\n",
      "Epoch [71/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=6.8]\n",
      "Epoch [72/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.94it/s, loss=6.22]\n",
      "Epoch [73/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.54it/s, loss=5.67]\n",
      "Epoch [74/3000]: 100%|██████████| 9/9 [00:00<00:00, 167.11it/s, loss=8.17]\n",
      "Epoch [75/3000]: 100%|██████████| 9/9 [00:00<00:00, 161.14it/s, loss=6.95]\n",
      "Epoch [76/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=5.81]\n",
      "Epoch [77/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=9]\n",
      "Epoch [78/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=6.04]\n",
      "Epoch [79/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=4.57]\n",
      "Epoch [80/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.54it/s, loss=6.59]\n",
      "Epoch [81/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.94it/s, loss=7.23]\n",
      "Epoch [82/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.17it/s, loss=8.33]\n",
      "Epoch [83/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=6.15]\n",
      "Epoch [84/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=8.25]\n",
      "Epoch [85/3000]: 100%|██████████| 9/9 [00:00<00:00, 209.86it/s, loss=8.36]\n",
      "Epoch [86/3000]: 100%|██████████| 9/9 [00:00<00:00, 214.86it/s, loss=6.02]\n",
      "Epoch [87/3000]: 100%|██████████| 9/9 [00:00<00:00, 237.48it/s, loss=10.3]\n",
      "Epoch [88/3000]: 100%|██████████| 9/9 [00:00<00:00, 225.60it/s, loss=4.81]\n",
      "Epoch [89/3000]: 100%|██████████| 9/9 [00:00<00:00, 237.47it/s, loss=5.6]\n",
      "Epoch [90/3000]: 100%|██████████| 9/9 [00:00<00:00, 231.38it/s, loss=5.59]\n",
      "Epoch [91/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.17it/s, loss=13.5]\n",
      "Epoch [92/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=6.76]\n",
      "Epoch [93/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=5.87]\n",
      "Epoch [94/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.09it/s, loss=8.8]\n",
      "Epoch [95/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=5.7]\n",
      "Epoch [96/3000]: 100%|██████████| 9/9 [00:00<00:00, 214.86it/s, loss=5.68]\n",
      "Epoch [97/3000]: 100%|██████████| 9/9 [00:00<00:00, 220.10it/s, loss=5.78]\n",
      "Epoch [98/3000]: 100%|██████████| 9/9 [00:00<00:00, 231.39it/s, loss=5.03]\n",
      "Epoch [99/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=6.33]\n",
      "Epoch [100/3000]: 100%|██████████| 9/9 [00:00<00:00, 118.74it/s, loss=6.15]\n",
      "Epoch [101/3000]: 100%|██████████| 9/9 [00:00<00:00, 220.10it/s, loss=6.94]\n",
      "Epoch [102/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=6.22]\n",
      "Epoch [103/3000]: 100%|██████████| 9/9 [00:00<00:00, 214.86it/s, loss=7.8]\n",
      "Epoch [104/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=5.97]\n",
      "Epoch [105/3000]: 100%|██████████| 9/9 [00:00<00:00, 214.86it/s, loss=4.52]\n",
      "Epoch [106/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.09it/s, loss=5.99]\n",
      "Epoch [107/3000]: 100%|██████████| 9/9 [00:00<00:00, 237.47it/s, loss=5.36]\n",
      "Epoch [108/3000]: 100%|██████████| 9/9 [00:00<00:00, 231.38it/s, loss=4.68]\n",
      "Epoch [109/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=4.77]\n",
      "Epoch [110/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=16]\n",
      "Epoch [111/3000]: 100%|██████████| 9/9 [00:00<00:00, 225.60it/s, loss=9.94]\n",
      "Epoch [112/3000]: 100%|██████████| 9/9 [00:00<00:00, 225.60it/s, loss=6.62]\n",
      "Epoch [113/3000]: 100%|██████████| 9/9 [00:00<00:00, 225.60it/s, loss=5.43]\n",
      "Epoch [114/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.09it/s, loss=6.3]\n",
      "Epoch [115/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.53it/s, loss=5.94]\n",
      "Epoch [116/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=6.63]\n",
      "Epoch [117/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=4.84]\n",
      "Epoch [118/3000]: 100%|██████████| 9/9 [00:00<00:00, 214.85it/s, loss=4.9]\n",
      "Epoch [119/3000]: 100%|██████████| 9/9 [00:00<00:00, 220.10it/s, loss=4.39]\n",
      "Epoch [120/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=4.51]\n",
      "Epoch [121/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.17it/s, loss=10.4]\n",
      "Epoch [122/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=5.64]\n",
      "Epoch [123/3000]: 100%|██████████| 9/9 [00:00<00:00, 209.86it/s, loss=4.95]\n",
      "Epoch [124/3000]: 100%|██████████| 9/9 [00:00<00:00, 170.27it/s, loss=5.62]\n",
      "Epoch [125/3000]: 100%|██████████| 9/9 [00:00<00:00, 225.61it/s, loss=8.78]\n",
      "Epoch [126/3000]: 100%|██████████| 9/9 [00:00<00:00, 158.32it/s, loss=4.92]\n",
      "Epoch [127/3000]: 100%|██████████| 9/9 [00:00<00:00, 170.27it/s, loss=5.16]\n",
      "Epoch [128/3000]: 100%|██████████| 9/9 [00:00<00:00, 170.25it/s, loss=6.43]\n",
      "Epoch [129/3000]: 100%|██████████| 9/9 [00:00<00:00, 161.14it/s, loss=5.54]\n",
      "Epoch [130/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.54it/s, loss=4.64]\n",
      "Epoch [131/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.47it/s, loss=5.36]\n",
      "Epoch [132/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.53it/s, loss=5.05]\n",
      "Epoch [133/3000]: 100%|██████████| 9/9 [00:00<00:00, 170.26it/s, loss=6.37]\n",
      "Epoch [134/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=4.02]\n",
      "Epoch [135/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.09it/s, loss=5.68]\n",
      "Epoch [136/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.54it/s, loss=4.94]\n",
      "Epoch [137/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.54it/s, loss=4.83]\n",
      "Epoch [138/3000]: 100%|██████████| 9/9 [00:00<00:00, 214.86it/s, loss=5.85]\n",
      "Epoch [139/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.54it/s, loss=5.25]\n",
      "Epoch [140/3000]: 100%|██████████| 9/9 [00:00<00:00, 237.47it/s, loss=6.88]\n",
      "Epoch [141/3000]: 100%|██████████| 9/9 [00:00<00:00, 147.94it/s, loss=5.86]\n",
      "Epoch [142/3000]: 100%|██████████| 9/9 [00:00<00:00, 94.99it/s, loss=8.13]\n",
      "Epoch [143/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.54it/s, loss=6.26]\n",
      "Epoch [144/3000]: 100%|██████████| 9/9 [00:00<00:00, 209.86it/s, loss=8.03]\n",
      "Epoch [145/3000]: 100%|██████████| 9/9 [00:00<00:00, 214.90it/s, loss=5.59]\n",
      "Epoch [146/3000]: 100%|██████████| 9/9 [00:00<00:00, 167.11it/s, loss=6.44]\n",
      "Epoch [147/3000]: 100%|██████████| 9/9 [00:00<00:00, 146.36it/s, loss=4.71]\n",
      "Epoch [148/3000]: 100%|██████████| 9/9 [00:00<00:00, 177.05it/s, loss=4.85]\n",
      "Epoch [149/3000]: 100%|██████████| 9/9 [00:00<00:00, 179.57it/s, loss=4.29]\n",
      "Epoch [150/3000]: 100%|██████████| 9/9 [00:00<00:00, 210.56it/s, loss=5.15]\n",
      "Epoch [151/3000]: 100%|██████████| 9/9 [00:00<00:00, 167.68it/s, loss=3.96]\n",
      "Epoch [152/3000]: 100%|██████████| 9/9 [00:00<00:00, 172.37it/s, loss=5.26]\n",
      "Epoch [153/3000]: 100%|██████████| 9/9 [00:00<00:00, 169.49it/s, loss=4.4]\n",
      "Epoch [154/3000]: 100%|██████████| 9/9 [00:00<00:00, 174.43it/s, loss=7.06]\n",
      "Epoch [155/3000]: 100%|██████████| 9/9 [00:00<00:00, 212.88it/s, loss=5.72]\n",
      "Epoch [156/3000]: 100%|██████████| 9/9 [00:00<00:00, 154.95it/s, loss=4.37]\n",
      "Epoch [157/3000]: 100%|██████████| 9/9 [00:00<00:00, 170.26it/s, loss=5.3]\n",
      "Epoch [158/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.94it/s, loss=5.11]\n",
      "Epoch [159/3000]: 100%|██████████| 9/9 [00:00<00:00, 170.26it/s, loss=5.18]\n",
      "Epoch [160/3000]: 100%|██████████| 9/9 [00:00<00:00, 164.08it/s, loss=5.37]\n",
      "Epoch [161/3000]: 100%|██████████| 9/9 [00:00<00:00, 155.59it/s, loss=4.66]\n",
      "Epoch [162/3000]: 100%|██████████| 9/9 [00:00<00:00, 167.12it/s, loss=5.14]\n",
      "Epoch [163/3000]: 100%|██████████| 9/9 [00:00<00:00, 145.55it/s, loss=4.87]\n",
      "Epoch [164/3000]: 100%|██████████| 9/9 [00:00<00:00, 170.26it/s, loss=6.21]\n",
      "Epoch [165/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=4.74]\n",
      "Epoch [166/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=9.01]\n",
      "Epoch [167/3000]: 100%|██████████| 9/9 [00:00<00:00, 214.80it/s, loss=4.33]\n",
      "Epoch [168/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=4.15]\n",
      "Epoch [169/3000]: 100%|██████████| 9/9 [00:00<00:00, 225.60it/s, loss=6.18]\n",
      "Epoch [170/3000]: 100%|██████████| 9/9 [00:00<00:00, 167.11it/s, loss=3.66]\n",
      "Epoch [171/3000]: 100%|██████████| 9/9 [00:00<00:00, 220.10it/s, loss=4.68]\n",
      "Epoch [172/3000]: 100%|██████████| 9/9 [00:00<00:00, 209.86it/s, loss=5.24]\n",
      "Epoch [173/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=3.94]\n",
      "Epoch [174/3000]: 100%|██████████| 9/9 [00:00<00:00, 237.48it/s, loss=5.49]\n",
      "Epoch [175/3000]: 100%|██████████| 9/9 [00:00<00:00, 220.02it/s, loss=5.25]\n",
      "Epoch [176/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=4.05]\n",
      "Epoch [177/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.09it/s, loss=5.5]\n",
      "Epoch [178/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.53it/s, loss=3.55]\n",
      "Epoch [179/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.56it/s, loss=6.25]\n",
      "Epoch [180/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.94it/s, loss=5.31]\n",
      "Epoch [181/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=4.47]\n",
      "Epoch [182/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=5.5]\n",
      "Epoch [183/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=8.13]\n",
      "Epoch [184/3000]: 100%|██████████| 9/9 [00:00<00:00, 112.80it/s, loss=4.54]\n",
      "Epoch [185/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=4.95]\n",
      "Epoch [186/3000]: 100%|██████████| 9/9 [00:00<00:00, 167.11it/s, loss=5.03]\n",
      "Epoch [187/3000]: 100%|██████████| 9/9 [00:00<00:00, 164.08it/s, loss=5.48]\n",
      "Epoch [188/3000]: 100%|██████████| 9/9 [00:00<00:00, 220.10it/s, loss=4.43]\n",
      "Epoch [189/3000]: 100%|██████████| 9/9 [00:00<00:00, 167.11it/s, loss=4.4]\n",
      "Epoch [190/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.10it/s, loss=5.29]\n",
      "Epoch [191/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=6.26]\n",
      "Epoch [192/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=5.48]\n",
      "Epoch [193/3000]: 100%|██████████| 9/9 [00:00<00:00, 209.86it/s, loss=5.16]\n",
      "Epoch [194/3000]: 100%|██████████| 9/9 [00:00<00:00, 209.86it/s, loss=3.86]\n",
      "Epoch [195/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=3.77]\n",
      "Epoch [196/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.53it/s, loss=6.45]\n",
      "Epoch [197/3000]: 100%|██████████| 9/9 [00:00<00:00, 161.14it/s, loss=4.56]\n",
      "Epoch [198/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=4.09]\n",
      "Epoch [199/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=4.62]\n",
      "Epoch [200/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=4.54]\n",
      "Epoch [201/3000]: 100%|██████████| 9/9 [00:00<00:00, 209.87it/s, loss=4.8]\n",
      "Epoch [202/3000]: 100%|██████████| 9/9 [00:00<00:00, 225.60it/s, loss=4.14]\n",
      "Epoch [203/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=3.6]\n",
      "Epoch [204/3000]: 100%|██████████| 9/9 [00:00<00:00, 237.48it/s, loss=3.51]\n",
      "Epoch [205/3000]: 100%|██████████| 9/9 [00:00<00:00, 214.86it/s, loss=4.03]\n",
      "Epoch [206/3000]: 100%|██████████| 9/9 [00:00<00:00, 225.61it/s, loss=4.23]\n",
      "Epoch [207/3000]: 100%|██████████| 9/9 [00:00<00:00, 225.60it/s, loss=5.51]\n",
      "Epoch [208/3000]: 100%|██████████| 9/9 [00:00<00:00, 170.26it/s, loss=5.39]\n",
      "Epoch [209/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=6.3]\n",
      "Epoch [210/3000]: 100%|██████████| 9/9 [00:00<00:00, 136.73it/s, loss=3.9]\n",
      "Epoch [211/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.94it/s, loss=4.5]\n",
      "Epoch [212/3000]: 100%|██████████| 9/9 [00:00<00:00, 147.94it/s, loss=5.75]\n",
      "Epoch [213/3000]: 100%|██████████| 9/9 [00:00<00:00, 147.94it/s, loss=6.38]\n",
      "Epoch [214/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.09it/s, loss=6.83]\n",
      "Epoch [215/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=5.6]\n",
      "Epoch [216/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.54it/s, loss=4.85]\n",
      "Epoch [217/3000]: 100%|██████████| 9/9 [00:00<00:00, 170.26it/s, loss=5.97]\n",
      "Epoch [218/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.54it/s, loss=5.63]\n",
      "Epoch [219/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=5.78]\n",
      "Epoch [220/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.09it/s, loss=3.51]\n",
      "Epoch [221/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=5.33]\n",
      "Epoch [222/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.95it/s, loss=6.54]\n",
      "Epoch [223/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.54it/s, loss=4.68]\n",
      "Epoch [224/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=3.81]\n",
      "Epoch [225/3000]: 100%|██████████| 9/9 [00:00<00:00, 209.86it/s, loss=3.87]\n",
      "Epoch [226/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=3.17]\n",
      "Epoch [227/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.17it/s, loss=4.41]\n",
      "Epoch [228/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.94it/s, loss=4.67]\n",
      "Epoch [229/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=6.87]\n",
      "Epoch [230/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=5.21]\n",
      "Epoch [231/3000]: 100%|██████████| 9/9 [00:00<00:00, 237.48it/s, loss=4.96]\n",
      "Epoch [232/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.49it/s, loss=3.81]\n",
      "Epoch [233/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=5.05]\n",
      "Epoch [234/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.09it/s, loss=2.86]\n",
      "Epoch [235/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=4.05]\n",
      "Epoch [236/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.09it/s, loss=5.88]\n",
      "Epoch [237/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=4.47]\n",
      "Epoch [238/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=4.1]\n",
      "Epoch [239/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.09it/s, loss=4.25]\n",
      "Epoch [240/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.09it/s, loss=4.48]\n",
      "Epoch [241/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.09it/s, loss=4.69]\n",
      "Epoch [242/3000]: 100%|██████████| 9/9 [00:00<00:00, 209.87it/s, loss=4.3]\n",
      "Epoch [243/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=4.84]\n",
      "Epoch [244/3000]: 100%|██████████| 9/9 [00:00<00:00, 231.39it/s, loss=5.18]\n",
      "Epoch [245/3000]: 100%|██████████| 9/9 [00:00<00:00, 220.10it/s, loss=5.24]\n",
      "Epoch [246/3000]: 100%|██████████| 9/9 [00:00<00:00, 225.60it/s, loss=3.44]\n",
      "Epoch [247/3000]: 100%|██████████| 9/9 [00:00<00:00, 231.39it/s, loss=3.93]\n",
      "Epoch [248/3000]: 100%|██████████| 9/9 [00:00<00:00, 209.86it/s, loss=4.13]\n",
      "Epoch [249/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.53it/s, loss=4.4]\n",
      "Epoch [250/3000]: 100%|██████████| 9/9 [00:00<00:00, 158.32it/s, loss=4.51]\n",
      "Epoch [251/3000]: 100%|██████████| 9/9 [00:00<00:00, 143.24it/s, loss=4.75]\n",
      "Epoch [252/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=4.32]\n",
      "Epoch [253/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=4.72]\n",
      "Epoch [254/3000]: 100%|██████████| 9/9 [00:00<00:00, 209.86it/s, loss=5.28]\n",
      "Epoch [255/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.16it/s, loss=4.29]\n",
      "Epoch [256/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=4.17]\n",
      "Epoch [257/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.94it/s, loss=3.38]\n",
      "Epoch [258/3000]: 100%|██████████| 9/9 [00:00<00:00, 214.86it/s, loss=3.84]\n",
      "Epoch [259/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.53it/s, loss=3.03]\n",
      "Epoch [260/3000]: 100%|██████████| 9/9 [00:00<00:00, 250.68it/s, loss=4.35]\n",
      "Epoch [261/3000]: 100%|██████████| 9/9 [00:00<00:00, 209.86it/s, loss=4.32]\n",
      "Epoch [262/3000]: 100%|██████████| 9/9 [00:00<00:00, 237.47it/s, loss=5.49]\n",
      "Epoch [263/3000]: 100%|██████████| 9/9 [00:00<00:00, 220.10it/s, loss=4.98]\n",
      "Epoch [264/3000]: 100%|██████████| 9/9 [00:00<00:00, 231.38it/s, loss=3.4]\n",
      "Epoch [265/3000]: 100%|██████████| 9/9 [00:00<00:00, 231.39it/s, loss=4.13]\n",
      "Epoch [266/3000]: 100%|██████████| 9/9 [00:00<00:00, 209.86it/s, loss=3.76]\n",
      "Epoch [267/3000]: 100%|██████████| 9/9 [00:00<00:00, 237.48it/s, loss=3.43]\n",
      "Epoch [268/3000]: 100%|██████████| 9/9 [00:00<00:00, 220.11it/s, loss=4.52]\n",
      "Epoch [269/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=3.78]\n",
      "Epoch [270/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.53it/s, loss=2.16]\n",
      "Epoch [271/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.09it/s, loss=2.91]\n",
      "Epoch [272/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=3.56]\n",
      "Epoch [273/3000]: 100%|██████████| 9/9 [00:00<00:00, 231.39it/s, loss=4.2]\n",
      "Epoch [274/3000]: 100%|██████████| 9/9 [00:00<00:00, 209.86it/s, loss=3.96]\n",
      "Epoch [275/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.09it/s, loss=3.86]\n",
      "Epoch [276/3000]: 100%|██████████| 9/9 [00:00<00:00, 237.48it/s, loss=4.19]\n",
      "Epoch [277/3000]: 100%|██████████| 9/9 [00:00<00:00, 257.83it/s, loss=2.91]\n",
      "Epoch [278/3000]: 100%|██████████| 9/9 [00:00<00:00, 225.60it/s, loss=3.2]\n",
      "Epoch [279/3000]: 100%|██████████| 9/9 [00:00<00:00, 231.39it/s, loss=4.76]\n",
      "Epoch [280/3000]: 100%|██████████| 9/9 [00:00<00:00, 209.95it/s, loss=4.91]\n",
      "Epoch [281/3000]: 100%|██████████| 9/9 [00:00<00:00, 225.60it/s, loss=4.32]\n",
      "Epoch [282/3000]: 100%|██████████| 9/9 [00:00<00:00, 220.10it/s, loss=3.75]\n",
      "Epoch [283/3000]: 100%|██████████| 9/9 [00:00<00:00, 225.60it/s, loss=7.23]\n",
      "Epoch [284/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.10it/s, loss=8.97]\n",
      "Epoch [285/3000]: 100%|██████████| 9/9 [00:00<00:00, 225.60it/s, loss=5.54]\n",
      "Epoch [286/3000]: 100%|██████████| 9/9 [00:00<00:00, 225.60it/s, loss=4.56]\n",
      "Epoch [287/3000]: 100%|██████████| 9/9 [00:00<00:00, 225.61it/s, loss=4.53]\n",
      "Epoch [288/3000]: 100%|██████████| 9/9 [00:00<00:00, 209.84it/s, loss=3.31]\n",
      "Epoch [289/3000]: 100%|██████████| 9/9 [00:00<00:00, 161.14it/s, loss=3.07]\n",
      "Epoch [290/3000]: 100%|██████████| 9/9 [00:00<00:00, 209.87it/s, loss=3.96]\n",
      "Epoch [291/3000]: 100%|██████████| 9/9 [00:00<00:00, 225.61it/s, loss=5.22]\n",
      "Epoch [292/3000]: 100%|██████████| 9/9 [00:00<00:00, 209.86it/s, loss=5.97]\n",
      "Epoch [293/3000]: 100%|██████████| 9/9 [00:00<00:00, 164.07it/s, loss=3.7]\n",
      "Epoch [294/3000]: 100%|██████████| 9/9 [00:00<00:00, 214.85it/s, loss=4.42]\n",
      "Epoch [295/3000]: 100%|██████████| 9/9 [00:00<00:00, 225.60it/s, loss=4.3]\n",
      "Epoch [296/3000]: 100%|██████████| 9/9 [00:00<00:00, 214.86it/s, loss=5.24]\n",
      "Epoch [297/3000]: 100%|██████████| 9/9 [00:00<00:00, 170.27it/s, loss=3.84]\n",
      "Epoch [298/3000]: 100%|██████████| 9/9 [00:00<00:00, 214.86it/s, loss=3.96]\n",
      "Epoch [299/3000]: 100%|██████████| 9/9 [00:00<00:00, 220.10it/s, loss=2.98]\n",
      "Epoch [300/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.09it/s, loss=3.44]\n",
      "Epoch [301/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=3.37]\n",
      "Epoch [302/3000]: 100%|██████████| 9/9 [00:00<00:00, 143.22it/s, loss=4.85]\n",
      "Epoch [303/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=4.92]\n",
      "Epoch [304/3000]: 100%|██████████| 9/9 [00:00<00:00, 150.40it/s, loss=3.22]\n",
      "Epoch [305/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.94it/s, loss=4.31]\n",
      "Epoch [306/3000]: 100%|██████████| 9/9 [00:00<00:00, 118.74it/s, loss=4.49]\n",
      "Epoch [307/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=4.15]\n",
      "Epoch [308/3000]: 100%|██████████| 9/9 [00:00<00:00, 94.94it/s, loss=3.99]\n",
      "Epoch [309/3000]: 100%|██████████| 9/9 [00:00<00:00, 97.65it/s, loss=3.07]\n",
      "Epoch [310/3000]: 100%|██████████| 9/9 [00:00<00:00, 241.48it/s, loss=6.11]\n",
      "Epoch [311/3000]: 100%|██████████| 9/9 [00:00<00:00, 114.39it/s, loss=4.46]\n",
      "Epoch [312/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.01it/s, loss=4.24]\n",
      "Epoch [313/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.54it/s, loss=3.25]\n",
      "Epoch [314/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=4.17]\n",
      "Epoch [315/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=3.2]\n",
      "Epoch [316/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.09it/s, loss=2.75]\n",
      "Epoch [317/3000]: 100%|██████████| 9/9 [00:00<00:00, 164.08it/s, loss=5.44]\n",
      "Epoch [318/3000]: 100%|██████████| 9/9 [00:00<00:00, 214.86it/s, loss=3.61]\n",
      "Epoch [319/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.53it/s, loss=3.61]\n",
      "Epoch [320/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=3.3]\n",
      "Epoch [321/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.20it/s, loss=3.07]\n",
      "Epoch [322/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.17it/s, loss=3.55]\n",
      "Epoch [323/3000]: 100%|██████████| 9/9 [00:00<00:00, 164.08it/s, loss=3.14]\n",
      "Epoch [324/3000]: 100%|██████████| 9/9 [00:00<00:00, 170.26it/s, loss=5.05]\n",
      "Epoch [325/3000]: 100%|██████████| 9/9 [00:00<00:00, 209.86it/s, loss=4.11]\n",
      "Epoch [326/3000]: 100%|██████████| 9/9 [00:00<00:00, 220.10it/s, loss=3.11]\n",
      "Epoch [327/3000]: 100%|██████████| 9/9 [00:00<00:00, 209.86it/s, loss=3.46]\n",
      "Epoch [328/3000]: 100%|██████████| 9/9 [00:00<00:00, 209.86it/s, loss=3.42]\n",
      "Epoch [329/3000]: 100%|██████████| 9/9 [00:00<00:00, 220.10it/s, loss=4.37]\n",
      "Epoch [330/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=4.48]\n",
      "Epoch [331/3000]: 100%|██████████| 9/9 [00:00<00:00, 220.10it/s, loss=3.47]\n",
      "Epoch [332/3000]: 100%|██████████| 9/9 [00:00<00:00, 231.39it/s, loss=3.04]\n",
      "Epoch [333/3000]: 100%|██████████| 9/9 [00:00<00:00, 225.61it/s, loss=4.05]\n",
      "Epoch [334/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.10it/s, loss=3.47]\n",
      "Epoch [335/3000]: 100%|██████████| 9/9 [00:00<00:00, 225.61it/s, loss=5.26]\n",
      "Epoch [336/3000]: 100%|██████████| 9/9 [00:00<00:00, 243.89it/s, loss=3.87]\n",
      "Epoch [337/3000]: 100%|██████████| 9/9 [00:00<00:00, 220.18it/s, loss=3.49]\n",
      "Epoch [338/3000]: 100%|██████████| 9/9 [00:00<00:00, 214.86it/s, loss=4.3]\n",
      "Epoch [339/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.17it/s, loss=4.06]\n",
      "Epoch [340/3000]: 100%|██████████| 9/9 [00:00<00:00, 209.86it/s, loss=5.1]\n",
      "Epoch [341/3000]: 100%|██████████| 9/9 [00:00<00:00, 250.67it/s, loss=3.57]\n",
      "Epoch [342/3000]: 100%|██████████| 9/9 [00:00<00:00, 214.86it/s, loss=3.55]\n",
      "Epoch [343/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.10it/s, loss=4.74]\n",
      "Epoch [344/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=3.88]\n",
      "Epoch [345/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=4.11]\n",
      "Epoch [346/3000]: 100%|██████████| 9/9 [00:00<00:00, 220.20it/s, loss=3.98]\n",
      "Epoch [347/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.53it/s, loss=2.75]\n",
      "Epoch [348/3000]: 100%|██████████| 9/9 [00:00<00:00, 214.86it/s, loss=3.56]\n",
      "Epoch [349/3000]: 100%|██████████| 9/9 [00:00<00:00, 155.59it/s, loss=4.41]\n",
      "Epoch [350/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.53it/s, loss=5.01]\n",
      "Epoch [351/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=6.11]\n",
      "Epoch [352/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=3.74]\n",
      "Epoch [353/3000]: 100%|██████████| 9/9 [00:00<00:00, 63.11it/s, loss=3.92]\n",
      "Epoch [354/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=2.56]\n",
      "Epoch [355/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.94it/s, loss=5.09]\n",
      "Epoch [356/3000]: 100%|██████████| 9/9 [00:00<00:00, 155.59it/s, loss=3.94]\n",
      "Epoch [357/3000]: 100%|██████████| 9/9 [00:00<00:00, 127.10it/s, loss=4.34]\n",
      "Epoch [358/3000]: 100%|██████████| 9/9 [00:00<00:00, 164.07it/s, loss=6.37]\n",
      "Epoch [359/3000]: 100%|██████████| 9/9 [00:00<00:00, 125.33it/s, loss=3.54]\n",
      "Epoch [360/3000]: 100%|██████████| 9/9 [00:00<00:00, 167.11it/s, loss=2.94]\n",
      "Epoch [361/3000]: 100%|██████████| 9/9 [00:00<00:00, 214.86it/s, loss=4.02]\n",
      "Epoch [362/3000]: 100%|██████████| 9/9 [00:00<00:00, 225.60it/s, loss=4.01]\n",
      "Epoch [363/3000]: 100%|██████████| 9/9 [00:00<00:00, 225.60it/s, loss=5.69]\n",
      "Epoch [364/3000]: 100%|██████████| 9/9 [00:00<00:00, 231.38it/s, loss=4.35]\n",
      "Epoch [365/3000]: 100%|██████████| 9/9 [00:00<00:00, 231.39it/s, loss=5]\n",
      "Epoch [366/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.54it/s, loss=3.32]\n",
      "Epoch [367/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.09it/s, loss=3.94]\n",
      "Epoch [368/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=5.3]\n",
      "Epoch [369/3000]: 100%|██████████| 9/9 [00:00<00:00, 170.26it/s, loss=3.11]\n",
      "Epoch [370/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=3.26]\n",
      "Epoch [371/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=3.72]\n",
      "Epoch [372/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=2.84]\n",
      "Epoch [373/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.09it/s, loss=2.51]\n",
      "Epoch [374/3000]: 100%|██████████| 9/9 [00:00<00:00, 145.55it/s, loss=3.77]\n",
      "Epoch [375/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.94it/s, loss=5.85]\n",
      "Epoch [376/3000]: 100%|██████████| 9/9 [00:00<00:00, 150.40it/s, loss=4.32]\n",
      "Epoch [377/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=4.38]\n",
      "Epoch [378/3000]: 100%|██████████| 9/9 [00:00<00:00, 220.10it/s, loss=4.11]\n",
      "Epoch [379/3000]: 100%|██████████| 9/9 [00:00<00:00, 214.86it/s, loss=5.27]\n",
      "Epoch [380/3000]: 100%|██████████| 9/9 [00:00<00:00, 209.87it/s, loss=4.57]\n",
      "Epoch [381/3000]: 100%|██████████| 9/9 [00:00<00:00, 209.86it/s, loss=3.06]\n",
      "Epoch [382/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.53it/s, loss=3.52]\n",
      "Epoch [383/3000]: 100%|██████████| 9/9 [00:00<00:00, 209.86it/s, loss=2.67]\n",
      "Epoch [384/3000]: 100%|██████████| 9/9 [00:00<00:00, 220.10it/s, loss=3.84]\n",
      "Epoch [385/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.49it/s, loss=2.64]\n",
      "Epoch [386/3000]: 100%|██████████| 9/9 [00:00<00:00, 209.86it/s, loss=2.83]\n",
      "Epoch [387/3000]: 100%|██████████| 9/9 [00:00<00:00, 220.10it/s, loss=3.09]\n",
      "Epoch [388/3000]: 100%|██████████| 9/9 [00:00<00:00, 243.90it/s, loss=3.7]\n",
      "Epoch [389/3000]: 100%|██████████| 9/9 [00:00<00:00, 225.61it/s, loss=2.16]\n",
      "Epoch [390/3000]: 100%|██████████| 9/9 [00:00<00:00, 237.47it/s, loss=4.13]\n",
      "Epoch [391/3000]: 100%|██████████| 9/9 [00:00<00:00, 231.39it/s, loss=2.78]\n",
      "Epoch [392/3000]: 100%|██████████| 9/9 [00:00<00:00, 231.38it/s, loss=5.42]\n",
      "Epoch [393/3000]: 100%|██████████| 9/9 [00:00<00:00, 231.40it/s, loss=2.86]\n",
      "Epoch [394/3000]: 100%|██████████| 9/9 [00:00<00:00, 237.48it/s, loss=5.04]\n",
      "Epoch [395/3000]: 100%|██████████| 9/9 [00:00<00:00, 117.19it/s, loss=3.49]\n",
      "Epoch [396/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.10it/s, loss=3.18]\n",
      "Epoch [397/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.17it/s, loss=3.83]\n",
      "Epoch [398/3000]: 100%|██████████| 9/9 [00:00<00:00, 209.87it/s, loss=3.13]\n",
      "Epoch [399/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=3.41]\n",
      "Epoch [400/3000]: 100%|██████████| 9/9 [00:00<00:00, 225.60it/s, loss=5.3]\n",
      "Epoch [401/3000]: 100%|██████████| 9/9 [00:00<00:00, 231.38it/s, loss=2.89]\n",
      "Epoch [402/3000]: 100%|██████████| 9/9 [00:00<00:00, 225.60it/s, loss=2.95]\n",
      "Epoch [403/3000]: 100%|██████████| 9/9 [00:00<00:00, 231.38it/s, loss=3.39]\n",
      "Epoch [404/3000]: 100%|██████████| 9/9 [00:00<00:00, 220.12it/s, loss=2.85]\n",
      "Epoch [405/3000]: 100%|██████████| 9/9 [00:00<00:00, 231.39it/s, loss=2.86]\n",
      "Epoch [406/3000]: 100%|██████████| 9/9 [00:00<00:00, 214.86it/s, loss=3.87]\n",
      "Epoch [407/3000]: 100%|██████████| 9/9 [00:00<00:00, 225.60it/s, loss=2.82]\n",
      "Epoch [408/3000]: 100%|██████████| 9/9 [00:00<00:00, 231.38it/s, loss=3.62]\n",
      "Epoch [409/3000]: 100%|██████████| 9/9 [00:00<00:00, 237.48it/s, loss=4.56]\n",
      "Epoch [410/3000]: 100%|██████████| 9/9 [00:00<00:00, 237.48it/s, loss=8.95]\n",
      "Epoch [411/3000]: 100%|██████████| 9/9 [00:00<00:00, 231.46it/s, loss=2.68]\n",
      "Epoch [412/3000]: 100%|██████████| 9/9 [00:00<00:00, 243.89it/s, loss=3.11]\n",
      "Epoch [413/3000]: 100%|██████████| 9/9 [00:00<00:00, 243.90it/s, loss=4.24]\n",
      "Epoch [414/3000]: 100%|██████████| 9/9 [00:00<00:00, 220.10it/s, loss=2.96]\n",
      "Epoch [415/3000]: 100%|██████████| 9/9 [00:00<00:00, 237.48it/s, loss=2.5]\n",
      "Epoch [416/3000]: 100%|██████████| 9/9 [00:00<00:00, 243.89it/s, loss=3.04]\n",
      "Epoch [417/3000]: 100%|██████████| 9/9 [00:00<00:00, 243.90it/s, loss=3.3]\n",
      "Epoch [418/3000]: 100%|██████████| 9/9 [00:00<00:00, 243.89it/s, loss=3.35]\n",
      "Epoch [419/3000]: 100%|██████████| 9/9 [00:00<00:00, 243.90it/s, loss=1.97]\n",
      "Epoch [420/3000]: 100%|██████████| 9/9 [00:00<00:00, 220.10it/s, loss=3.2]\n",
      "Epoch [421/3000]: 100%|██████████| 9/9 [00:00<00:00, 214.86it/s, loss=5.89]\n",
      "Epoch [422/3000]: 100%|██████████| 9/9 [00:00<00:00, 220.10it/s, loss=2.7]\n",
      "Epoch [423/3000]: 100%|██████████| 9/9 [00:00<00:00, 250.67it/s, loss=5.14]\n",
      "Epoch [424/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=4.1]\n",
      "Epoch [425/3000]: 100%|██████████| 9/9 [00:00<00:00, 231.39it/s, loss=3.09]\n",
      "Epoch [426/3000]: 100%|██████████| 9/9 [00:00<00:00, 237.48it/s, loss=2.31]\n",
      "Epoch [427/3000]: 100%|██████████| 9/9 [00:00<00:00, 231.38it/s, loss=3.44]\n",
      "Epoch [428/3000]: 100%|██████████| 9/9 [00:00<00:00, 225.60it/s, loss=4.09]\n",
      "Epoch [429/3000]: 100%|██████████| 9/9 [00:00<00:00, 243.89it/s, loss=4.78]\n",
      "Epoch [430/3000]: 100%|██████████| 9/9 [00:00<00:00, 225.61it/s, loss=2.79]\n",
      "Epoch [431/3000]: 100%|██████████| 9/9 [00:00<00:00, 231.39it/s, loss=3.26]\n",
      "Epoch [432/3000]: 100%|██████████| 9/9 [00:00<00:00, 214.86it/s, loss=2.34]\n",
      "Epoch [433/3000]: 100%|██████████| 9/9 [00:00<00:00, 225.61it/s, loss=2.19]\n",
      "Epoch [434/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=3.27]\n",
      "Epoch [435/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.53it/s, loss=3.63]\n",
      "Epoch [436/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=2.88]\n",
      "Epoch [437/3000]: 100%|██████████| 9/9 [00:00<00:00, 115.71it/s, loss=3.43]\n",
      "Epoch [438/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.09it/s, loss=3.91]\n",
      "Epoch [439/3000]: 100%|██████████| 9/9 [00:00<00:00, 231.38it/s, loss=2.45]\n",
      "Epoch [440/3000]: 100%|██████████| 9/9 [00:00<00:00, 220.10it/s, loss=3.23]\n",
      "Epoch [441/3000]: 100%|██████████| 9/9 [00:00<00:00, 220.10it/s, loss=2.91]\n",
      "Epoch [442/3000]: 100%|██████████| 9/9 [00:00<00:00, 214.90it/s, loss=2.98]\n",
      "Epoch [443/3000]: 100%|██████████| 9/9 [00:00<00:00, 220.10it/s, loss=3.78]\n",
      "Epoch [444/3000]: 100%|██████████| 9/9 [00:00<00:00, 209.86it/s, loss=3.84]\n",
      "Epoch [445/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=2.6]\n",
      "Epoch [446/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=2.5]\n",
      "Epoch [447/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.94it/s, loss=4.44]\n",
      "Epoch [448/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=2.53]\n",
      "Epoch [449/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=2.41]\n",
      "Epoch [450/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=3.17]\n",
      "Epoch [451/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.17it/s, loss=3.23]\n",
      "Epoch [452/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.01it/s, loss=3.1]\n",
      "Epoch [453/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=2.41]\n",
      "Epoch [454/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=3.74]\n",
      "Epoch [455/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=3.15]\n",
      "Epoch [456/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.24it/s, loss=3.03]\n",
      "Epoch [457/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=2.4]\n",
      "Epoch [458/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.17it/s, loss=2.38]\n",
      "Epoch [459/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=2.41]\n",
      "Epoch [460/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=1.98]\n",
      "Epoch [461/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=5.79]\n",
      "Epoch [462/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=3.65]\n",
      "Epoch [463/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=3.87]\n",
      "Epoch [464/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=2.39]\n",
      "Epoch [465/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=4.2]\n",
      "Epoch [466/3000]: 100%|██████████| 9/9 [00:00<00:00, 123.62it/s, loss=3.02]\n",
      "Epoch [467/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.54it/s, loss=2.89]\n",
      "Epoch [468/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=2.94]\n",
      "Epoch [469/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=2.32]\n",
      "Epoch [470/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=1.42]\n",
      "Epoch [471/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=2.8]\n",
      "Epoch [472/3000]: 100%|██████████| 9/9 [00:00<00:00, 164.07it/s, loss=3.51]\n",
      "Epoch [473/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.94it/s, loss=4.51]\n",
      "Epoch [474/3000]: 100%|██████████| 9/9 [00:00<00:00, 164.07it/s, loss=3.18]\n",
      "Epoch [475/3000]: 100%|██████████| 9/9 [00:00<00:00, 170.27it/s, loss=2.98]\n",
      "Epoch [476/3000]: 100%|██████████| 9/9 [00:00<00:00, 167.11it/s, loss=3.23]\n",
      "Epoch [477/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.54it/s, loss=4]\n",
      "Epoch [478/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=10.5]\n",
      "Epoch [479/3000]: 100%|██████████| 9/9 [00:00<00:00, 99.17it/s, loss=6.31]\n",
      "Epoch [480/3000]: 100%|██████████| 9/9 [00:00<00:00, 161.15it/s, loss=5.67]\n",
      "Epoch [481/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=3.29]\n",
      "Epoch [482/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.01it/s, loss=2.67]\n",
      "Epoch [483/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=2.92]\n",
      "Epoch [484/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=2.3]\n",
      "Epoch [485/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.94it/s, loss=2.34]\n",
      "Epoch [486/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=2.92]\n",
      "Epoch [487/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=2.36]\n",
      "Epoch [488/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.19it/s, loss=2.12]\n",
      "Epoch [489/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=3.28]\n",
      "Epoch [490/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=2.77]\n",
      "Epoch [491/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=2.18]\n",
      "Epoch [492/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.09it/s, loss=2.55]\n",
      "Epoch [493/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.10it/s, loss=5.42]\n",
      "Epoch [494/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.09it/s, loss=4.03]\n",
      "Epoch [495/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.53it/s, loss=2.87]\n",
      "Epoch [496/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=2.5]\n",
      "Epoch [497/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=3.01]\n",
      "Epoch [498/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=2.51]\n",
      "Epoch [499/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=3.06]\n",
      "Epoch [500/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=3.93]\n",
      "Epoch [501/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=2.69]\n",
      "Epoch [502/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.17it/s, loss=3.37]\n",
      "Epoch [503/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=2.53]\n",
      "Epoch [504/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=3.18]\n",
      "Epoch [505/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=2.69]\n",
      "Epoch [506/3000]: 100%|██████████| 9/9 [00:00<00:00, 170.27it/s, loss=2.82]\n",
      "Epoch [507/3000]: 100%|██████████| 9/9 [00:00<00:00, 170.27it/s, loss=2.41]\n",
      "Epoch [508/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.53it/s, loss=3.46]\n",
      "Epoch [509/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.60it/s, loss=3.16]\n",
      "Epoch [510/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=2.35]\n",
      "Epoch [511/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.17it/s, loss=5.12]\n",
      "Epoch [512/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.53it/s, loss=3.12]\n",
      "Epoch [513/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=3.35]\n",
      "Epoch [514/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=2.16]\n",
      "Epoch [515/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=2.58]\n",
      "Epoch [516/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.10it/s, loss=2.9]\n",
      "Epoch [517/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=2.1]\n",
      "Epoch [518/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.53it/s, loss=2.8]\n",
      "Epoch [519/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=2.37]\n",
      "Epoch [520/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.18it/s, loss=4.13]\n",
      "Epoch [521/3000]: 100%|██████████| 9/9 [00:00<00:00, 110.05it/s, loss=2.27]\n",
      "Epoch [522/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=2.14]\n",
      "Epoch [523/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.17it/s, loss=2.65]\n",
      "Epoch [524/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=4.12]\n",
      "Epoch [525/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.09it/s, loss=2.94]\n",
      "Epoch [526/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.01it/s, loss=3.66]\n",
      "Epoch [527/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=6.76]\n",
      "Epoch [528/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.17it/s, loss=7.04]\n",
      "Epoch [529/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=2.48]\n",
      "Epoch [530/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.10it/s, loss=5.54]\n",
      "Epoch [531/3000]: 100%|██████████| 9/9 [00:00<00:00, 117.19it/s, loss=3.21]\n",
      "Epoch [532/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=2.27]\n",
      "Epoch [533/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.09it/s, loss=1.86]\n",
      "Epoch [534/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.53it/s, loss=2.67]\n",
      "Epoch [535/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.10it/s, loss=3.61]\n",
      "Epoch [536/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.10it/s, loss=3.29]\n",
      "Epoch [537/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.17it/s, loss=2.59]\n",
      "Epoch [538/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.54it/s, loss=2.36]\n",
      "Epoch [539/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=3.92]\n",
      "Epoch [540/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=2.56]\n",
      "Epoch [541/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=3.11]\n",
      "Epoch [542/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=4.25]\n",
      "Epoch [543/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=2.95]\n",
      "Epoch [544/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=3.31]\n",
      "Epoch [545/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.53it/s, loss=3.24]\n",
      "Epoch [546/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=3.05]\n",
      "Epoch [547/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=2.64]\n",
      "Epoch [548/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=3.54]\n",
      "Epoch [549/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=2.51]\n",
      "Epoch [550/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=2.27]\n",
      "Epoch [551/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=2.67]\n",
      "Epoch [552/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=3.15]\n",
      "Epoch [553/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.09it/s, loss=2.85]\n",
      "Epoch [554/3000]: 100%|██████████| 9/9 [00:00<00:00, 209.86it/s, loss=2.96]\n",
      "Epoch [555/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=2.36]\n",
      "Epoch [556/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=2.97]\n",
      "Epoch [557/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=3.13]\n",
      "Epoch [558/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.94it/s, loss=3.61]\n",
      "Epoch [559/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=7.59]\n",
      "Epoch [560/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=2.85]\n",
      "Epoch [561/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.53it/s, loss=2.36]\n",
      "Epoch [562/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.07it/s, loss=2.47]\n",
      "Epoch [563/3000]: 100%|██████████| 9/9 [00:00<00:00, 115.69it/s, loss=3.02]\n",
      "Epoch [564/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=2.3]\n",
      "Epoch [565/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=2.46]\n",
      "Epoch [566/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=3.59]\n",
      "Epoch [567/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=1.82]\n",
      "Epoch [568/3000]: 100%|██████████| 9/9 [00:00<00:00, 209.86it/s, loss=3.07]\n",
      "Epoch [569/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=2.44]\n",
      "Epoch [570/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.09it/s, loss=2.64]\n",
      "Epoch [571/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=2.69]\n",
      "Epoch [572/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.10it/s, loss=3.45]\n",
      "Epoch [573/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=2.64]\n",
      "Epoch [574/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=1.84]\n",
      "Epoch [575/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=2.38]\n",
      "Epoch [576/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.17it/s, loss=2.93]\n",
      "Epoch [577/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.09it/s, loss=2.77]\n",
      "Epoch [578/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=2.17]\n",
      "Epoch [579/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=3.15]\n",
      "Epoch [580/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.09it/s, loss=4.86]\n",
      "Epoch [581/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.05it/s, loss=2.75]\n",
      "Epoch [582/3000]: 100%|██████████| 9/9 [00:00<00:00, 209.86it/s, loss=2.86]\n",
      "Epoch [583/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.09it/s, loss=2.46]\n",
      "Epoch [584/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=2.37]\n",
      "Epoch [585/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=2.29]\n",
      "Epoch [586/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=3.41]\n",
      "Epoch [587/3000]: 100%|██████████| 9/9 [00:00<00:00, 209.86it/s, loss=2.84]\n",
      "Epoch [588/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.16it/s, loss=2.42]\n",
      "Epoch [589/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=2.45]\n",
      "Epoch [590/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=2.45]\n",
      "Epoch [591/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=6.09]\n",
      "Epoch [592/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=5.48]\n",
      "Epoch [593/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=3.23]\n",
      "Epoch [594/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=2.24]\n",
      "Epoch [595/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=4]\n",
      "Epoch [596/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.09it/s, loss=3.47]\n",
      "Epoch [597/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=3.66]\n",
      "Epoch [598/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=2.21]\n",
      "Epoch [599/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.16it/s, loss=3.71]\n",
      "Epoch [600/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.09it/s, loss=2.6]\n",
      "Epoch [601/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.09it/s, loss=1.88]\n",
      "Epoch [602/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.53it/s, loss=2.92]\n",
      "Epoch [603/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=5.14]\n",
      "Epoch [604/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=2.02]\n",
      "Epoch [605/3000]: 100%|██████████| 9/9 [00:00<00:00, 110.05it/s, loss=6.43]\n",
      "Epoch [606/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=3.75]\n",
      "Epoch [607/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=2.7]\n",
      "Epoch [608/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=2.13]\n",
      "Epoch [609/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=2.52]\n",
      "Epoch [610/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=2.59]\n",
      "Epoch [611/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=2.79]\n",
      "Epoch [612/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=1.98]\n",
      "Epoch [613/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=2.73]\n",
      "Epoch [614/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=2.43]\n",
      "Epoch [615/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.17it/s, loss=2.4]\n",
      "Epoch [616/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=2.51]\n",
      "Epoch [617/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.09it/s, loss=2.66]\n",
      "Epoch [618/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=3.06]\n",
      "Epoch [619/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=3.03]\n",
      "Epoch [620/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=5.1]\n",
      "Epoch [621/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.04it/s, loss=4.15]\n",
      "Epoch [622/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.53it/s, loss=3.55]\n",
      "Epoch [623/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=2.86]\n",
      "Epoch [624/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.10it/s, loss=2.59]\n",
      "Epoch [625/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.21it/s, loss=2.05]\n",
      "Epoch [626/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.54it/s, loss=4.98]\n",
      "Epoch [627/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.95it/s, loss=3.54]\n",
      "Epoch [628/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=4.08]\n",
      "Epoch [629/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=2.75]\n",
      "Epoch [630/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=2.88]\n",
      "Epoch [631/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.53it/s, loss=2.33]\n",
      "Epoch [632/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=2.11]\n",
      "Epoch [633/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.09it/s, loss=2.78]\n",
      "Epoch [634/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.17it/s, loss=3.69]\n",
      "Epoch [635/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=2.26]\n",
      "Epoch [636/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=2.12]\n",
      "Epoch [637/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.49it/s, loss=1.77]\n",
      "Epoch [638/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.95it/s, loss=2.03]\n",
      "Epoch [639/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.53it/s, loss=3.21]\n",
      "Epoch [640/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.53it/s, loss=4.21]\n",
      "Epoch [641/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.09it/s, loss=4.29]\n",
      "Epoch [642/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=2.5]\n",
      "Epoch [643/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.09it/s, loss=2.75]\n",
      "Epoch [644/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=2.56]\n",
      "Epoch [645/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.06it/s, loss=3.17]\n",
      "Epoch [646/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=2.09]\n",
      "Epoch [647/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.17it/s, loss=2.91]\n",
      "Epoch [648/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.17it/s, loss=2.68]\n",
      "Epoch [649/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.17it/s, loss=4.64]\n",
      "Epoch [650/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=3.68]\n",
      "Epoch [651/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=3.11]\n",
      "Epoch [652/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=2.16]\n",
      "Epoch [653/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=1.76]\n",
      "Epoch [654/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=2.38]\n",
      "Epoch [655/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.59]\n",
      "Epoch [656/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.94it/s, loss=2.87]\n",
      "Epoch [657/3000]: 100%|██████████| 9/9 [00:00<00:00, 170.27it/s, loss=3.91]\n",
      "Epoch [658/3000]: 100%|██████████| 9/9 [00:00<00:00, 161.14it/s, loss=3.27]\n",
      "Epoch [659/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.50it/s, loss=3.38]\n",
      "Epoch [660/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.95it/s, loss=2.62]\n",
      "Epoch [661/3000]: 100%|██████████| 9/9 [00:00<00:00, 145.55it/s, loss=2.65]\n",
      "Epoch [662/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.48it/s, loss=2.38]\n",
      "Epoch [663/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=3.31]\n",
      "Epoch [664/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=3.18]\n",
      "Epoch [665/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=2.24]\n",
      "Epoch [666/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=3.55]\n",
      "Epoch [667/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=2.9]\n",
      "Epoch [668/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=1.79]\n",
      "Epoch [669/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=4.02]\n",
      "Epoch [670/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=7.18]\n",
      "Epoch [671/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=3.19]\n",
      "Epoch [672/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=2.88]\n",
      "Epoch [673/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=1.85]\n",
      "Epoch [674/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.17it/s, loss=2.67]\n",
      "Epoch [675/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=2.87]\n",
      "Epoch [676/3000]: 100%|██████████| 9/9 [00:00<00:00, 152.95it/s, loss=2.48]\n",
      "Epoch [677/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.17it/s, loss=3.2]\n",
      "Epoch [678/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=2.74]\n",
      "Epoch [679/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=2.06]\n",
      "Epoch [680/3000]: 100%|██████████| 9/9 [00:00<00:00, 167.11it/s, loss=2.41]\n",
      "Epoch [681/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=7.15]\n",
      "Epoch [682/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.09it/s, loss=3.64]\n",
      "Epoch [683/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.14it/s, loss=3.61]\n",
      "Epoch [684/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=2.4]\n",
      "Epoch [685/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=2.98]\n",
      "Epoch [686/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=2.48]\n",
      "Epoch [687/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=2.49]\n",
      "Epoch [688/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=2.58]\n",
      "Epoch [689/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=8.46]\n",
      "Epoch [690/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=9.03]\n",
      "Epoch [691/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=4.83]\n",
      "Epoch [692/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.53it/s, loss=2.41]\n",
      "Epoch [693/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.10it/s, loss=2.8]\n",
      "Epoch [694/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=2.39]\n",
      "Epoch [695/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=2.8]\n",
      "Epoch [696/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=2.1]\n",
      "Epoch [697/3000]: 100%|██████████| 9/9 [00:00<00:00, 209.86it/s, loss=1.99]\n",
      "Epoch [698/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=2.32]\n",
      "Epoch [699/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=3.23]\n",
      "Epoch [700/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=2.29]\n",
      "Epoch [701/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=2.01]\n",
      "Epoch [702/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.53it/s, loss=5.33]\n",
      "Epoch [703/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=2.67]\n",
      "Epoch [704/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.17it/s, loss=4.03]\n",
      "Epoch [705/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=2.5]\n",
      "Epoch [706/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.94it/s, loss=1.54]\n",
      "Epoch [707/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.92]\n",
      "Epoch [708/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=2.41]\n",
      "Epoch [709/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=2.28]\n",
      "Epoch [710/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=3.66]\n",
      "Epoch [711/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.09it/s, loss=2.19]\n",
      "Epoch [712/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=3.73]\n",
      "Epoch [713/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.17it/s, loss=2.32]\n",
      "Epoch [714/3000]: 100%|██████████| 9/9 [00:00<00:00, 132.68it/s, loss=4.14]\n",
      "Epoch [715/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.49it/s, loss=3.01]\n",
      "Epoch [716/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=2.58]\n",
      "Epoch [717/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.53it/s, loss=2.8]\n",
      "Epoch [718/3000]: 100%|██████████| 9/9 [00:00<00:00, 164.08it/s, loss=3.01]\n",
      "Epoch [719/3000]: 100%|██████████| 9/9 [00:00<00:00, 158.32it/s, loss=2.76]\n",
      "Epoch [720/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.54it/s, loss=3.18]\n",
      "Epoch [721/3000]: 100%|██████████| 9/9 [00:00<00:00, 170.27it/s, loss=4.58]\n",
      "Epoch [722/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=7.48]\n",
      "Epoch [723/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=2.22]\n",
      "Epoch [724/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=2.31]\n",
      "Epoch [725/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=2.87]\n",
      "Epoch [726/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.53it/s, loss=2.15]\n",
      "Epoch [727/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.09it/s, loss=2.19]\n",
      "Epoch [728/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.01it/s, loss=3.02]\n",
      "Epoch [729/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=2.43]\n",
      "Epoch [730/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.17it/s, loss=2.25]\n",
      "Epoch [731/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.17it/s, loss=1.9]\n",
      "Epoch [732/3000]: 100%|██████████| 9/9 [00:00<00:00, 114.23it/s, loss=2.49]\n",
      "Epoch [733/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.15it/s, loss=2.25]\n",
      "Epoch [734/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=2.96]\n",
      "Epoch [735/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=2.9]\n",
      "Epoch [736/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=2.42]\n",
      "Epoch [737/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=3.67]\n",
      "Epoch [738/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=3.32]\n",
      "Epoch [739/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=2.68]\n",
      "Epoch [740/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.06it/s, loss=1.64]\n",
      "Epoch [741/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=2.39]\n",
      "Epoch [742/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=2.82]\n",
      "Epoch [743/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.17it/s, loss=2.98]\n",
      "Epoch [744/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=2.52]\n",
      "Epoch [745/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=2.32]\n",
      "Epoch [746/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=5.97]\n",
      "Epoch [747/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.09it/s, loss=3.61]\n",
      "Epoch [748/3000]: 100%|██████████| 9/9 [00:00<00:00, 158.32it/s, loss=2.82]\n",
      "Epoch [749/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=1.64]\n",
      "Epoch [750/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.08]\n",
      "Epoch [751/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.53it/s, loss=1.95]\n",
      "Epoch [752/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=2.29]\n",
      "Epoch [753/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=3]\n",
      "Epoch [754/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=2.11]\n",
      "Epoch [755/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.53it/s, loss=2.15]\n",
      "Epoch [756/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.09it/s, loss=2.85]\n",
      "Epoch [757/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.17it/s, loss=2.8]\n",
      "Epoch [758/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.54it/s, loss=2.43]\n",
      "Epoch [759/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=2.42]\n",
      "Epoch [760/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.17it/s, loss=2.43]\n",
      "Epoch [761/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.53it/s, loss=2.96]\n",
      "Epoch [762/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.09it/s, loss=2.24]\n",
      "Epoch [763/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=2.43]\n",
      "Epoch [764/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=3.17]\n",
      "Epoch [765/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.26it/s, loss=1.54]\n",
      "Epoch [766/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.17it/s, loss=2.74]\n",
      "Epoch [767/3000]: 100%|██████████| 9/9 [00:00<00:00, 209.89it/s, loss=2.29]\n",
      "Epoch [768/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.53it/s, loss=2.46]\n",
      "Epoch [769/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.53it/s, loss=1.95]\n",
      "Epoch [770/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.94it/s, loss=1.7]\n",
      "Epoch [771/3000]: 100%|██████████| 9/9 [00:00<00:00, 209.86it/s, loss=2.2]\n",
      "Epoch [772/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.17it/s, loss=2.93]\n",
      "Epoch [773/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=1.73]\n",
      "Epoch [774/3000]: 100%|██████████| 9/9 [00:00<00:00, 110.05it/s, loss=2.14]\n",
      "Epoch [775/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=3.28]\n",
      "Epoch [776/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=1.84]\n",
      "Epoch [777/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.17it/s, loss=1.67]\n",
      "Epoch [778/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.17it/s, loss=1.85]\n",
      "Epoch [779/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.53it/s, loss=2.28]\n",
      "Epoch [780/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.53it/s, loss=2.43]\n",
      "Epoch [781/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=2.03]\n",
      "Epoch [782/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.53it/s, loss=1.79]\n",
      "Epoch [783/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.01it/s, loss=4.54]\n",
      "Epoch [784/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=7.23]\n",
      "Epoch [785/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=13.7]\n",
      "Epoch [786/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.09it/s, loss=9.02]\n",
      "Epoch [787/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.09it/s, loss=3.58]\n",
      "Epoch [788/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=2.3]\n",
      "Epoch [789/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=2.62]\n",
      "Epoch [790/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=2.26]\n",
      "Epoch [791/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.54it/s, loss=2.19]\n",
      "Epoch [792/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=2.03]\n",
      "Epoch [793/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=2.33]\n",
      "Epoch [794/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=3.21]\n",
      "Epoch [795/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.54it/s, loss=3.75]\n",
      "Epoch [796/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.53it/s, loss=2.8]\n",
      "Epoch [797/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.10it/s, loss=3.37]\n",
      "Epoch [798/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=2.08]\n",
      "Epoch [799/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=3.02]\n",
      "Epoch [800/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=2.23]\n",
      "Epoch [801/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=2.77]\n",
      "Epoch [802/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.09it/s, loss=2.95]\n",
      "Epoch [803/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.53it/s, loss=3.65]\n",
      "Epoch [804/3000]: 100%|██████████| 9/9 [00:00<00:00, 209.86it/s, loss=2.52]\n",
      "Epoch [805/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.53it/s, loss=1.92]\n",
      "Epoch [806/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=2.2]\n",
      "Epoch [807/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=2.98]\n",
      "Epoch [808/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.17it/s, loss=2.02]\n",
      "Epoch [809/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=2.96]\n",
      "Epoch [810/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=6.05]\n",
      "Epoch [811/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=2.51]\n",
      "Epoch [812/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.54it/s, loss=5.26]\n",
      "Epoch [813/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.54it/s, loss=2.3]\n",
      "Epoch [814/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.23it/s, loss=6.43]\n",
      "Epoch [815/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=2.4]\n",
      "Epoch [816/3000]: 100%|██████████| 9/9 [00:00<00:00, 108.72it/s, loss=3.19]\n",
      "Epoch [817/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=2.61]\n",
      "Epoch [818/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=2.23]\n",
      "Epoch [819/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=2.59]\n",
      "Epoch [820/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.53it/s, loss=2.68]\n",
      "Epoch [821/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=2.54]\n",
      "Epoch [822/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=3.57]\n",
      "Epoch [823/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=1.88]\n",
      "Epoch [824/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=1.68]\n",
      "Epoch [825/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.53it/s, loss=1.77]\n",
      "Epoch [826/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.09it/s, loss=2.08]\n",
      "Epoch [827/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.17it/s, loss=2.97]\n",
      "Epoch [828/3000]: 100%|██████████| 9/9 [00:00<00:00, 161.14it/s, loss=2.04]\n",
      "Epoch [829/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=1.88]\n",
      "Epoch [830/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=2.14]\n",
      "Epoch [831/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=2.52]\n",
      "Epoch [832/3000]: 100%|██████████| 9/9 [00:00<00:00, 191.98it/s, loss=2.48]\n",
      "Epoch [833/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=2.36]\n",
      "Epoch [834/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=2.55]\n",
      "Epoch [835/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=2.7]\n",
      "Epoch [836/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=2.24]\n",
      "Epoch [837/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=2.09]\n",
      "Epoch [838/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.92]\n",
      "Epoch [839/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=1.94]\n",
      "Epoch [840/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.53it/s, loss=2.73]\n",
      "Epoch [841/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=2.58]\n",
      "Epoch [842/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.53it/s, loss=2.27]\n",
      "Epoch [843/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.09it/s, loss=2.26]\n",
      "Epoch [844/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=2.12]\n",
      "Epoch [845/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=3.18]\n",
      "Epoch [846/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=6.29]\n",
      "Epoch [847/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=4.74]\n",
      "Epoch [848/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.17it/s, loss=2.08]\n",
      "Epoch [849/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.53it/s, loss=2.13]\n",
      "Epoch [850/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=2.82]\n",
      "Epoch [851/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=1.71]\n",
      "Epoch [852/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=2.99]\n",
      "Epoch [853/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.17it/s, loss=2.37]\n",
      "Epoch [854/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=2.8]\n",
      "Epoch [855/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.53it/s, loss=3.54]\n",
      "Epoch [856/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=2.85]\n",
      "Epoch [857/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=2.52]\n",
      "Epoch [858/3000]: 100%|██████████| 9/9 [00:00<00:00, 111.41it/s, loss=3.11]\n",
      "Epoch [859/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=2.27]\n",
      "Epoch [860/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.22it/s, loss=2.37]\n",
      "Epoch [861/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=2.59]\n",
      "Epoch [862/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=1.77]\n",
      "Epoch [863/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=2.06]\n",
      "Epoch [864/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=1.82]\n",
      "Epoch [865/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.17it/s, loss=3.43]\n",
      "Epoch [866/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=3.41]\n",
      "Epoch [867/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=3.04]\n",
      "Epoch [868/3000]: 100%|██████████| 9/9 [00:00<00:00, 209.86it/s, loss=1.88]\n",
      "Epoch [869/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=2.87]\n",
      "Epoch [870/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=2.17]\n",
      "Epoch [871/3000]: 100%|██████████| 9/9 [00:00<00:00, 170.26it/s, loss=2.31]\n",
      "Epoch [872/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=2.26]\n",
      "Epoch [873/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.53it/s, loss=2.34]\n",
      "Epoch [874/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.09it/s, loss=2.43]\n",
      "Epoch [875/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=2.85]\n",
      "Epoch [876/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.53it/s, loss=1.96]\n",
      "Epoch [877/3000]: 100%|██████████| 9/9 [00:00<00:00, 191.91it/s, loss=2.48]\n",
      "Epoch [878/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.17it/s, loss=1.63]\n",
      "Epoch [879/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=2.18]\n",
      "Epoch [880/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=1.98]\n",
      "Epoch [881/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=2.51]\n",
      "Epoch [882/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=2.1]\n",
      "Epoch [883/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.14it/s, loss=2.21]\n",
      "Epoch [884/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.09it/s, loss=1.98]\n",
      "Epoch [885/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=1.71]\n",
      "Epoch [886/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=1.59]\n",
      "Epoch [887/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=2.52]\n",
      "Epoch [888/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.94it/s, loss=4.29]\n",
      "Epoch [889/3000]: 100%|██████████| 9/9 [00:00<00:00, 170.26it/s, loss=2.68]\n",
      "Epoch [890/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=2.28]\n",
      "Epoch [891/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.54it/s, loss=2.19]\n",
      "Epoch [892/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=3.13]\n",
      "Epoch [893/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=2.24]\n",
      "Epoch [894/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=2.34]\n",
      "Epoch [895/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.17it/s, loss=2.46]\n",
      "Epoch [896/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=2.23]\n",
      "Epoch [897/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=2.44]\n",
      "Epoch [898/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=2.11]\n",
      "Epoch [899/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=2.32]\n",
      "Epoch [900/3000]: 100%|██████████| 9/9 [00:00<00:00, 111.40it/s, loss=1.86]\n",
      "Epoch [901/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=2.07]\n",
      "Epoch [902/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.58it/s, loss=2.36]\n",
      "Epoch [903/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=1.96]\n",
      "Epoch [904/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=1.8]\n",
      "Epoch [905/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=2.86]\n",
      "Epoch [906/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=3]\n",
      "Epoch [907/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=2.51]\n",
      "Epoch [908/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.53it/s, loss=2.9]\n",
      "Epoch [909/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=3.3]\n",
      "Epoch [910/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=1.47]\n",
      "Epoch [911/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=2.36]\n",
      "Epoch [912/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.17it/s, loss=2.74]\n",
      "Epoch [913/3000]: 100%|██████████| 9/9 [00:00<00:00, 191.97it/s, loss=1.96]\n",
      "Epoch [914/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=2.09]\n",
      "Epoch [915/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=2.04]\n",
      "Epoch [916/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.53it/s, loss=1.72]\n",
      "Epoch [917/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.53it/s, loss=1.65]\n",
      "Epoch [918/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.09it/s, loss=2.36]\n",
      "Epoch [919/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=1.83]\n",
      "Epoch [920/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.17it/s, loss=2.89]\n",
      "Epoch [921/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=1.62]\n",
      "Epoch [922/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.17it/s, loss=4.55]\n",
      "Epoch [923/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=1.99]\n",
      "Epoch [924/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.54it/s, loss=4.25]\n",
      "Epoch [925/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=2.27]\n",
      "Epoch [926/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.17it/s, loss=3.39]\n",
      "Epoch [927/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=2]\n",
      "Epoch [928/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.17it/s, loss=1.84]\n",
      "Epoch [929/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.01it/s, loss=2.87]\n",
      "Epoch [930/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=1.93]\n",
      "Epoch [931/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=3.21]\n",
      "Epoch [932/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.53it/s, loss=1.68]\n",
      "Epoch [933/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=1.95]\n",
      "Epoch [934/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=2.71]\n",
      "Epoch [935/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.03it/s, loss=2.77]\n",
      "Epoch [936/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=3.35]\n",
      "Epoch [937/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.53it/s, loss=1.6]\n",
      "Epoch [938/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.09it/s, loss=3.22]\n",
      "Epoch [939/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.95]\n",
      "Epoch [940/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=2.16]\n",
      "Epoch [941/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=1.83]\n",
      "Epoch [942/3000]: 100%|██████████| 9/9 [00:00<00:00, 112.80it/s, loss=2.53]\n",
      "Epoch [943/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.09it/s, loss=2.68]\n",
      "Epoch [944/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=2.3]\n",
      "Epoch [945/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=2.43]\n",
      "Epoch [946/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=3.24]\n",
      "Epoch [947/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=4.09]\n",
      "Epoch [948/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=2.48]\n",
      "Epoch [949/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=4.12]\n",
      "Epoch [950/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.01it/s, loss=1.91]\n",
      "Epoch [951/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=1.49]\n",
      "Epoch [952/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=2.41]\n",
      "Epoch [953/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=2.73]\n",
      "Epoch [954/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=2.25]\n",
      "Epoch [955/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=1.91]\n",
      "Epoch [956/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=2.5]\n",
      "Epoch [957/3000]: 100%|██████████| 9/9 [00:00<00:00, 170.26it/s, loss=2.48]\n",
      "Epoch [958/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=1.61]\n",
      "Epoch [959/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=2.32]\n",
      "Epoch [960/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=1.74]\n",
      "Epoch [961/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=1.93]\n",
      "Epoch [962/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=4.5]\n",
      "Epoch [963/3000]: 100%|██████████| 9/9 [00:00<00:00, 170.26it/s, loss=2.3]\n",
      "Epoch [964/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.83]\n",
      "Epoch [965/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.9]\n",
      "Epoch [966/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=2.45]\n",
      "Epoch [967/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=2.66]\n",
      "Epoch [968/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=2.19]\n",
      "Epoch [969/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.94it/s, loss=2.19]\n",
      "Epoch [970/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.94it/s, loss=1.96]\n",
      "Epoch [971/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=2.43]\n",
      "Epoch [972/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=2.55]\n",
      "Epoch [973/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.12it/s, loss=2.02]\n",
      "Epoch [974/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=2.11]\n",
      "Epoch [975/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=2.16]\n",
      "Epoch [976/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=2.69]\n",
      "Epoch [977/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.17it/s, loss=2.35]\n",
      "Epoch [978/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=2.25]\n",
      "Epoch [979/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=1.57]\n",
      "Epoch [980/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=1.72]\n",
      "Epoch [981/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=1.68]\n",
      "Epoch [982/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.17it/s, loss=1.96]\n",
      "Epoch [983/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=2.31]\n",
      "Epoch [984/3000]: 100%|██████████| 9/9 [00:00<00:00, 106.16it/s, loss=1.62]\n",
      "Epoch [985/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.17it/s, loss=5.6]\n",
      "Epoch [986/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=2.73]\n",
      "Epoch [987/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=2.96]\n",
      "Epoch [988/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.93]\n",
      "Epoch [989/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=3.75]\n",
      "Epoch [990/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=2.79]\n",
      "Epoch [991/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.02it/s, loss=2.1]\n",
      "Epoch [992/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=2.2]\n",
      "Epoch [993/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=2.35]\n",
      "Epoch [994/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.09it/s, loss=3.48]\n",
      "Epoch [995/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=2.43]\n",
      "Epoch [996/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=2.06]\n",
      "Epoch [997/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.17it/s, loss=1.67]\n",
      "Epoch [998/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=1.88]\n",
      "Epoch [999/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=1.9]\n",
      "Epoch [1000/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=2.05]\n",
      "Epoch [1001/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=2.63]\n",
      "Epoch [1002/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=2.64]\n",
      "Epoch [1003/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=2.69]\n",
      "Epoch [1004/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.02it/s, loss=1.78]\n",
      "Epoch [1005/3000]: 100%|██████████| 9/9 [00:00<00:00, 161.14it/s, loss=2.77]\n",
      "Epoch [1006/3000]: 100%|██████████| 9/9 [00:00<00:00, 150.40it/s, loss=2.29]\n",
      "Epoch [1007/3000]: 100%|██████████| 9/9 [00:00<00:00, 150.40it/s, loss=2.17]\n",
      "Epoch [1008/3000]: 100%|██████████| 9/9 [00:00<00:00, 161.14it/s, loss=1.73]\n",
      "Epoch [1009/3000]: 100%|██████████| 9/9 [00:00<00:00, 161.17it/s, loss=2.02]\n",
      "Epoch [1010/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.64]\n",
      "Epoch [1011/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.53it/s, loss=2.62]\n",
      "Epoch [1012/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=4.49]\n",
      "Epoch [1013/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.94it/s, loss=1.89]\n",
      "Epoch [1014/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=3]\n",
      "Epoch [1015/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.53it/s, loss=3.42]\n",
      "Epoch [1016/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.09it/s, loss=3.41]\n",
      "Epoch [1017/3000]: 100%|██████████| 9/9 [00:00<00:00, 209.87it/s, loss=1.67]\n",
      "Epoch [1018/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=2.21]\n",
      "Epoch [1019/3000]: 100%|██████████| 9/9 [00:00<00:00, 143.24it/s, loss=2.75]\n",
      "Epoch [1020/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.17it/s, loss=1.75]\n",
      "Epoch [1021/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.09it/s, loss=2.33]\n",
      "Epoch [1022/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=2.34]\n",
      "Epoch [1023/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.15it/s, loss=3.07]\n",
      "Epoch [1024/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=3.03]\n",
      "Epoch [1025/3000]: 100%|██████████| 9/9 [00:00<00:00, 170.27it/s, loss=1.98]\n",
      "Epoch [1026/3000]: 100%|██████████| 9/9 [00:00<00:00, 97.02it/s, loss=2.09]\n",
      "Epoch [1027/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.54it/s, loss=2.57]\n",
      "Epoch [1028/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.54it/s, loss=2.57]\n",
      "Epoch [1029/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=2.9]\n",
      "Epoch [1030/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.17it/s, loss=2.47]\n",
      "Epoch [1031/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.09it/s, loss=2.2]\n",
      "Epoch [1032/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.09it/s, loss=2.1]\n",
      "Epoch [1033/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=2.07]\n",
      "Epoch [1034/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.53it/s, loss=2.24]\n",
      "Epoch [1035/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.09it/s, loss=1.59]\n",
      "Epoch [1036/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=2.59]\n",
      "Epoch [1037/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=3.63]\n",
      "Epoch [1038/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=2.95]\n",
      "Epoch [1039/3000]: 100%|██████████| 9/9 [00:00<00:00, 167.18it/s, loss=2.48]\n",
      "Epoch [1040/3000]: 100%|██████████| 9/9 [00:00<00:00, 155.59it/s, loss=1.45]\n",
      "Epoch [1041/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.96]\n",
      "Epoch [1042/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.94it/s, loss=1.95]\n",
      "Epoch [1043/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=1.95]\n",
      "Epoch [1044/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.54it/s, loss=2.51]\n",
      "Epoch [1045/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=2.12]\n",
      "Epoch [1046/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=2.6]\n",
      "Epoch [1047/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=3.25]\n",
      "Epoch [1048/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.17it/s, loss=2.11]\n",
      "Epoch [1049/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.54it/s, loss=1.79]\n",
      "Epoch [1050/3000]: 100%|██████████| 9/9 [00:00<00:00, 161.14it/s, loss=2.19]\n",
      "Epoch [1051/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=1.52]\n",
      "Epoch [1052/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.17it/s, loss=2.02]\n",
      "Epoch [1053/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.54it/s, loss=2.38]\n",
      "Epoch [1054/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.17it/s, loss=1.67]\n",
      "Epoch [1055/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.45it/s, loss=2.18]\n",
      "Epoch [1056/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.53it/s, loss=2.98]\n",
      "Epoch [1057/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=2.93]\n",
      "Epoch [1058/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.17it/s, loss=1.84]\n",
      "Epoch [1059/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=1.69]\n",
      "Epoch [1060/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.58it/s, loss=2.34]\n",
      "Epoch [1061/3000]: 100%|██████████| 9/9 [00:00<00:00, 161.14it/s, loss=1.61]\n",
      "Epoch [1062/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=1.98]\n",
      "Epoch [1063/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=1.79]\n",
      "Epoch [1064/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.54it/s, loss=2.04]\n",
      "Epoch [1065/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=1.65]\n",
      "Epoch [1066/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.94it/s, loss=2.92]\n",
      "Epoch [1067/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=2.56]\n",
      "Epoch [1068/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.6]\n",
      "Epoch [1069/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=2.4]\n",
      "Epoch [1070/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=1.39]\n",
      "Epoch [1071/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=3.24]\n",
      "Epoch [1072/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.94it/s, loss=1.9]\n",
      "Epoch [1073/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=2.74]\n",
      "Epoch [1074/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=2.27]\n",
      "Epoch [1075/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=2.14]\n",
      "Epoch [1076/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.64]\n",
      "Epoch [1077/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.56it/s, loss=2.76]\n",
      "Epoch [1078/3000]: 100%|██████████| 9/9 [00:00<00:00, 134.68it/s, loss=3.76]\n",
      "Epoch [1079/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=2.92]\n",
      "Epoch [1080/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=1.63]\n",
      "Epoch [1081/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.54it/s, loss=2.88]\n",
      "Epoch [1082/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.94it/s, loss=2.29]\n",
      "Epoch [1083/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.73]\n",
      "Epoch [1084/3000]: 100%|██████████| 9/9 [00:00<00:00, 209.86it/s, loss=2.3]\n",
      "Epoch [1085/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.09it/s, loss=1.32]\n",
      "Epoch [1086/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.94it/s, loss=1.62]\n",
      "Epoch [1087/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=1.79]\n",
      "Epoch [1088/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=2.5]\n",
      "Epoch [1089/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=2.79]\n",
      "Epoch [1090/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.09it/s, loss=3.06]\n",
      "Epoch [1091/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=2.97]\n",
      "Epoch [1092/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=1.97]\n",
      "Epoch [1093/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=2.93]\n",
      "Epoch [1094/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.55it/s, loss=1.78]\n",
      "Epoch [1095/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=3.06]\n",
      "Epoch [1096/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.53it/s, loss=2.57]\n",
      "Epoch [1097/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.17it/s, loss=1.35]\n",
      "Epoch [1098/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=2.55]\n",
      "Epoch [1099/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=2.46]\n",
      "Epoch [1100/3000]: 100%|██████████| 9/9 [00:00<00:00, 191.95it/s, loss=2.45]\n",
      "Epoch [1101/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=2.93]\n",
      "Epoch [1102/3000]: 100%|██████████| 9/9 [00:00<00:00, 117.20it/s, loss=2.01]\n",
      "Epoch [1103/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.01it/s, loss=1.79]\n",
      "Epoch [1104/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.93]\n",
      "Epoch [1105/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=2.44]\n",
      "Epoch [1106/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.17it/s, loss=2.66]\n",
      "Epoch [1107/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=2.93]\n",
      "Epoch [1108/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=1.61]\n",
      "Epoch [1109/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=1.94]\n",
      "Epoch [1110/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.94it/s, loss=2.11]\n",
      "Epoch [1111/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=2.29]\n",
      "Epoch [1112/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.02it/s, loss=2.22]\n",
      "Epoch [1113/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=1.79]\n",
      "Epoch [1114/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=1.84]\n",
      "Epoch [1115/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=3.3]\n",
      "Epoch [1116/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.81]\n",
      "Epoch [1117/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.09it/s, loss=2.68]\n",
      "Epoch [1118/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.17it/s, loss=1.8]\n",
      "Epoch [1119/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.52it/s, loss=2.1]\n",
      "Epoch [1120/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=2.62]\n",
      "Epoch [1121/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.53it/s, loss=1.75]\n",
      "Epoch [1122/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.94it/s, loss=1.9]\n",
      "Epoch [1123/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=2.31]\n",
      "Epoch [1124/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=2.25]\n",
      "Epoch [1125/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=3.29]\n",
      "Epoch [1126/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.17it/s, loss=1.6]\n",
      "Epoch [1127/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=2.94]\n",
      "Epoch [1128/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=2.23]\n",
      "Epoch [1129/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.53it/s, loss=1.62]\n",
      "Epoch [1130/3000]: 100%|██████████| 9/9 [00:00<00:00, 209.89it/s, loss=1.79]\n",
      "Epoch [1131/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.09it/s, loss=2.9]\n",
      "Epoch [1132/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=4.29]\n",
      "Epoch [1133/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.10it/s, loss=1.74]\n",
      "Epoch [1134/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=2.48]\n",
      "Epoch [1135/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=2.86]\n",
      "Epoch [1136/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.09it/s, loss=1.77]\n",
      "Epoch [1137/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.53it/s, loss=2.32]\n",
      "Epoch [1138/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=2.47]\n",
      "Epoch [1139/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=4.12]\n",
      "Epoch [1140/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.94it/s, loss=2.3]\n",
      "Epoch [1141/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=3.8]\n",
      "Epoch [1142/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.09it/s, loss=2.58]\n",
      "Epoch [1143/3000]: 100%|██████████| 9/9 [00:00<00:00, 214.86it/s, loss=3.61]\n",
      "Epoch [1144/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=2.17]\n",
      "Epoch [1145/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.17it/s, loss=1.87]\n",
      "Epoch [1146/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=1.82]\n",
      "Epoch [1147/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=2.2]\n",
      "Epoch [1148/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.09it/s, loss=2.31]\n",
      "Epoch [1149/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.09it/s, loss=2.56]\n",
      "Epoch [1150/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.95]\n",
      "Epoch [1151/3000]: 100%|██████████| 9/9 [00:00<00:00, 209.87it/s, loss=1.22]\n",
      "Epoch [1152/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.10it/s, loss=1.74]\n",
      "Epoch [1153/3000]: 100%|██████████| 9/9 [00:00<00:00, 110.05it/s, loss=1.89]\n",
      "Epoch [1154/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.17it/s, loss=2.34]\n",
      "Epoch [1155/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.53it/s, loss=2.95]\n",
      "Epoch [1156/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=2.17]\n",
      "Epoch [1157/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.09it/s, loss=1.83]\n",
      "Epoch [1158/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=1.8]\n",
      "Epoch [1159/3000]: 100%|██████████| 9/9 [00:00<00:00, 167.64it/s, loss=2.98]\n",
      "Epoch [1160/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=1.54]\n",
      "Epoch [1161/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.53it/s, loss=4.19]\n",
      "Epoch [1162/3000]: 100%|██████████| 9/9 [00:00<00:00, 167.12it/s, loss=1.38]\n",
      "Epoch [1163/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=1.63]\n",
      "Epoch [1164/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=2.59]\n",
      "Epoch [1165/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=1.53]\n",
      "Epoch [1166/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.53it/s, loss=2.22]\n",
      "Epoch [1167/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=2.2]\n",
      "Epoch [1168/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=1.96]\n",
      "Epoch [1169/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.17it/s, loss=2.64]\n",
      "Epoch [1170/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.17it/s, loss=2.02]\n",
      "Epoch [1171/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=1.74]\n",
      "Epoch [1172/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.78]\n",
      "Epoch [1173/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.73]\n",
      "Epoch [1174/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.10it/s, loss=1.37]\n",
      "Epoch [1175/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=2.11]\n",
      "Epoch [1176/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=1.85]\n",
      "Epoch [1177/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=2.52]\n",
      "Epoch [1178/3000]: 100%|██████████| 9/9 [00:00<00:00, 155.59it/s, loss=2.34]\n",
      "Epoch [1179/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=1.92]\n",
      "Epoch [1180/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.94it/s, loss=3.95]\n",
      "Epoch [1181/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=1.57]\n",
      "Epoch [1182/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=2.89]\n",
      "Epoch [1183/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.10it/s, loss=1.93]\n",
      "Epoch [1184/3000]: 100%|██████████| 9/9 [00:00<00:00, 94.56it/s, loss=2.15]\n",
      "Epoch [1185/3000]: 100%|██████████| 9/9 [00:00<00:00, 99.63it/s, loss=1.81]\n",
      "Epoch [1186/3000]: 100%|██████████| 9/9 [00:00<00:00, 94.99it/s, loss=2.43]\n",
      "Epoch [1187/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=2.47]\n",
      "Epoch [1188/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=2.27]\n",
      "Epoch [1189/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.13it/s, loss=2.43]\n",
      "Epoch [1190/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.93]\n",
      "Epoch [1191/3000]: 100%|██████████| 9/9 [00:00<00:00, 125.33it/s, loss=2.79]\n",
      "Epoch [1192/3000]: 100%|██████████| 9/9 [00:00<00:00, 125.33it/s, loss=1.95]\n",
      "Epoch [1193/3000]: 100%|██████████| 9/9 [00:00<00:00, 114.23it/s, loss=1.51]\n",
      "Epoch [1194/3000]: 100%|██████████| 9/9 [00:00<00:00, 121.95it/s, loss=1.73]\n",
      "Epoch [1195/3000]: 100%|██████████| 9/9 [00:00<00:00, 97.03it/s, loss=1.88]\n",
      "Epoch [1196/3000]: 100%|██████████| 9/9 [00:00<00:00, 150.40it/s, loss=3.09]\n",
      "Epoch [1197/3000]: 100%|██████████| 9/9 [00:00<00:00, 136.73it/s, loss=2.08]\n",
      "Epoch [1198/3000]: 100%|██████████| 9/9 [00:00<00:00, 150.40it/s, loss=2.11]\n",
      "Epoch [1199/3000]: 100%|██████████| 9/9 [00:00<00:00, 150.40it/s, loss=1.7]\n",
      "Epoch [1200/3000]: 100%|██████████| 9/9 [00:00<00:00, 145.55it/s, loss=1.64]\n",
      "Epoch [1201/3000]: 100%|██████████| 9/9 [00:00<00:00, 147.94it/s, loss=2.35]\n",
      "Epoch [1202/3000]: 100%|██████████| 9/9 [00:00<00:00, 158.32it/s, loss=2.11]\n",
      "Epoch [1203/3000]: 100%|██████████| 9/9 [00:00<00:00, 161.15it/s, loss=2.3]\n",
      "Epoch [1204/3000]: 100%|██████████| 9/9 [00:00<00:00, 167.11it/s, loss=2.67]\n",
      "Epoch [1205/3000]: 100%|██████████| 9/9 [00:00<00:00, 161.14it/s, loss=1.82]\n",
      "Epoch [1206/3000]: 100%|██████████| 9/9 [00:00<00:00, 140.97it/s, loss=2.18]\n",
      "Epoch [1207/3000]: 100%|██████████| 9/9 [00:00<00:00, 150.40it/s, loss=1.93]\n",
      "Epoch [1208/3000]: 100%|██████████| 9/9 [00:00<00:00, 150.41it/s, loss=3.56]\n",
      "Epoch [1209/3000]: 100%|██████████| 9/9 [00:00<00:00, 150.40it/s, loss=2.11]\n",
      "Epoch [1210/3000]: 100%|██████████| 9/9 [00:00<00:00, 150.40it/s, loss=1.84]\n",
      "Epoch [1211/3000]: 100%|██████████| 9/9 [00:00<00:00, 141.00it/s, loss=1.94]\n",
      "Epoch [1212/3000]: 100%|██████████| 9/9 [00:00<00:00, 94.99it/s, loss=2.34]\n",
      "Epoch [1213/3000]: 100%|██████████| 9/9 [00:00<00:00, 170.28it/s, loss=2.09]\n",
      "Epoch [1214/3000]: 100%|██████████| 9/9 [00:00<00:00, 214.86it/s, loss=1.66]\n",
      "Epoch [1215/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.09it/s, loss=2.65]\n",
      "Epoch [1216/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.17it/s, loss=2.31]\n",
      "Epoch [1217/3000]: 100%|██████████| 9/9 [00:00<00:00, 220.11it/s, loss=2.37]\n",
      "Epoch [1218/3000]: 100%|██████████| 9/9 [00:00<00:00, 214.86it/s, loss=2.82]\n",
      "Epoch [1219/3000]: 100%|██████████| 9/9 [00:00<00:00, 209.86it/s, loss=2.15]\n",
      "Epoch [1220/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.23it/s, loss=2.74]\n",
      "Epoch [1221/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=2.6]\n",
      "Epoch [1222/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.10it/s, loss=3.3]\n",
      "Epoch [1223/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=1.96]\n",
      "Epoch [1224/3000]: 100%|██████████| 9/9 [00:00<00:00, 214.86it/s, loss=2.15]\n",
      "Epoch [1225/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.11it/s, loss=2.38]\n",
      "Epoch [1226/3000]: 100%|██████████| 9/9 [00:00<00:00, 112.80it/s, loss=1.95]\n",
      "Epoch [1227/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.01it/s, loss=1.53]\n",
      "Epoch [1228/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.51]\n",
      "Epoch [1229/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=2.5]\n",
      "Epoch [1230/3000]: 100%|██████████| 9/9 [00:00<00:00, 145.55it/s, loss=2.17]\n",
      "Epoch [1231/3000]: 100%|██████████| 9/9 [00:00<00:00, 79.16it/s, loss=2.43]\n",
      "Epoch [1232/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.09it/s, loss=3.04]\n",
      "Epoch [1233/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.56it/s, loss=1.86]\n",
      "Epoch [1234/3000]: 100%|██████████| 9/9 [00:00<00:00, 138.83it/s, loss=2.41]\n",
      "Epoch [1235/3000]: 100%|██████████| 9/9 [00:00<00:00, 155.59it/s, loss=2.21]\n",
      "Epoch [1236/3000]: 100%|██████████| 9/9 [00:00<00:00, 136.73it/s, loss=2.01]\n",
      "Epoch [1237/3000]: 100%|██████████| 9/9 [00:00<00:00, 99.17it/s, loss=2.31]\n",
      "Epoch [1238/3000]: 100%|██████████| 9/9 [00:00<00:00, 134.69it/s, loss=1.57]\n",
      "Epoch [1239/3000]: 100%|██████████| 9/9 [00:00<00:00, 170.27it/s, loss=2.13]\n",
      "Epoch [1240/3000]: 100%|██████████| 9/9 [00:00<00:00, 156.10it/s, loss=2.29]\n",
      "Epoch [1241/3000]: 100%|██████████| 9/9 [00:00<00:00, 214.86it/s, loss=3.16]\n",
      "Epoch [1242/3000]: 100%|██████████| 9/9 [00:00<00:00, 214.86it/s, loss=1.33]\n",
      "Epoch [1243/3000]: 100%|██████████| 9/9 [00:00<00:00, 161.18it/s, loss=2.55]\n",
      "Epoch [1244/3000]: 100%|██████████| 9/9 [00:00<00:00, 147.94it/s, loss=1.58]\n",
      "Epoch [1245/3000]: 100%|██████████| 9/9 [00:00<00:00, 147.93it/s, loss=1.95]\n",
      "Epoch [1246/3000]: 100%|██████████| 9/9 [00:00<00:00, 132.71it/s, loss=1.88]\n",
      "Epoch [1247/3000]: 100%|██████████| 9/9 [00:00<00:00, 158.32it/s, loss=2.29]\n",
      "Epoch [1248/3000]: 100%|██████████| 9/9 [00:00<00:00, 145.55it/s, loss=3.3]\n",
      "Epoch [1249/3000]: 100%|██████████| 9/9 [00:00<00:00, 127.10it/s, loss=2.77]\n",
      "Epoch [1250/3000]: 100%|██████████| 9/9 [00:00<00:00, 141.00it/s, loss=2.23]\n",
      "Epoch [1251/3000]: 100%|██████████| 9/9 [00:00<00:00, 155.58it/s, loss=3.52]\n",
      "Epoch [1252/3000]: 100%|██████████| 9/9 [00:00<00:00, 152.95it/s, loss=2.4]\n",
      "Epoch [1253/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=2.8]\n",
      "Epoch [1254/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.09it/s, loss=2.07]\n",
      "Epoch [1255/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=2.13]\n",
      "Epoch [1256/3000]: 100%|██████████| 9/9 [00:00<00:00, 214.86it/s, loss=1.74]\n",
      "Epoch [1257/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=1.28]\n",
      "Epoch [1258/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.09it/s, loss=2.73]\n",
      "Epoch [1259/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=2.08]\n",
      "Epoch [1260/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=2.64]\n",
      "Epoch [1261/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=2]\n",
      "Epoch [1262/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=6.79]\n",
      "Epoch [1263/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=2.09]\n",
      "Epoch [1264/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=1.71]\n",
      "Epoch [1265/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.53it/s, loss=2.17]\n",
      "Epoch [1266/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=1.54]\n",
      "Epoch [1267/3000]: 100%|██████████| 9/9 [00:00<00:00, 209.86it/s, loss=1.54]\n",
      "Epoch [1268/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.09it/s, loss=2.5]\n",
      "Epoch [1269/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.54it/s, loss=3.48]\n",
      "Epoch [1270/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=1.85]\n",
      "Epoch [1271/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=2.77]\n",
      "Epoch [1272/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.09it/s, loss=2.05]\n",
      "Epoch [1273/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.53it/s, loss=2.37]\n",
      "Epoch [1274/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.17it/s, loss=2.55]\n",
      "Epoch [1275/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.10it/s, loss=3.19]\n",
      "Epoch [1276/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=2.83]\n",
      "Epoch [1277/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=1.69]\n",
      "Epoch [1278/3000]: 100%|██████████| 9/9 [00:00<00:00, 214.85it/s, loss=2.55]\n",
      "Epoch [1279/3000]: 100%|██████████| 9/9 [00:00<00:00, 94.00it/s, loss=1.93]\n",
      "Epoch [1280/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.17it/s, loss=1.67]\n",
      "Epoch [1281/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.01it/s, loss=1.5]\n",
      "Epoch [1282/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=2.11]\n",
      "Epoch [1283/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=2.88]\n",
      "Epoch [1284/3000]: 100%|██████████| 9/9 [00:00<00:00, 209.86it/s, loss=1.6]\n",
      "Epoch [1285/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.94it/s, loss=1.74]\n",
      "Epoch [1286/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.17it/s, loss=1.53]\n",
      "Epoch [1287/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.17it/s, loss=1.58]\n",
      "Epoch [1288/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=2.03]\n",
      "Epoch [1289/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.09it/s, loss=3.16]\n",
      "Epoch [1290/3000]: 100%|██████████| 9/9 [00:00<00:00, 199.89it/s, loss=2.93]\n",
      "Epoch [1291/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.6]\n",
      "Epoch [1292/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.17it/s, loss=1.36]\n",
      "Epoch [1293/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=1.76]\n",
      "Epoch [1294/3000]: 100%|██████████| 9/9 [00:00<00:00, 209.86it/s, loss=2.54]\n",
      "Epoch [1295/3000]: 100%|██████████| 9/9 [00:00<00:00, 209.92it/s, loss=2.14]\n",
      "Epoch [1296/3000]: 100%|██████████| 9/9 [00:00<00:00, 204.97it/s, loss=2.25]\n",
      "Epoch [1297/3000]: 100%|██████████| 9/9 [00:00<00:00, 209.86it/s, loss=1.12]\n",
      "Epoch [1298/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=2.04]\n",
      "Epoch [1299/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.09it/s, loss=1.82]\n",
      "Epoch [1300/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.94it/s, loss=1.6]\n",
      "Epoch [1301/3000]: 100%|██████████| 9/9 [00:00<00:00, 170.26it/s, loss=1.72]\n",
      "Epoch [1302/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.09it/s, loss=1.78]\n",
      "Epoch [1303/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=2.66]\n",
      "Epoch [1304/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=1.79]\n",
      "Epoch [1305/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.42it/s, loss=2.48]\n",
      "Epoch [1306/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=1.96]\n",
      "Epoch [1307/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.53it/s, loss=2.36]\n",
      "Epoch [1308/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.53it/s, loss=1.93]\n",
      "Epoch [1309/3000]: 100%|██████████| 9/9 [00:00<00:00, 191.92it/s, loss=1.52]\n",
      "Epoch [1310/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=1.49]\n",
      "Epoch [1311/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=2.57]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3000]: Train loss: 214.2723, Valid loss: 113.6523\n",
      "Saving model with loss 113.652...\n",
      "Epoch [2/3000]: Train loss: 80.4898, Valid loss: 96.8883\n",
      "Saving model with loss 96.888...\n",
      "Epoch [3/3000]: Train loss: 59.6256, Valid loss: 55.9641\n",
      "Saving model with loss 55.964...\n",
      "Epoch [4/3000]: Train loss: 50.5092, Valid loss: 50.3099\n",
      "Saving model with loss 50.310...\n",
      "Epoch [5/3000]: Train loss: 49.1287, Valid loss: 49.7028\n",
      "Saving model with loss 49.703...\n",
      "Epoch [6/3000]: Train loss: 48.3627, Valid loss: 52.9697\n",
      "Epoch [7/3000]: Train loss: 48.9421, Valid loss: 43.7860\n",
      "Saving model with loss 43.786...\n",
      "Epoch [8/3000]: Train loss: 48.1919, Valid loss: 47.2328\n",
      "Epoch [9/3000]: Train loss: 46.9934, Valid loss: 55.6460\n",
      "Epoch [10/3000]: Train loss: 47.4353, Valid loss: 53.9172\n",
      "Epoch [11/3000]: Train loss: 44.9965, Valid loss: 52.1107\n",
      "Epoch [12/3000]: Train loss: 44.6087, Valid loss: 49.6368\n",
      "Epoch [13/3000]: Train loss: 42.4504, Valid loss: 41.3038\n",
      "Saving model with loss 41.304...\n",
      "Epoch [14/3000]: Train loss: 41.3929, Valid loss: 43.0699\n",
      "Epoch [15/3000]: Train loss: 40.0004, Valid loss: 40.9452\n",
      "Saving model with loss 40.945...\n",
      "Epoch [16/3000]: Train loss: 40.0466, Valid loss: 39.2403\n",
      "Saving model with loss 39.240...\n",
      "Epoch [17/3000]: Train loss: 39.0437, Valid loss: 48.9329\n",
      "Epoch [18/3000]: Train loss: 41.3664, Valid loss: 42.5751\n",
      "Epoch [19/3000]: Train loss: 37.3826, Valid loss: 36.9110\n",
      "Saving model with loss 36.911...\n",
      "Epoch [20/3000]: Train loss: 34.8084, Valid loss: 35.6710\n",
      "Saving model with loss 35.671...\n",
      "Epoch [21/3000]: Train loss: 34.0883, Valid loss: 37.0111\n",
      "Epoch [22/3000]: Train loss: 34.1022, Valid loss: 39.6392\n",
      "Epoch [23/3000]: Train loss: 33.8151, Valid loss: 39.3531\n",
      "Epoch [24/3000]: Train loss: 32.8632, Valid loss: 39.1087\n",
      "Epoch [25/3000]: Train loss: 33.8593, Valid loss: 36.6738\n",
      "Epoch [26/3000]: Train loss: 31.7733, Valid loss: 40.8847\n",
      "Epoch [27/3000]: Train loss: 30.6477, Valid loss: 27.4592\n",
      "Saving model with loss 27.459...\n",
      "Epoch [28/3000]: Train loss: 30.0241, Valid loss: 29.5080\n",
      "Epoch [29/3000]: Train loss: 28.4557, Valid loss: 43.8749\n",
      "Epoch [30/3000]: Train loss: 30.1386, Valid loss: 29.4323\n",
      "Epoch [31/3000]: Train loss: 29.0105, Valid loss: 52.1368\n",
      "Epoch [32/3000]: Train loss: 33.0826, Valid loss: 45.4835\n",
      "Epoch [33/3000]: Train loss: 38.5264, Valid loss: 52.1128\n",
      "Epoch [34/3000]: Train loss: 36.8397, Valid loss: 36.3261\n",
      "Epoch [35/3000]: Train loss: 32.9245, Valid loss: 26.5699\n",
      "Saving model with loss 26.570...\n",
      "Epoch [36/3000]: Train loss: 28.6259, Valid loss: 27.3638\n",
      "Epoch [37/3000]: Train loss: 25.5480, Valid loss: 24.0992\n",
      "Saving model with loss 24.099...\n",
      "Epoch [38/3000]: Train loss: 24.0120, Valid loss: 31.5593\n",
      "Epoch [39/3000]: Train loss: 21.8530, Valid loss: 22.0032\n",
      "Saving model with loss 22.003...\n",
      "Epoch [40/3000]: Train loss: 19.9099, Valid loss: 22.7382\n",
      "Epoch [41/3000]: Train loss: 18.6578, Valid loss: 17.3888\n",
      "Saving model with loss 17.389...\n",
      "Epoch [42/3000]: Train loss: 16.5442, Valid loss: 16.4034\n",
      "Saving model with loss 16.403...\n",
      "Epoch [43/3000]: Train loss: 14.0540, Valid loss: 15.9671\n",
      "Saving model with loss 15.967...\n",
      "Epoch [44/3000]: Train loss: 13.3075, Valid loss: 15.8558\n",
      "Saving model with loss 15.856...\n",
      "Epoch [45/3000]: Train loss: 49.3070, Valid loss: 149.5172\n",
      "Epoch [46/3000]: Train loss: 129.4772, Valid loss: 138.4679\n",
      "Epoch [47/3000]: Train loss: 131.3420, Valid loss: 146.5473\n",
      "Epoch [48/3000]: Train loss: 131.3630, Valid loss: 139.6968\n",
      "Epoch [49/3000]: Train loss: 128.8522, Valid loss: 149.2546\n",
      "Epoch [50/3000]: Train loss: 127.3496, Valid loss: 132.7013\n",
      "Epoch [51/3000]: Train loss: 125.2736, Valid loss: 146.4499\n",
      "Epoch [52/3000]: Train loss: 120.5087, Valid loss: 122.0330\n",
      "Epoch [53/3000]: Train loss: 73.8416, Valid loss: 20.5410\n",
      "Epoch [54/3000]: Train loss: 24.2779, Valid loss: 22.3900\n",
      "Epoch [55/3000]: Train loss: 24.0431, Valid loss: 19.8766\n",
      "Epoch [56/3000]: Train loss: 21.0696, Valid loss: 19.7221\n",
      "Epoch [57/3000]: Train loss: 16.7635, Valid loss: 21.9969\n",
      "Epoch [58/3000]: Train loss: 13.4819, Valid loss: 15.2077\n",
      "Saving model with loss 15.208...\n",
      "Epoch [59/3000]: Train loss: 12.0520, Valid loss: 9.5495\n",
      "Saving model with loss 9.550...\n",
      "Epoch [60/3000]: Train loss: 10.8626, Valid loss: 12.4066\n",
      "Epoch [61/3000]: Train loss: 9.9759, Valid loss: 7.0274\n",
      "Saving model with loss 7.027...\n",
      "Epoch [62/3000]: Train loss: 10.3948, Valid loss: 26.0115\n",
      "Epoch [63/3000]: Train loss: 19.0054, Valid loss: 16.9510\n",
      "Epoch [64/3000]: Train loss: 10.1857, Valid loss: 10.5289\n",
      "Epoch [65/3000]: Train loss: 8.8441, Valid loss: 7.2584\n",
      "Epoch [66/3000]: Train loss: 8.2976, Valid loss: 8.3588\n",
      "Epoch [67/3000]: Train loss: 10.1059, Valid loss: 11.7975\n",
      "Epoch [68/3000]: Train loss: 10.9252, Valid loss: 6.3254\n",
      "Saving model with loss 6.325...\n",
      "Epoch [69/3000]: Train loss: 8.0246, Valid loss: 10.5401\n",
      "Epoch [70/3000]: Train loss: 7.9587, Valid loss: 6.3123\n",
      "Saving model with loss 6.312...\n",
      "Epoch [71/3000]: Train loss: 6.5927, Valid loss: 6.2961\n",
      "Saving model with loss 6.296...\n",
      "Epoch [72/3000]: Train loss: 6.6915, Valid loss: 8.2811\n",
      "Epoch [73/3000]: Train loss: 9.3717, Valid loss: 10.3255\n",
      "Epoch [74/3000]: Train loss: 8.1851, Valid loss: 7.1692\n",
      "Epoch [75/3000]: Train loss: 7.1404, Valid loss: 8.0714\n",
      "Epoch [76/3000]: Train loss: 6.9150, Valid loss: 6.4155\n",
      "Epoch [77/3000]: Train loss: 6.7208, Valid loss: 10.3165\n",
      "Epoch [78/3000]: Train loss: 7.1160, Valid loss: 5.7866\n",
      "Saving model with loss 5.787...\n",
      "Epoch [79/3000]: Train loss: 6.1151, Valid loss: 7.6919\n",
      "Epoch [80/3000]: Train loss: 7.1875, Valid loss: 7.4080\n",
      "Epoch [81/3000]: Train loss: 6.6767, Valid loss: 8.7606\n",
      "Epoch [82/3000]: Train loss: 7.4383, Valid loss: 6.0118\n",
      "Epoch [83/3000]: Train loss: 6.4371, Valid loss: 5.2610\n",
      "Saving model with loss 5.261...\n",
      "Epoch [84/3000]: Train loss: 6.4921, Valid loss: 8.0682\n",
      "Epoch [85/3000]: Train loss: 6.6159, Valid loss: 4.9675\n",
      "Saving model with loss 4.968...\n",
      "Epoch [86/3000]: Train loss: 6.3874, Valid loss: 8.0452\n",
      "Epoch [87/3000]: Train loss: 7.6809, Valid loss: 6.5613\n",
      "Epoch [88/3000]: Train loss: 6.1583, Valid loss: 5.9455\n",
      "Epoch [89/3000]: Train loss: 5.6600, Valid loss: 6.4633\n",
      "Epoch [90/3000]: Train loss: 6.2454, Valid loss: 12.8560\n",
      "Epoch [91/3000]: Train loss: 10.8086, Valid loss: 5.8327\n",
      "Epoch [92/3000]: Train loss: 8.2095, Valid loss: 7.2888\n",
      "Epoch [93/3000]: Train loss: 7.2004, Valid loss: 9.3998\n",
      "Epoch [94/3000]: Train loss: 7.4580, Valid loss: 10.9604\n",
      "Epoch [95/3000]: Train loss: 8.4243, Valid loss: 12.3112\n",
      "Epoch [96/3000]: Train loss: 8.8904, Valid loss: 7.6597\n",
      "Epoch [97/3000]: Train loss: 6.6773, Valid loss: 7.1338\n",
      "Epoch [98/3000]: Train loss: 5.8168, Valid loss: 5.8404\n",
      "Epoch [99/3000]: Train loss: 5.5619, Valid loss: 4.8622\n",
      "Saving model with loss 4.862...\n",
      "Epoch [100/3000]: Train loss: 5.7243, Valid loss: 6.6771\n",
      "Epoch [101/3000]: Train loss: 5.7083, Valid loss: 5.3281\n",
      "Epoch [102/3000]: Train loss: 5.9740, Valid loss: 7.5056\n",
      "Epoch [103/3000]: Train loss: 7.0880, Valid loss: 5.5546\n",
      "Epoch [104/3000]: Train loss: 7.3842, Valid loss: 5.3035\n",
      "Epoch [105/3000]: Train loss: 6.0263, Valid loss: 7.6995\n",
      "Epoch [106/3000]: Train loss: 6.0855, Valid loss: 5.7703\n",
      "Epoch [107/3000]: Train loss: 6.3907, Valid loss: 6.7034\n",
      "Epoch [108/3000]: Train loss: 5.7339, Valid loss: 5.4798\n",
      "Epoch [109/3000]: Train loss: 6.2544, Valid loss: 7.3173\n",
      "Epoch [110/3000]: Train loss: 9.8695, Valid loss: 7.1687\n",
      "Epoch [111/3000]: Train loss: 10.4994, Valid loss: 6.3794\n",
      "Epoch [112/3000]: Train loss: 7.3664, Valid loss: 6.8377\n",
      "Epoch [113/3000]: Train loss: 6.2919, Valid loss: 9.9003\n",
      "Epoch [114/3000]: Train loss: 6.1847, Valid loss: 4.9157\n",
      "Epoch [115/3000]: Train loss: 5.8087, Valid loss: 5.6814\n",
      "Epoch [116/3000]: Train loss: 6.9914, Valid loss: 6.2328\n",
      "Epoch [117/3000]: Train loss: 5.8855, Valid loss: 5.3087\n",
      "Epoch [118/3000]: Train loss: 5.5962, Valid loss: 5.7202\n",
      "Epoch [119/3000]: Train loss: 5.4742, Valid loss: 5.2008\n",
      "Epoch [120/3000]: Train loss: 6.1595, Valid loss: 7.5374\n",
      "Epoch [121/3000]: Train loss: 6.6928, Valid loss: 7.2466\n",
      "Epoch [122/3000]: Train loss: 5.9824, Valid loss: 5.9288\n",
      "Epoch [123/3000]: Train loss: 5.3831, Valid loss: 4.4564\n",
      "Saving model with loss 4.456...\n",
      "Epoch [124/3000]: Train loss: 5.1935, Valid loss: 4.7920\n",
      "Epoch [125/3000]: Train loss: 5.8379, Valid loss: 6.1659\n",
      "Epoch [126/3000]: Train loss: 5.6957, Valid loss: 4.8638\n",
      "Epoch [127/3000]: Train loss: 5.1649, Valid loss: 6.1359\n",
      "Epoch [128/3000]: Train loss: 5.5045, Valid loss: 5.4736\n",
      "Epoch [129/3000]: Train loss: 5.8544, Valid loss: 5.0845\n",
      "Epoch [130/3000]: Train loss: 5.3543, Valid loss: 6.2359\n",
      "Epoch [131/3000]: Train loss: 6.4550, Valid loss: 5.0549\n",
      "Epoch [132/3000]: Train loss: 5.7350, Valid loss: 5.1326\n",
      "Epoch [133/3000]: Train loss: 5.5874, Valid loss: 6.2938\n",
      "Epoch [134/3000]: Train loss: 5.4130, Valid loss: 5.8834\n",
      "Epoch [135/3000]: Train loss: 5.9322, Valid loss: 6.6097\n",
      "Epoch [136/3000]: Train loss: 5.6055, Valid loss: 4.9203\n",
      "Epoch [137/3000]: Train loss: 5.1431, Valid loss: 4.9598\n",
      "Epoch [138/3000]: Train loss: 5.0805, Valid loss: 5.5271\n",
      "Epoch [139/3000]: Train loss: 5.1556, Valid loss: 6.2342\n",
      "Epoch [140/3000]: Train loss: 5.6759, Valid loss: 5.6006\n",
      "Epoch [141/3000]: Train loss: 5.7137, Valid loss: 5.5797\n",
      "Epoch [142/3000]: Train loss: 5.8395, Valid loss: 4.4724\n",
      "Epoch [143/3000]: Train loss: 5.6675, Valid loss: 5.3035\n",
      "Epoch [144/3000]: Train loss: 5.7312, Valid loss: 4.7853\n",
      "Epoch [145/3000]: Train loss: 5.1706, Valid loss: 6.0015\n",
      "Epoch [146/3000]: Train loss: 6.1923, Valid loss: 5.1134\n",
      "Epoch [147/3000]: Train loss: 5.3773, Valid loss: 4.5517\n",
      "Epoch [148/3000]: Train loss: 5.1209, Valid loss: 7.2538\n",
      "Epoch [149/3000]: Train loss: 5.7921, Valid loss: 6.0312\n",
      "Epoch [150/3000]: Train loss: 5.2603, Valid loss: 5.2084\n",
      "Epoch [151/3000]: Train loss: 5.1799, Valid loss: 5.0072\n",
      "Epoch [152/3000]: Train loss: 5.0511, Valid loss: 5.5323\n",
      "Epoch [153/3000]: Train loss: 5.3893, Valid loss: 7.0175\n",
      "Epoch [154/3000]: Train loss: 7.9001, Valid loss: 10.9181\n",
      "Epoch [155/3000]: Train loss: 6.3910, Valid loss: 6.1271\n",
      "Epoch [156/3000]: Train loss: 5.3429, Valid loss: 6.3097\n",
      "Epoch [157/3000]: Train loss: 5.1200, Valid loss: 5.6801\n",
      "Epoch [158/3000]: Train loss: 5.2131, Valid loss: 5.4610\n",
      "Epoch [159/3000]: Train loss: 5.4655, Valid loss: 5.8669\n",
      "Epoch [160/3000]: Train loss: 4.9499, Valid loss: 9.4359\n",
      "Epoch [161/3000]: Train loss: 5.9297, Valid loss: 4.9317\n",
      "Epoch [162/3000]: Train loss: 4.9206, Valid loss: 4.7039\n",
      "Epoch [163/3000]: Train loss: 4.9996, Valid loss: 6.8047\n",
      "Epoch [164/3000]: Train loss: 5.6733, Valid loss: 6.7625\n",
      "Epoch [165/3000]: Train loss: 5.7863, Valid loss: 5.8492\n",
      "Epoch [166/3000]: Train loss: 5.6951, Valid loss: 6.6619\n",
      "Epoch [167/3000]: Train loss: 5.8398, Valid loss: 4.5949\n",
      "Epoch [168/3000]: Train loss: 4.8548, Valid loss: 4.9318\n",
      "Epoch [169/3000]: Train loss: 4.8157, Valid loss: 5.2763\n",
      "Epoch [170/3000]: Train loss: 4.6400, Valid loss: 6.0825\n",
      "Epoch [171/3000]: Train loss: 5.0907, Valid loss: 4.4082\n",
      "Saving model with loss 4.408...\n",
      "Epoch [172/3000]: Train loss: 5.6358, Valid loss: 5.2864\n",
      "Epoch [173/3000]: Train loss: 4.7866, Valid loss: 8.6309\n",
      "Epoch [174/3000]: Train loss: 5.5810, Valid loss: 4.8639\n",
      "Epoch [175/3000]: Train loss: 4.8035, Valid loss: 6.1229\n",
      "Epoch [176/3000]: Train loss: 5.0588, Valid loss: 4.6308\n",
      "Epoch [177/3000]: Train loss: 4.7750, Valid loss: 5.2598\n",
      "Epoch [178/3000]: Train loss: 4.5320, Valid loss: 4.6478\n",
      "Epoch [179/3000]: Train loss: 4.8824, Valid loss: 6.0201\n",
      "Epoch [180/3000]: Train loss: 4.6506, Valid loss: 4.4891\n",
      "Epoch [181/3000]: Train loss: 4.7734, Valid loss: 6.7203\n",
      "Epoch [182/3000]: Train loss: 5.8521, Valid loss: 5.1003\n",
      "Epoch [183/3000]: Train loss: 6.7035, Valid loss: 5.6512\n",
      "Epoch [184/3000]: Train loss: 5.3060, Valid loss: 4.9356\n",
      "Epoch [185/3000]: Train loss: 4.9370, Valid loss: 4.8268\n",
      "Epoch [186/3000]: Train loss: 4.6878, Valid loss: 5.2142\n",
      "Epoch [187/3000]: Train loss: 4.8455, Valid loss: 4.5351\n",
      "Epoch [188/3000]: Train loss: 4.7576, Valid loss: 5.2883\n",
      "Epoch [189/3000]: Train loss: 4.4754, Valid loss: 5.9825\n",
      "Epoch [190/3000]: Train loss: 4.9566, Valid loss: 4.5672\n",
      "Epoch [191/3000]: Train loss: 5.1253, Valid loss: 5.6108\n",
      "Epoch [192/3000]: Train loss: 5.1771, Valid loss: 5.6727\n",
      "Epoch [193/3000]: Train loss: 4.7537, Valid loss: 3.9299\n",
      "Saving model with loss 3.930...\n",
      "Epoch [194/3000]: Train loss: 4.3664, Valid loss: 4.2257\n",
      "Epoch [195/3000]: Train loss: 4.3645, Valid loss: 4.5413\n",
      "Epoch [196/3000]: Train loss: 4.7540, Valid loss: 4.3615\n",
      "Epoch [197/3000]: Train loss: 4.6945, Valid loss: 5.4573\n",
      "Epoch [198/3000]: Train loss: 4.6690, Valid loss: 5.1553\n",
      "Epoch [199/3000]: Train loss: 4.5290, Valid loss: 4.6915\n",
      "Epoch [200/3000]: Train loss: 4.8274, Valid loss: 7.4370\n",
      "Epoch [201/3000]: Train loss: 6.4516, Valid loss: 5.3212\n",
      "Epoch [202/3000]: Train loss: 5.4682, Valid loss: 4.9695\n",
      "Epoch [203/3000]: Train loss: 4.5338, Valid loss: 4.4455\n",
      "Epoch [204/3000]: Train loss: 4.4987, Valid loss: 7.1670\n",
      "Epoch [205/3000]: Train loss: 5.4854, Valid loss: 4.5791\n",
      "Epoch [206/3000]: Train loss: 4.3527, Valid loss: 4.8272\n",
      "Epoch [207/3000]: Train loss: 4.4144, Valid loss: 5.3333\n",
      "Epoch [208/3000]: Train loss: 4.7377, Valid loss: 4.4797\n",
      "Epoch [209/3000]: Train loss: 4.8293, Valid loss: 4.1172\n",
      "Epoch [210/3000]: Train loss: 4.6476, Valid loss: 4.0876\n",
      "Epoch [211/3000]: Train loss: 4.4686, Valid loss: 4.0776\n",
      "Epoch [212/3000]: Train loss: 4.6834, Valid loss: 3.7577\n",
      "Saving model with loss 3.758...\n",
      "Epoch [213/3000]: Train loss: 5.6163, Valid loss: 3.7993\n",
      "Epoch [214/3000]: Train loss: 5.1222, Valid loss: 6.1255\n",
      "Epoch [215/3000]: Train loss: 5.1215, Valid loss: 4.5471\n",
      "Epoch [216/3000]: Train loss: 4.7922, Valid loss: 5.2413\n",
      "Epoch [217/3000]: Train loss: 5.4759, Valid loss: 6.0434\n",
      "Epoch [218/3000]: Train loss: 4.8699, Valid loss: 3.7187\n",
      "Saving model with loss 3.719...\n",
      "Epoch [219/3000]: Train loss: 4.5021, Valid loss: 4.3420\n",
      "Epoch [220/3000]: Train loss: 4.3628, Valid loss: 4.7843\n",
      "Epoch [221/3000]: Train loss: 4.6052, Valid loss: 5.1645\n",
      "Epoch [222/3000]: Train loss: 4.9171, Valid loss: 6.1050\n",
      "Epoch [223/3000]: Train loss: 4.7011, Valid loss: 5.2032\n",
      "Epoch [224/3000]: Train loss: 4.2010, Valid loss: 3.9647\n",
      "Epoch [225/3000]: Train loss: 4.1468, Valid loss: 4.5988\n",
      "Epoch [226/3000]: Train loss: 4.1319, Valid loss: 3.9653\n",
      "Epoch [227/3000]: Train loss: 4.2085, Valid loss: 4.1696\n",
      "Epoch [228/3000]: Train loss: 4.3622, Valid loss: 5.9322\n",
      "Epoch [229/3000]: Train loss: 4.7572, Valid loss: 4.4839\n",
      "Epoch [230/3000]: Train loss: 4.6141, Valid loss: 4.2834\n",
      "Epoch [231/3000]: Train loss: 4.7374, Valid loss: 5.2492\n",
      "Epoch [232/3000]: Train loss: 4.4937, Valid loss: 3.8249\n",
      "Epoch [233/3000]: Train loss: 4.4059, Valid loss: 4.3936\n",
      "Epoch [234/3000]: Train loss: 4.1442, Valid loss: 3.8216\n",
      "Epoch [235/3000]: Train loss: 4.2344, Valid loss: 4.1101\n",
      "Epoch [236/3000]: Train loss: 4.6019, Valid loss: 4.6176\n",
      "Epoch [237/3000]: Train loss: 5.8211, Valid loss: 5.5042\n",
      "Epoch [238/3000]: Train loss: 4.3919, Valid loss: 4.1111\n",
      "Epoch [239/3000]: Train loss: 4.1093, Valid loss: 4.8592\n",
      "Epoch [240/3000]: Train loss: 4.5405, Valid loss: 4.3447\n",
      "Epoch [241/3000]: Train loss: 4.1121, Valid loss: 3.9818\n",
      "Epoch [242/3000]: Train loss: 4.3035, Valid loss: 3.6905\n",
      "Saving model with loss 3.691...\n",
      "Epoch [243/3000]: Train loss: 4.1457, Valid loss: 4.6919\n",
      "Epoch [244/3000]: Train loss: 4.1557, Valid loss: 4.3005\n",
      "Epoch [245/3000]: Train loss: 4.6370, Valid loss: 3.9162\n",
      "Epoch [246/3000]: Train loss: 4.0811, Valid loss: 3.5619\n",
      "Saving model with loss 3.562...\n",
      "Epoch [247/3000]: Train loss: 4.1426, Valid loss: 4.9443\n",
      "Epoch [248/3000]: Train loss: 4.1405, Valid loss: 5.0389\n",
      "Epoch [249/3000]: Train loss: 4.5608, Valid loss: 4.4330\n",
      "Epoch [250/3000]: Train loss: 4.4353, Valid loss: 4.3507\n",
      "Epoch [251/3000]: Train loss: 4.2328, Valid loss: 4.0703\n",
      "Epoch [252/3000]: Train loss: 4.6212, Valid loss: 5.0854\n",
      "Epoch [253/3000]: Train loss: 4.6710, Valid loss: 4.2805\n",
      "Epoch [254/3000]: Train loss: 5.1314, Valid loss: 4.2285\n",
      "Epoch [255/3000]: Train loss: 4.2633, Valid loss: 4.0663\n",
      "Epoch [256/3000]: Train loss: 4.0212, Valid loss: 3.9533\n",
      "Epoch [257/3000]: Train loss: 3.9213, Valid loss: 4.5493\n",
      "Epoch [258/3000]: Train loss: 4.2254, Valid loss: 4.5962\n",
      "Epoch [259/3000]: Train loss: 4.1939, Valid loss: 4.2767\n",
      "Epoch [260/3000]: Train loss: 4.6972, Valid loss: 4.2994\n",
      "Epoch [261/3000]: Train loss: 4.0739, Valid loss: 4.6854\n",
      "Epoch [262/3000]: Train loss: 5.0987, Valid loss: 3.6211\n",
      "Epoch [263/3000]: Train loss: 4.8789, Valid loss: 6.1845\n",
      "Epoch [264/3000]: Train loss: 4.0893, Valid loss: 4.1419\n",
      "Epoch [265/3000]: Train loss: 3.9572, Valid loss: 4.1192\n",
      "Epoch [266/3000]: Train loss: 4.0918, Valid loss: 4.4728\n",
      "Epoch [267/3000]: Train loss: 3.8152, Valid loss: 3.8726\n",
      "Epoch [268/3000]: Train loss: 3.9615, Valid loss: 4.8842\n",
      "Epoch [269/3000]: Train loss: 4.1028, Valid loss: 4.5312\n",
      "Epoch [270/3000]: Train loss: 4.2415, Valid loss: 3.3436\n",
      "Saving model with loss 3.344...\n",
      "Epoch [271/3000]: Train loss: 3.7704, Valid loss: 4.0517\n",
      "Epoch [272/3000]: Train loss: 3.7370, Valid loss: 4.1114\n",
      "Epoch [273/3000]: Train loss: 3.9941, Valid loss: 4.3371\n",
      "Epoch [274/3000]: Train loss: 4.2254, Valid loss: 3.7883\n",
      "Epoch [275/3000]: Train loss: 3.7900, Valid loss: 3.5526\n",
      "Epoch [276/3000]: Train loss: 3.8745, Valid loss: 4.2282\n",
      "Epoch [277/3000]: Train loss: 3.8159, Valid loss: 4.9795\n",
      "Epoch [278/3000]: Train loss: 3.7620, Valid loss: 3.5728\n",
      "Epoch [279/3000]: Train loss: 3.7908, Valid loss: 4.0075\n",
      "Epoch [280/3000]: Train loss: 4.3387, Valid loss: 3.4196\n",
      "Epoch [281/3000]: Train loss: 4.0185, Valid loss: 3.7201\n",
      "Epoch [282/3000]: Train loss: 4.1470, Valid loss: 5.4657\n",
      "Epoch [283/3000]: Train loss: 6.7999, Valid loss: 6.5897\n",
      "Epoch [284/3000]: Train loss: 8.0619, Valid loss: 5.2450\n",
      "Epoch [285/3000]: Train loss: 5.5873, Valid loss: 4.4408\n",
      "Epoch [286/3000]: Train loss: 4.6026, Valid loss: 5.0781\n",
      "Epoch [287/3000]: Train loss: 4.2248, Valid loss: 3.4005\n",
      "Epoch [288/3000]: Train loss: 3.9428, Valid loss: 3.8142\n",
      "Epoch [289/3000]: Train loss: 3.8843, Valid loss: 4.8433\n",
      "Epoch [290/3000]: Train loss: 4.5301, Valid loss: 5.5398\n",
      "Epoch [291/3000]: Train loss: 5.5462, Valid loss: 3.2270\n",
      "Saving model with loss 3.227...\n",
      "Epoch [292/3000]: Train loss: 4.4855, Valid loss: 4.2727\n",
      "Epoch [293/3000]: Train loss: 4.3009, Valid loss: 5.4022\n",
      "Epoch [294/3000]: Train loss: 4.3204, Valid loss: 4.0215\n",
      "Epoch [295/3000]: Train loss: 4.4637, Valid loss: 3.4569\n",
      "Epoch [296/3000]: Train loss: 4.7027, Valid loss: 4.5046\n",
      "Epoch [297/3000]: Train loss: 4.2751, Valid loss: 4.9335\n",
      "Epoch [298/3000]: Train loss: 4.0351, Valid loss: 4.2846\n",
      "Epoch [299/3000]: Train loss: 3.9552, Valid loss: 4.4218\n",
      "Epoch [300/3000]: Train loss: 3.9330, Valid loss: 4.0426\n",
      "Epoch [301/3000]: Train loss: 3.9020, Valid loss: 5.3313\n",
      "Epoch [302/3000]: Train loss: 4.6198, Valid loss: 3.5257\n",
      "Epoch [303/3000]: Train loss: 3.7827, Valid loss: 3.1462\n",
      "Saving model with loss 3.146...\n",
      "Epoch [304/3000]: Train loss: 4.1180, Valid loss: 7.4584\n",
      "Epoch [305/3000]: Train loss: 4.5066, Valid loss: 3.1776\n",
      "Epoch [306/3000]: Train loss: 3.5382, Valid loss: 4.7583\n",
      "Epoch [307/3000]: Train loss: 3.8547, Valid loss: 3.7104\n",
      "Epoch [308/3000]: Train loss: 4.1649, Valid loss: 3.6706\n",
      "Epoch [309/3000]: Train loss: 5.1440, Valid loss: 5.9966\n",
      "Epoch [310/3000]: Train loss: 5.2944, Valid loss: 4.9750\n",
      "Epoch [311/3000]: Train loss: 4.9123, Valid loss: 9.4237\n",
      "Epoch [312/3000]: Train loss: 5.4895, Valid loss: 5.6365\n",
      "Epoch [313/3000]: Train loss: 3.8518, Valid loss: 3.5876\n",
      "Epoch [314/3000]: Train loss: 3.6322, Valid loss: 3.3687\n",
      "Epoch [315/3000]: Train loss: 3.6091, Valid loss: 4.5267\n",
      "Epoch [316/3000]: Train loss: 3.8085, Valid loss: 3.4311\n",
      "Epoch [317/3000]: Train loss: 4.5490, Valid loss: 5.4031\n",
      "Epoch [318/3000]: Train loss: 4.9440, Valid loss: 4.0787\n",
      "Epoch [319/3000]: Train loss: 3.5319, Valid loss: 3.7528\n",
      "Epoch [320/3000]: Train loss: 3.6187, Valid loss: 3.6411\n",
      "Epoch [321/3000]: Train loss: 3.4890, Valid loss: 6.1242\n",
      "Epoch [322/3000]: Train loss: 4.0075, Valid loss: 3.9842\n",
      "Epoch [323/3000]: Train loss: 4.1922, Valid loss: 4.7047\n",
      "Epoch [324/3000]: Train loss: 4.8333, Valid loss: 4.8853\n",
      "Epoch [325/3000]: Train loss: 4.0113, Valid loss: 3.4205\n",
      "Epoch [326/3000]: Train loss: 3.9498, Valid loss: 4.0744\n",
      "Epoch [327/3000]: Train loss: 3.8915, Valid loss: 3.2369\n",
      "Epoch [328/3000]: Train loss: 3.8381, Valid loss: 3.4772\n",
      "Epoch [329/3000]: Train loss: 4.4235, Valid loss: 5.0635\n",
      "Epoch [330/3000]: Train loss: 4.2783, Valid loss: 3.5456\n",
      "Epoch [331/3000]: Train loss: 3.6677, Valid loss: 3.7963\n",
      "Epoch [332/3000]: Train loss: 3.5498, Valid loss: 3.9069\n",
      "Epoch [333/3000]: Train loss: 3.8087, Valid loss: 6.9838\n",
      "Epoch [334/3000]: Train loss: 4.4515, Valid loss: 3.4020\n",
      "Epoch [335/3000]: Train loss: 4.1129, Valid loss: 4.7347\n",
      "Epoch [336/3000]: Train loss: 5.2710, Valid loss: 6.1921\n",
      "Epoch [337/3000]: Train loss: 4.3443, Valid loss: 3.5481\n",
      "Epoch [338/3000]: Train loss: 3.5238, Valid loss: 3.0453\n",
      "Saving model with loss 3.045...\n",
      "Epoch [339/3000]: Train loss: 3.5621, Valid loss: 3.2534\n",
      "Epoch [340/3000]: Train loss: 3.6174, Valid loss: 4.9619\n",
      "Epoch [341/3000]: Train loss: 3.8122, Valid loss: 3.5899\n",
      "Epoch [342/3000]: Train loss: 3.9676, Valid loss: 4.5616\n",
      "Epoch [343/3000]: Train loss: 3.8116, Valid loss: 3.2508\n",
      "Epoch [344/3000]: Train loss: 3.8774, Valid loss: 4.6699\n",
      "Epoch [345/3000]: Train loss: 3.9020, Valid loss: 5.7903\n",
      "Epoch [346/3000]: Train loss: 3.8539, Valid loss: 5.0083\n",
      "Epoch [347/3000]: Train loss: 3.4190, Valid loss: 4.0115\n",
      "Epoch [348/3000]: Train loss: 3.5405, Valid loss: 3.5053\n",
      "Epoch [349/3000]: Train loss: 3.8586, Valid loss: 3.7594\n",
      "Epoch [350/3000]: Train loss: 4.2250, Valid loss: 10.7757\n",
      "Epoch [351/3000]: Train loss: 6.2822, Valid loss: 3.1746\n",
      "Epoch [352/3000]: Train loss: 3.9421, Valid loss: 4.4141\n",
      "Epoch [353/3000]: Train loss: 3.6690, Valid loss: 4.4602\n",
      "Epoch [354/3000]: Train loss: 3.4451, Valid loss: 3.1030\n",
      "Epoch [355/3000]: Train loss: 3.5286, Valid loss: 2.8621\n",
      "Saving model with loss 2.862...\n",
      "Epoch [356/3000]: Train loss: 4.1759, Valid loss: 5.5173\n",
      "Epoch [357/3000]: Train loss: 4.6725, Valid loss: 3.3671\n",
      "Epoch [358/3000]: Train loss: 5.0926, Valid loss: 4.8698\n",
      "Epoch [359/3000]: Train loss: 4.0855, Valid loss: 4.1846\n",
      "Epoch [360/3000]: Train loss: 4.1304, Valid loss: 4.3982\n",
      "Epoch [361/3000]: Train loss: 3.6604, Valid loss: 3.4210\n",
      "Epoch [362/3000]: Train loss: 3.4698, Valid loss: 3.5171\n",
      "Epoch [363/3000]: Train loss: 4.2099, Valid loss: 4.3193\n",
      "Epoch [364/3000]: Train loss: 4.7390, Valid loss: 5.4525\n",
      "Epoch [365/3000]: Train loss: 4.9848, Valid loss: 5.0713\n",
      "Epoch [366/3000]: Train loss: 4.2218, Valid loss: 3.8592\n",
      "Epoch [367/3000]: Train loss: 3.4322, Valid loss: 3.2517\n",
      "Epoch [368/3000]: Train loss: 3.7499, Valid loss: 6.6252\n",
      "Epoch [369/3000]: Train loss: 3.7352, Valid loss: 3.2325\n",
      "Epoch [370/3000]: Train loss: 3.3009, Valid loss: 3.4413\n",
      "Epoch [371/3000]: Train loss: 3.2792, Valid loss: 3.5867\n",
      "Epoch [372/3000]: Train loss: 3.9351, Valid loss: 3.1240\n",
      "Epoch [373/3000]: Train loss: 3.5343, Valid loss: 3.3650\n",
      "Epoch [374/3000]: Train loss: 4.4156, Valid loss: 6.1688\n",
      "Epoch [375/3000]: Train loss: 4.8026, Valid loss: 4.0467\n",
      "Epoch [376/3000]: Train loss: 4.3986, Valid loss: 6.8019\n",
      "Epoch [377/3000]: Train loss: 5.0999, Valid loss: 3.9813\n",
      "Epoch [378/3000]: Train loss: 3.7855, Valid loss: 3.2135\n",
      "Epoch [379/3000]: Train loss: 4.8888, Valid loss: 4.0287\n",
      "Epoch [380/3000]: Train loss: 4.9393, Valid loss: 5.0328\n",
      "Epoch [381/3000]: Train loss: 3.3099, Valid loss: 2.6774\n",
      "Saving model with loss 2.677...\n",
      "Epoch [382/3000]: Train loss: 3.2483, Valid loss: 3.9800\n",
      "Epoch [383/3000]: Train loss: 3.1209, Valid loss: 3.1236\n",
      "Epoch [384/3000]: Train loss: 3.2659, Valid loss: 3.3763\n",
      "Epoch [385/3000]: Train loss: 3.3906, Valid loss: 3.3713\n",
      "Epoch [386/3000]: Train loss: 3.3342, Valid loss: 2.7582\n",
      "Epoch [387/3000]: Train loss: 3.0347, Valid loss: 2.8717\n",
      "Epoch [388/3000]: Train loss: 3.1847, Valid loss: 3.1764\n",
      "Epoch [389/3000]: Train loss: 2.9839, Valid loss: 2.8762\n",
      "Epoch [390/3000]: Train loss: 3.0878, Valid loss: 2.9685\n",
      "Epoch [391/3000]: Train loss: 3.1841, Valid loss: 6.9367\n",
      "Epoch [392/3000]: Train loss: 5.2674, Valid loss: 3.5073\n",
      "Epoch [393/3000]: Train loss: 3.6453, Valid loss: 3.2435\n",
      "Epoch [394/3000]: Train loss: 3.6182, Valid loss: 3.7146\n",
      "Epoch [395/3000]: Train loss: 3.2604, Valid loss: 3.6721\n",
      "Epoch [396/3000]: Train loss: 3.3595, Valid loss: 4.7349\n",
      "Epoch [397/3000]: Train loss: 3.5746, Valid loss: 4.4329\n",
      "Epoch [398/3000]: Train loss: 4.0566, Valid loss: 5.6045\n",
      "Epoch [399/3000]: Train loss: 4.4245, Valid loss: 4.6683\n",
      "Epoch [400/3000]: Train loss: 4.7900, Valid loss: 4.5730\n",
      "Epoch [401/3000]: Train loss: 3.9701, Valid loss: 4.2518\n",
      "Epoch [402/3000]: Train loss: 3.4056, Valid loss: 3.1042\n",
      "Epoch [403/3000]: Train loss: 3.1316, Valid loss: 3.2925\n",
      "Epoch [404/3000]: Train loss: 3.0173, Valid loss: 3.2068\n",
      "Epoch [405/3000]: Train loss: 3.2445, Valid loss: 2.9048\n",
      "Epoch [406/3000]: Train loss: 3.1637, Valid loss: 3.2136\n",
      "Epoch [407/3000]: Train loss: 2.9960, Valid loss: 3.2958\n",
      "Epoch [408/3000]: Train loss: 3.0572, Valid loss: 3.3108\n",
      "Epoch [409/3000]: Train loss: 4.1847, Valid loss: 5.1548\n",
      "Epoch [410/3000]: Train loss: 5.9300, Valid loss: 3.2833\n",
      "Epoch [411/3000]: Train loss: 6.3907, Valid loss: 5.9627\n",
      "Epoch [412/3000]: Train loss: 5.0378, Valid loss: 5.4600\n",
      "Epoch [413/3000]: Train loss: 4.4998, Valid loss: 3.0796\n",
      "Epoch [414/3000]: Train loss: 3.2522, Valid loss: 2.9008\n",
      "Epoch [415/3000]: Train loss: 2.9505, Valid loss: 3.0053\n",
      "Epoch [416/3000]: Train loss: 3.1079, Valid loss: 3.9193\n",
      "Epoch [417/3000]: Train loss: 3.1363, Valid loss: 3.6952\n",
      "Epoch [418/3000]: Train loss: 3.1351, Valid loss: 2.8957\n",
      "Epoch [419/3000]: Train loss: 3.1068, Valid loss: 4.5263\n",
      "Epoch [420/3000]: Train loss: 3.4393, Valid loss: 3.6923\n",
      "Epoch [421/3000]: Train loss: 5.0586, Valid loss: 3.0657\n",
      "Epoch [422/3000]: Train loss: 3.7112, Valid loss: 3.8144\n",
      "Epoch [423/3000]: Train loss: 3.6268, Valid loss: 2.8441\n",
      "Epoch [424/3000]: Train loss: 3.2332, Valid loss: 2.7738\n",
      "Epoch [425/3000]: Train loss: 3.0042, Valid loss: 3.2741\n",
      "Epoch [426/3000]: Train loss: 2.8711, Valid loss: 2.9244\n",
      "Epoch [427/3000]: Train loss: 3.0176, Valid loss: 3.2186\n",
      "Epoch [428/3000]: Train loss: 3.0862, Valid loss: 3.9489\n",
      "Epoch [429/3000]: Train loss: 3.7258, Valid loss: 3.4296\n",
      "Epoch [430/3000]: Train loss: 3.7124, Valid loss: 2.8192\n",
      "Epoch [431/3000]: Train loss: 2.9974, Valid loss: 2.8775\n",
      "Epoch [432/3000]: Train loss: 2.8439, Valid loss: 3.3771\n",
      "Epoch [433/3000]: Train loss: 2.9173, Valid loss: 3.5671\n",
      "Epoch [434/3000]: Train loss: 3.9294, Valid loss: 5.1737\n",
      "Epoch [435/3000]: Train loss: 3.2423, Valid loss: 4.8960\n",
      "Epoch [436/3000]: Train loss: 3.8418, Valid loss: 2.8257\n",
      "Epoch [437/3000]: Train loss: 3.7615, Valid loss: 3.2710\n",
      "Epoch [438/3000]: Train loss: 3.6318, Valid loss: 3.2483\n",
      "Epoch [439/3000]: Train loss: 2.9781, Valid loss: 3.2039\n",
      "Epoch [440/3000]: Train loss: 2.9615, Valid loss: 2.8460\n",
      "Epoch [441/3000]: Train loss: 2.9358, Valid loss: 3.1724\n",
      "Epoch [442/3000]: Train loss: 2.9524, Valid loss: 2.9386\n",
      "Epoch [443/3000]: Train loss: 2.9860, Valid loss: 3.1368\n",
      "Epoch [444/3000]: Train loss: 3.0890, Valid loss: 3.2344\n",
      "Epoch [445/3000]: Train loss: 3.0523, Valid loss: 3.1994\n",
      "Epoch [446/3000]: Train loss: 2.9450, Valid loss: 3.1493\n",
      "Epoch [447/3000]: Train loss: 3.6077, Valid loss: 2.8734\n",
      "Epoch [448/3000]: Train loss: 3.1886, Valid loss: 3.4385\n",
      "Epoch [449/3000]: Train loss: 2.8468, Valid loss: 4.1671\n",
      "Epoch [450/3000]: Train loss: 3.1572, Valid loss: 4.3721\n",
      "Epoch [451/3000]: Train loss: 3.2306, Valid loss: 3.5708\n",
      "Epoch [452/3000]: Train loss: 2.8554, Valid loss: 3.8132\n",
      "Epoch [453/3000]: Train loss: 3.0797, Valid loss: 3.0709\n",
      "Epoch [454/3000]: Train loss: 3.0562, Valid loss: 2.7122\n",
      "Epoch [455/3000]: Train loss: 2.9010, Valid loss: 2.9790\n",
      "Epoch [456/3000]: Train loss: 2.8523, Valid loss: 2.8228\n",
      "Epoch [457/3000]: Train loss: 2.7960, Valid loss: 3.3323\n",
      "Epoch [458/3000]: Train loss: 2.8064, Valid loss: 3.2252\n",
      "Epoch [459/3000]: Train loss: 2.9389, Valid loss: 2.7606\n",
      "Epoch [460/3000]: Train loss: 2.6634, Valid loss: 3.3062\n",
      "Epoch [461/3000]: Train loss: 3.6796, Valid loss: 3.1658\n",
      "Epoch [462/3000]: Train loss: 3.3295, Valid loss: 3.0609\n",
      "Epoch [463/3000]: Train loss: 3.0327, Valid loss: 3.2251\n",
      "Epoch [464/3000]: Train loss: 2.6988, Valid loss: 3.9697\n",
      "Epoch [465/3000]: Train loss: 3.2247, Valid loss: 4.2094\n",
      "Epoch [466/3000]: Train loss: 3.3611, Valid loss: 2.6969\n",
      "Epoch [467/3000]: Train loss: 2.8051, Valid loss: 3.0383\n",
      "Epoch [468/3000]: Train loss: 3.0207, Valid loss: 2.4217\n",
      "Saving model with loss 2.422...\n",
      "Epoch [469/3000]: Train loss: 2.8596, Valid loss: 2.7428\n",
      "Epoch [470/3000]: Train loss: 2.7432, Valid loss: 3.4306\n",
      "Epoch [471/3000]: Train loss: 3.1954, Valid loss: 3.5414\n",
      "Epoch [472/3000]: Train loss: 4.1384, Valid loss: 3.4793\n",
      "Epoch [473/3000]: Train loss: 4.9633, Valid loss: 3.6075\n",
      "Epoch [474/3000]: Train loss: 4.9279, Valid loss: 3.6754\n",
      "Epoch [475/3000]: Train loss: 3.2380, Valid loss: 2.8930\n",
      "Epoch [476/3000]: Train loss: 2.7997, Valid loss: 2.5319\n",
      "Epoch [477/3000]: Train loss: 3.0251, Valid loss: 4.6171\n",
      "Epoch [478/3000]: Train loss: 5.1406, Valid loss: 8.1461\n",
      "Epoch [479/3000]: Train loss: 5.2424, Valid loss: 4.2110\n",
      "Epoch [480/3000]: Train loss: 5.1230, Valid loss: 3.4402\n",
      "Epoch [481/3000]: Train loss: 4.1161, Valid loss: 3.1589\n",
      "Epoch [482/3000]: Train loss: 2.8851, Valid loss: 3.0400\n",
      "Epoch [483/3000]: Train loss: 2.8674, Valid loss: 2.7645\n",
      "Epoch [484/3000]: Train loss: 2.7058, Valid loss: 3.0691\n",
      "Epoch [485/3000]: Train loss: 2.6543, Valid loss: 2.7650\n",
      "Epoch [486/3000]: Train loss: 2.7995, Valid loss: 2.8292\n",
      "Epoch [487/3000]: Train loss: 2.8275, Valid loss: 2.3318\n",
      "Saving model with loss 2.332...\n",
      "Epoch [488/3000]: Train loss: 2.7358, Valid loss: 3.1901\n",
      "Epoch [489/3000]: Train loss: 2.8913, Valid loss: 2.9989\n",
      "Epoch [490/3000]: Train loss: 3.0043, Valid loss: 4.1771\n",
      "Epoch [491/3000]: Train loss: 3.3972, Valid loss: 5.5686\n",
      "Epoch [492/3000]: Train loss: 4.7749, Valid loss: 5.5671\n",
      "Epoch [493/3000]: Train loss: 3.9377, Valid loss: 4.5553\n",
      "Epoch [494/3000]: Train loss: 3.5140, Valid loss: 2.5208\n",
      "Epoch [495/3000]: Train loss: 3.0398, Valid loss: 2.7926\n",
      "Epoch [496/3000]: Train loss: 2.7420, Valid loss: 3.3797\n",
      "Epoch [497/3000]: Train loss: 2.7665, Valid loss: 2.9804\n",
      "Epoch [498/3000]: Train loss: 2.9290, Valid loss: 2.6400\n",
      "Epoch [499/3000]: Train loss: 2.6893, Valid loss: 2.9351\n",
      "Epoch [500/3000]: Train loss: 3.1622, Valid loss: 2.8667\n",
      "Epoch [501/3000]: Train loss: 2.8352, Valid loss: 3.5959\n",
      "Epoch [502/3000]: Train loss: 3.5624, Valid loss: 4.4156\n",
      "Epoch [503/3000]: Train loss: 3.7310, Valid loss: 4.4239\n",
      "Epoch [504/3000]: Train loss: 4.5471, Valid loss: 4.8584\n",
      "Epoch [505/3000]: Train loss: 3.2461, Valid loss: 2.7520\n",
      "Epoch [506/3000]: Train loss: 2.9398, Valid loss: 6.7734\n",
      "Epoch [507/3000]: Train loss: 3.5501, Valid loss: 2.7243\n",
      "Epoch [508/3000]: Train loss: 3.0970, Valid loss: 2.3827\n",
      "Epoch [509/3000]: Train loss: 3.2149, Valid loss: 3.1447\n",
      "Epoch [510/3000]: Train loss: 2.7023, Valid loss: 2.6027\n",
      "Epoch [511/3000]: Train loss: 4.3522, Valid loss: 5.2211\n",
      "Epoch [512/3000]: Train loss: 3.4703, Valid loss: 3.0385\n",
      "Epoch [513/3000]: Train loss: 3.3800, Valid loss: 3.4849\n",
      "Epoch [514/3000]: Train loss: 3.9903, Valid loss: 3.5730\n",
      "Epoch [515/3000]: Train loss: 3.8147, Valid loss: 3.5287\n",
      "Epoch [516/3000]: Train loss: 3.1184, Valid loss: 2.7265\n",
      "Epoch [517/3000]: Train loss: 2.6621, Valid loss: 3.3538\n",
      "Epoch [518/3000]: Train loss: 2.8366, Valid loss: 2.8691\n",
      "Epoch [519/3000]: Train loss: 3.1347, Valid loss: 4.0056\n",
      "Epoch [520/3000]: Train loss: 3.3049, Valid loss: 4.2118\n",
      "Epoch [521/3000]: Train loss: 2.9558, Valid loss: 3.6154\n",
      "Epoch [522/3000]: Train loss: 2.7542, Valid loss: 2.7001\n",
      "Epoch [523/3000]: Train loss: 2.7058, Valid loss: 2.5468\n",
      "Epoch [524/3000]: Train loss: 3.9192, Valid loss: 3.5237\n",
      "Epoch [525/3000]: Train loss: 3.8505, Valid loss: 5.1210\n",
      "Epoch [526/3000]: Train loss: 3.9117, Valid loss: 5.8285\n",
      "Epoch [527/3000]: Train loss: 4.7717, Valid loss: 4.4988\n",
      "Epoch [528/3000]: Train loss: 4.3072, Valid loss: 2.7062\n",
      "Epoch [529/3000]: Train loss: 4.0664, Valid loss: 3.7026\n",
      "Epoch [530/3000]: Train loss: 3.4192, Valid loss: 3.6399\n",
      "Epoch [531/3000]: Train loss: 3.6356, Valid loss: 3.1449\n",
      "Epoch [532/3000]: Train loss: 2.6960, Valid loss: 2.4308\n",
      "Epoch [533/3000]: Train loss: 2.6071, Valid loss: 3.2662\n",
      "Epoch [534/3000]: Train loss: 3.1590, Valid loss: 3.5190\n",
      "Epoch [535/3000]: Train loss: 3.1838, Valid loss: 4.7082\n",
      "Epoch [536/3000]: Train loss: 3.3974, Valid loss: 2.7837\n",
      "Epoch [537/3000]: Train loss: 2.9084, Valid loss: 2.9134\n",
      "Epoch [538/3000]: Train loss: 2.5383, Valid loss: 3.5836\n",
      "Epoch [539/3000]: Train loss: 3.2431, Valid loss: 3.2966\n",
      "Epoch [540/3000]: Train loss: 3.0663, Valid loss: 2.4096\n",
      "Epoch [541/3000]: Train loss: 2.7611, Valid loss: 3.4833\n",
      "Epoch [542/3000]: Train loss: 2.9427, Valid loss: 4.1015\n",
      "Epoch [543/3000]: Train loss: 2.8825, Valid loss: 2.7452\n",
      "Epoch [544/3000]: Train loss: 2.8323, Valid loss: 2.6192\n",
      "Epoch [545/3000]: Train loss: 3.3088, Valid loss: 2.4884\n",
      "Epoch [546/3000]: Train loss: 3.2575, Valid loss: 2.9651\n",
      "Epoch [547/3000]: Train loss: 3.4952, Valid loss: 4.1222\n",
      "Epoch [548/3000]: Train loss: 2.9153, Valid loss: 3.7728\n",
      "Epoch [549/3000]: Train loss: 3.1326, Valid loss: 3.4813\n",
      "Epoch [550/3000]: Train loss: 2.6291, Valid loss: 2.7581\n",
      "Epoch [551/3000]: Train loss: 3.1278, Valid loss: 3.7016\n",
      "Epoch [552/3000]: Train loss: 3.6618, Valid loss: 2.9768\n",
      "Epoch [553/3000]: Train loss: 3.0392, Valid loss: 2.6525\n",
      "Epoch [554/3000]: Train loss: 2.7809, Valid loss: 2.2113\n",
      "Saving model with loss 2.211...\n",
      "Epoch [555/3000]: Train loss: 2.7394, Valid loss: 4.3085\n",
      "Epoch [556/3000]: Train loss: 3.0637, Valid loss: 4.4067\n",
      "Epoch [557/3000]: Train loss: 3.1507, Valid loss: 3.5273\n",
      "Epoch [558/3000]: Train loss: 3.3073, Valid loss: 3.8739\n",
      "Epoch [559/3000]: Train loss: 4.2543, Valid loss: 5.5511\n",
      "Epoch [560/3000]: Train loss: 3.6856, Valid loss: 3.5684\n",
      "Epoch [561/3000]: Train loss: 3.3594, Valid loss: 2.3784\n",
      "Epoch [562/3000]: Train loss: 2.6928, Valid loss: 3.0257\n",
      "Epoch [563/3000]: Train loss: 3.1278, Valid loss: 3.4233\n",
      "Epoch [564/3000]: Train loss: 3.1857, Valid loss: 3.0008\n",
      "Epoch [565/3000]: Train loss: 2.5699, Valid loss: 2.9440\n",
      "Epoch [566/3000]: Train loss: 2.5434, Valid loss: 2.9761\n",
      "Epoch [567/3000]: Train loss: 2.5028, Valid loss: 3.0406\n",
      "Epoch [568/3000]: Train loss: 2.6728, Valid loss: 2.4538\n",
      "Epoch [569/3000]: Train loss: 2.6497, Valid loss: 2.1722\n",
      "Saving model with loss 2.172...\n",
      "Epoch [570/3000]: Train loss: 2.7385, Valid loss: 3.1750\n",
      "Epoch [571/3000]: Train loss: 2.3988, Valid loss: 2.4323\n",
      "Epoch [572/3000]: Train loss: 2.6211, Valid loss: 2.3567\n",
      "Epoch [573/3000]: Train loss: 2.6039, Valid loss: 3.6321\n",
      "Epoch [574/3000]: Train loss: 2.6851, Valid loss: 4.2433\n",
      "Epoch [575/3000]: Train loss: 3.1442, Valid loss: 4.3787\n",
      "Epoch [576/3000]: Train loss: 3.7757, Valid loss: 2.8595\n",
      "Epoch [577/3000]: Train loss: 3.3929, Valid loss: 3.3417\n",
      "Epoch [578/3000]: Train loss: 3.1399, Valid loss: 4.4514\n",
      "Epoch [579/3000]: Train loss: 4.8232, Valid loss: 6.9210\n",
      "Epoch [580/3000]: Train loss: 4.2838, Valid loss: 4.0577\n",
      "Epoch [581/3000]: Train loss: 3.1105, Valid loss: 3.0556\n",
      "Epoch [582/3000]: Train loss: 3.6476, Valid loss: 3.8423\n",
      "Epoch [583/3000]: Train loss: 3.3217, Valid loss: 2.7488\n",
      "Epoch [584/3000]: Train loss: 2.4865, Valid loss: 2.9175\n",
      "Epoch [585/3000]: Train loss: 2.6654, Valid loss: 4.4887\n",
      "Epoch [586/3000]: Train loss: 3.7640, Valid loss: 4.8482\n",
      "Epoch [587/3000]: Train loss: 3.1740, Valid loss: 3.0255\n",
      "Epoch [588/3000]: Train loss: 2.7774, Valid loss: 2.3582\n",
      "Epoch [589/3000]: Train loss: 2.6343, Valid loss: 3.6569\n",
      "Epoch [590/3000]: Train loss: 3.4982, Valid loss: 5.9024\n",
      "Epoch [591/3000]: Train loss: 4.2390, Valid loss: 3.6179\n",
      "Epoch [592/3000]: Train loss: 3.6613, Valid loss: 2.6097\n",
      "Epoch [593/3000]: Train loss: 4.1252, Valid loss: 3.4847\n",
      "Epoch [594/3000]: Train loss: 3.3891, Valid loss: 3.1235\n",
      "Epoch [595/3000]: Train loss: 2.9988, Valid loss: 2.8925\n",
      "Epoch [596/3000]: Train loss: 3.1159, Valid loss: 2.7147\n",
      "Epoch [597/3000]: Train loss: 2.8705, Valid loss: 3.6334\n",
      "Epoch [598/3000]: Train loss: 3.5180, Valid loss: 3.8557\n",
      "Epoch [599/3000]: Train loss: 4.0364, Valid loss: 4.0592\n",
      "Epoch [600/3000]: Train loss: 3.3614, Valid loss: 3.3098\n",
      "Epoch [601/3000]: Train loss: 2.6143, Valid loss: 3.1836\n",
      "Epoch [602/3000]: Train loss: 2.9694, Valid loss: 3.2095\n",
      "Epoch [603/3000]: Train loss: 3.3954, Valid loss: 2.5367\n",
      "Epoch [604/3000]: Train loss: 3.7350, Valid loss: 4.2177\n",
      "Epoch [605/3000]: Train loss: 3.5995, Valid loss: 5.4008\n",
      "Epoch [606/3000]: Train loss: 3.9571, Valid loss: 4.2253\n",
      "Epoch [607/3000]: Train loss: 3.0151, Valid loss: 2.6018\n",
      "Epoch [608/3000]: Train loss: 2.4233, Valid loss: 5.9212\n",
      "Epoch [609/3000]: Train loss: 2.7382, Valid loss: 3.2884\n",
      "Epoch [610/3000]: Train loss: 2.4302, Valid loss: 2.8443\n",
      "Epoch [611/3000]: Train loss: 3.0586, Valid loss: 3.8485\n",
      "Epoch [612/3000]: Train loss: 2.8799, Valid loss: 3.3424\n",
      "Epoch [613/3000]: Train loss: 3.0007, Valid loss: 2.6400\n",
      "Epoch [614/3000]: Train loss: 2.3637, Valid loss: 2.7550\n",
      "Epoch [615/3000]: Train loss: 2.3365, Valid loss: 2.8121\n",
      "Epoch [616/3000]: Train loss: 2.5454, Valid loss: 3.0904\n",
      "Epoch [617/3000]: Train loss: 2.5285, Valid loss: 2.6900\n",
      "Epoch [618/3000]: Train loss: 2.5120, Valid loss: 4.4800\n",
      "Epoch [619/3000]: Train loss: 3.2258, Valid loss: 3.8254\n",
      "Epoch [620/3000]: Train loss: 3.0110, Valid loss: 3.5344\n",
      "Epoch [621/3000]: Train loss: 2.9929, Valid loss: 4.2833\n",
      "Epoch [622/3000]: Train loss: 3.1761, Valid loss: 2.3241\n",
      "Epoch [623/3000]: Train loss: 2.7577, Valid loss: 2.4969\n",
      "Epoch [624/3000]: Train loss: 2.7508, Valid loss: 2.5861\n",
      "Epoch [625/3000]: Train loss: 2.6716, Valid loss: 2.3177\n",
      "Epoch [626/3000]: Train loss: 2.9797, Valid loss: 2.2645\n",
      "Epoch [627/3000]: Train loss: 3.4038, Valid loss: 3.5192\n",
      "Epoch [628/3000]: Train loss: 3.3185, Valid loss: 5.4585\n",
      "Epoch [629/3000]: Train loss: 3.2084, Valid loss: 2.6967\n",
      "Epoch [630/3000]: Train loss: 2.5892, Valid loss: 2.7211\n",
      "Epoch [631/3000]: Train loss: 2.5486, Valid loss: 3.0789\n",
      "Epoch [632/3000]: Train loss: 2.5164, Valid loss: 3.1367\n",
      "Epoch [633/3000]: Train loss: 3.0725, Valid loss: 2.7156\n",
      "Epoch [634/3000]: Train loss: 3.0822, Valid loss: 2.8133\n",
      "Epoch [635/3000]: Train loss: 2.6243, Valid loss: 2.2139\n",
      "Epoch [636/3000]: Train loss: 2.5606, Valid loss: 2.9458\n",
      "Epoch [637/3000]: Train loss: 2.4150, Valid loss: 2.7764\n",
      "Epoch [638/3000]: Train loss: 2.3969, Valid loss: 2.5314\n",
      "Epoch [639/3000]: Train loss: 2.7612, Valid loss: 2.8378\n",
      "Epoch [640/3000]: Train loss: 3.1973, Valid loss: 3.2979\n",
      "Epoch [641/3000]: Train loss: 3.5519, Valid loss: 2.3988\n",
      "Epoch [642/3000]: Train loss: 2.9897, Valid loss: 3.8828\n",
      "Epoch [643/3000]: Train loss: 3.8759, Valid loss: 2.2741\n",
      "Epoch [644/3000]: Train loss: 3.3998, Valid loss: 5.4506\n",
      "Epoch [645/3000]: Train loss: 3.4574, Valid loss: 2.9555\n",
      "Epoch [646/3000]: Train loss: 2.5364, Valid loss: 2.7676\n",
      "Epoch [647/3000]: Train loss: 2.3448, Valid loss: 2.3749\n",
      "Epoch [648/3000]: Train loss: 2.6465, Valid loss: 3.5696\n",
      "Epoch [649/3000]: Train loss: 3.1064, Valid loss: 5.3109\n",
      "Epoch [650/3000]: Train loss: 3.5223, Valid loss: 3.0861\n",
      "Epoch [651/3000]: Train loss: 3.0381, Valid loss: 2.6133\n",
      "Epoch [652/3000]: Train loss: 2.9969, Valid loss: 2.3774\n",
      "Epoch [653/3000]: Train loss: 2.2678, Valid loss: 2.1620\n",
      "Saving model with loss 2.162...\n",
      "Epoch [654/3000]: Train loss: 2.3280, Valid loss: 2.4808\n",
      "Epoch [655/3000]: Train loss: 2.4460, Valid loss: 2.3368\n",
      "Epoch [656/3000]: Train loss: 2.6576, Valid loss: 3.6743\n",
      "Epoch [657/3000]: Train loss: 2.8642, Valid loss: 2.2560\n",
      "Epoch [658/3000]: Train loss: 2.3867, Valid loss: 2.5403\n",
      "Epoch [659/3000]: Train loss: 2.7534, Valid loss: 2.3425\n",
      "Epoch [660/3000]: Train loss: 2.5795, Valid loss: 2.4050\n",
      "Epoch [661/3000]: Train loss: 2.6821, Valid loss: 2.4376\n",
      "Epoch [662/3000]: Train loss: 2.6674, Valid loss: 2.9785\n",
      "Epoch [663/3000]: Train loss: 2.7192, Valid loss: 3.7357\n",
      "Epoch [664/3000]: Train loss: 2.8597, Valid loss: 2.9414\n",
      "Epoch [665/3000]: Train loss: 2.6450, Valid loss: 2.4143\n",
      "Epoch [666/3000]: Train loss: 3.0854, Valid loss: 2.5721\n",
      "Epoch [667/3000]: Train loss: 2.9787, Valid loss: 2.4357\n",
      "Epoch [668/3000]: Train loss: 2.8405, Valid loss: 4.2509\n",
      "Epoch [669/3000]: Train loss: 3.3913, Valid loss: 6.2237\n",
      "Epoch [670/3000]: Train loss: 4.2319, Valid loss: 3.5497\n",
      "Epoch [671/3000]: Train loss: 4.3870, Valid loss: 3.1442\n",
      "Epoch [672/3000]: Train loss: 2.9182, Valid loss: 3.5371\n",
      "Epoch [673/3000]: Train loss: 3.0991, Valid loss: 3.0767\n",
      "Epoch [674/3000]: Train loss: 2.6668, Valid loss: 2.8284\n",
      "Epoch [675/3000]: Train loss: 2.7804, Valid loss: 2.3294\n",
      "Epoch [676/3000]: Train loss: 2.5828, Valid loss: 4.4223\n",
      "Epoch [677/3000]: Train loss: 3.0779, Valid loss: 3.4405\n",
      "Epoch [678/3000]: Train loss: 2.8698, Valid loss: 2.7356\n",
      "Epoch [679/3000]: Train loss: 3.4839, Valid loss: 4.0080\n",
      "Epoch [680/3000]: Train loss: 4.3146, Valid loss: 6.7712\n",
      "Epoch [681/3000]: Train loss: 5.3314, Valid loss: 5.5761\n",
      "Epoch [682/3000]: Train loss: 4.6457, Valid loss: 3.7270\n",
      "Epoch [683/3000]: Train loss: 3.3741, Valid loss: 2.7271\n",
      "Epoch [684/3000]: Train loss: 2.5365, Valid loss: 3.0908\n",
      "Epoch [685/3000]: Train loss: 2.6019, Valid loss: 2.5646\n",
      "Epoch [686/3000]: Train loss: 2.5104, Valid loss: 2.5276\n",
      "Epoch [687/3000]: Train loss: 2.3075, Valid loss: 3.3834\n",
      "Epoch [688/3000]: Train loss: 2.5287, Valid loss: 3.0470\n",
      "Epoch [689/3000]: Train loss: 4.1649, Valid loss: 15.4099\n",
      "Epoch [690/3000]: Train loss: 9.9084, Valid loss: 2.3750\n",
      "Epoch [691/3000]: Train loss: 3.8767, Valid loss: 3.9624\n",
      "Epoch [692/3000]: Train loss: 3.7302, Valid loss: 4.0355\n",
      "Epoch [693/3000]: Train loss: 2.5823, Valid loss: 2.8553\n",
      "Epoch [694/3000]: Train loss: 2.4741, Valid loss: 2.6642\n",
      "Epoch [695/3000]: Train loss: 2.5938, Valid loss: 2.1610\n",
      "Saving model with loss 2.161...\n",
      "Epoch [696/3000]: Train loss: 2.2939, Valid loss: 2.6674\n",
      "Epoch [697/3000]: Train loss: 2.2172, Valid loss: 2.2452\n",
      "Epoch [698/3000]: Train loss: 3.0584, Valid loss: 3.4218\n",
      "Epoch [699/3000]: Train loss: 2.9363, Valid loss: 2.3653\n",
      "Epoch [700/3000]: Train loss: 3.4304, Valid loss: 3.5587\n",
      "Epoch [701/3000]: Train loss: 3.9518, Valid loss: 4.3279\n",
      "Epoch [702/3000]: Train loss: 4.0527, Valid loss: 2.5333\n",
      "Epoch [703/3000]: Train loss: 3.2607, Valid loss: 4.9486\n",
      "Epoch [704/3000]: Train loss: 3.3573, Valid loss: 2.6625\n",
      "Epoch [705/3000]: Train loss: 2.4360, Valid loss: 2.2969\n",
      "Epoch [706/3000]: Train loss: 2.2608, Valid loss: 2.8878\n",
      "Epoch [707/3000]: Train loss: 2.6518, Valid loss: 2.8562\n",
      "Epoch [708/3000]: Train loss: 2.3749, Valid loss: 2.8510\n",
      "Epoch [709/3000]: Train loss: 2.4894, Valid loss: 2.9262\n",
      "Epoch [710/3000]: Train loss: 2.7044, Valid loss: 2.2537\n",
      "Epoch [711/3000]: Train loss: 2.9302, Valid loss: 3.7080\n",
      "Epoch [712/3000]: Train loss: 3.4576, Valid loss: 6.7690\n",
      "Epoch [713/3000]: Train loss: 3.7984, Valid loss: 2.8263\n",
      "Epoch [714/3000]: Train loss: 2.8807, Valid loss: 2.4997\n",
      "Epoch [715/3000]: Train loss: 2.7977, Valid loss: 2.1151\n",
      "Saving model with loss 2.115...\n",
      "Epoch [716/3000]: Train loss: 2.5374, Valid loss: 2.3586\n",
      "Epoch [717/3000]: Train loss: 2.5713, Valid loss: 3.3670\n",
      "Epoch [718/3000]: Train loss: 2.6938, Valid loss: 3.1349\n",
      "Epoch [719/3000]: Train loss: 2.3751, Valid loss: 3.7210\n",
      "Epoch [720/3000]: Train loss: 3.2620, Valid loss: 4.5306\n",
      "Epoch [721/3000]: Train loss: 3.1951, Valid loss: 4.4388\n",
      "Epoch [722/3000]: Train loss: 4.1066, Valid loss: 4.6855\n",
      "Epoch [723/3000]: Train loss: 4.0176, Valid loss: 3.0331\n",
      "Epoch [724/3000]: Train loss: 2.7070, Valid loss: 2.9369\n",
      "Epoch [725/3000]: Train loss: 2.4112, Valid loss: 2.4592\n",
      "Epoch [726/3000]: Train loss: 2.4157, Valid loss: 2.3454\n",
      "Epoch [727/3000]: Train loss: 2.2550, Valid loss: 2.6191\n",
      "Epoch [728/3000]: Train loss: 2.5388, Valid loss: 2.6909\n",
      "Epoch [729/3000]: Train loss: 2.6265, Valid loss: 5.3097\n",
      "Epoch [730/3000]: Train loss: 3.1522, Valid loss: 2.7220\n",
      "Epoch [731/3000]: Train loss: 2.4185, Valid loss: 2.7074\n",
      "Epoch [732/3000]: Train loss: 2.3606, Valid loss: 2.1290\n",
      "Epoch [733/3000]: Train loss: 2.2351, Valid loss: 2.4883\n",
      "Epoch [734/3000]: Train loss: 2.3596, Valid loss: 2.7137\n",
      "Epoch [735/3000]: Train loss: 2.5131, Valid loss: 3.4871\n",
      "Epoch [736/3000]: Train loss: 2.3897, Valid loss: 3.0754\n",
      "Epoch [737/3000]: Train loss: 3.4145, Valid loss: 7.7158\n",
      "Epoch [738/3000]: Train loss: 4.0021, Valid loss: 2.5727\n",
      "Epoch [739/3000]: Train loss: 3.0636, Valid loss: 3.1410\n",
      "Epoch [740/3000]: Train loss: 2.7738, Valid loss: 2.6805\n",
      "Epoch [741/3000]: Train loss: 2.4197, Valid loss: 2.5069\n",
      "Epoch [742/3000]: Train loss: 2.2842, Valid loss: 3.1243\n",
      "Epoch [743/3000]: Train loss: 2.6369, Valid loss: 2.5920\n",
      "Epoch [744/3000]: Train loss: 2.4565, Valid loss: 2.4874\n",
      "Epoch [745/3000]: Train loss: 2.3234, Valid loss: 3.2447\n",
      "Epoch [746/3000]: Train loss: 3.3832, Valid loss: 5.6241\n",
      "Epoch [747/3000]: Train loss: 3.6075, Valid loss: 2.4062\n",
      "Epoch [748/3000]: Train loss: 2.6673, Valid loss: 2.7963\n",
      "Epoch [749/3000]: Train loss: 2.3610, Valid loss: 2.3596\n",
      "Epoch [750/3000]: Train loss: 2.1934, Valid loss: 2.5890\n",
      "Epoch [751/3000]: Train loss: 2.3026, Valid loss: 2.1174\n",
      "Epoch [752/3000]: Train loss: 2.4003, Valid loss: 2.3931\n",
      "Epoch [753/3000]: Train loss: 2.3238, Valid loss: 2.4787\n",
      "Epoch [754/3000]: Train loss: 2.6767, Valid loss: 2.0743\n",
      "Saving model with loss 2.074...\n",
      "Epoch [755/3000]: Train loss: 2.1951, Valid loss: 3.2787\n",
      "Epoch [756/3000]: Train loss: 2.3358, Valid loss: 2.1906\n",
      "Epoch [757/3000]: Train loss: 2.2992, Valid loss: 2.4482\n",
      "Epoch [758/3000]: Train loss: 2.2407, Valid loss: 2.4914\n",
      "Epoch [759/3000]: Train loss: 2.2092, Valid loss: 2.8785\n",
      "Epoch [760/3000]: Train loss: 2.2640, Valid loss: 3.5448\n",
      "Epoch [761/3000]: Train loss: 3.2043, Valid loss: 5.1018\n",
      "Epoch [762/3000]: Train loss: 3.4860, Valid loss: 4.6037\n",
      "Epoch [763/3000]: Train loss: 3.1395, Valid loss: 4.2800\n",
      "Epoch [764/3000]: Train loss: 3.1841, Valid loss: 2.5330\n",
      "Epoch [765/3000]: Train loss: 2.2803, Valid loss: 2.7830\n",
      "Epoch [766/3000]: Train loss: 2.2570, Valid loss: 2.4992\n",
      "Epoch [767/3000]: Train loss: 2.3782, Valid loss: 2.7676\n",
      "Epoch [768/3000]: Train loss: 2.2115, Valid loss: 2.1624\n",
      "Epoch [769/3000]: Train loss: 2.2002, Valid loss: 2.2016\n",
      "Epoch [770/3000]: Train loss: 2.5107, Valid loss: 2.7942\n",
      "Epoch [771/3000]: Train loss: 2.8737, Valid loss: 5.1335\n",
      "Epoch [772/3000]: Train loss: 3.0173, Valid loss: 3.6000\n",
      "Epoch [773/3000]: Train loss: 2.3788, Valid loss: 2.9332\n",
      "Epoch [774/3000]: Train loss: 2.9489, Valid loss: 3.2066\n",
      "Epoch [775/3000]: Train loss: 2.6306, Valid loss: 4.7830\n",
      "Epoch [776/3000]: Train loss: 3.1412, Valid loss: 3.6048\n",
      "Epoch [777/3000]: Train loss: 2.3640, Valid loss: 2.2763\n",
      "Epoch [778/3000]: Train loss: 2.2728, Valid loss: 2.2019\n",
      "Epoch [779/3000]: Train loss: 2.1596, Valid loss: 2.3433\n",
      "Epoch [780/3000]: Train loss: 2.2833, Valid loss: 2.5808\n",
      "Epoch [781/3000]: Train loss: 2.4334, Valid loss: 4.0629\n",
      "Epoch [782/3000]: Train loss: 3.0749, Valid loss: 3.4360\n",
      "Epoch [783/3000]: Train loss: 2.8688, Valid loss: 6.2229\n",
      "Epoch [784/3000]: Train loss: 4.3244, Valid loss: 13.3558\n",
      "Epoch [785/3000]: Train loss: 9.4055, Valid loss: 2.6459\n",
      "Epoch [786/3000]: Train loss: 5.9582, Valid loss: 3.7514\n",
      "Epoch [787/3000]: Train loss: 4.0442, Valid loss: 4.2020\n",
      "Epoch [788/3000]: Train loss: 3.0537, Valid loss: 3.0840\n",
      "Epoch [789/3000]: Train loss: 2.5822, Valid loss: 2.2792\n",
      "Epoch [790/3000]: Train loss: 2.4313, Valid loss: 2.5002\n",
      "Epoch [791/3000]: Train loss: 2.2359, Valid loss: 2.8345\n",
      "Epoch [792/3000]: Train loss: 2.1387, Valid loss: 2.5382\n",
      "Epoch [793/3000]: Train loss: 2.1759, Valid loss: 3.1358\n",
      "Epoch [794/3000]: Train loss: 2.9686, Valid loss: 7.6173\n",
      "Epoch [795/3000]: Train loss: 4.8730, Valid loss: 4.8965\n",
      "Epoch [796/3000]: Train loss: 3.7140, Valid loss: 3.4003\n",
      "Epoch [797/3000]: Train loss: 2.7574, Valid loss: 3.9717\n",
      "Epoch [798/3000]: Train loss: 2.5649, Valid loss: 2.1465\n",
      "Epoch [799/3000]: Train loss: 2.2058, Valid loss: 2.5511\n",
      "Epoch [800/3000]: Train loss: 2.2291, Valid loss: 2.8881\n",
      "Epoch [801/3000]: Train loss: 2.4369, Valid loss: 3.5810\n",
      "Epoch [802/3000]: Train loss: 2.5004, Valid loss: 3.0743\n",
      "Epoch [803/3000]: Train loss: 2.6857, Valid loss: 3.4327\n",
      "Epoch [804/3000]: Train loss: 2.5533, Valid loss: 2.9832\n",
      "Epoch [805/3000]: Train loss: 2.3358, Valid loss: 2.1365\n",
      "Epoch [806/3000]: Train loss: 2.3068, Valid loss: 2.5398\n",
      "Epoch [807/3000]: Train loss: 2.5869, Valid loss: 2.8276\n",
      "Epoch [808/3000]: Train loss: 2.6623, Valid loss: 3.3486\n",
      "Epoch [809/3000]: Train loss: 3.3098, Valid loss: 5.1098\n",
      "Epoch [810/3000]: Train loss: 4.3027, Valid loss: 3.3783\n",
      "Epoch [811/3000]: Train loss: 3.3119, Valid loss: 4.5114\n",
      "Epoch [812/3000]: Train loss: 3.6877, Valid loss: 4.8441\n",
      "Epoch [813/3000]: Train loss: 3.5243, Valid loss: 4.7050\n",
      "Epoch [814/3000]: Train loss: 4.1420, Valid loss: 2.0830\n",
      "Epoch [815/3000]: Train loss: 3.3112, Valid loss: 4.8867\n",
      "Epoch [816/3000]: Train loss: 3.5310, Valid loss: 3.1574\n",
      "Epoch [817/3000]: Train loss: 2.5890, Valid loss: 2.4073\n",
      "Epoch [818/3000]: Train loss: 2.3616, Valid loss: 3.0312\n",
      "Epoch [819/3000]: Train loss: 2.4276, Valid loss: 2.6280\n",
      "Epoch [820/3000]: Train loss: 2.2550, Valid loss: 2.8250\n",
      "Epoch [821/3000]: Train loss: 2.3465, Valid loss: 2.2686\n",
      "Epoch [822/3000]: Train loss: 2.4715, Valid loss: 2.5361\n",
      "Epoch [823/3000]: Train loss: 2.7009, Valid loss: 2.4393\n",
      "Epoch [824/3000]: Train loss: 2.1033, Valid loss: 2.7427\n",
      "Epoch [825/3000]: Train loss: 2.1865, Valid loss: 2.6302\n",
      "Epoch [826/3000]: Train loss: 2.1482, Valid loss: 2.8862\n",
      "Epoch [827/3000]: Train loss: 2.9540, Valid loss: 2.8493\n",
      "Epoch [828/3000]: Train loss: 2.3059, Valid loss: 2.8877\n",
      "Epoch [829/3000]: Train loss: 2.6696, Valid loss: 4.0802\n",
      "Epoch [830/3000]: Train loss: 2.3712, Valid loss: 2.4408\n",
      "Epoch [831/3000]: Train loss: 2.3376, Valid loss: 2.3265\n",
      "Epoch [832/3000]: Train loss: 2.2983, Valid loss: 2.1072\n",
      "Epoch [833/3000]: Train loss: 2.5664, Valid loss: 3.1690\n",
      "Epoch [834/3000]: Train loss: 2.5609, Valid loss: 2.2907\n",
      "Epoch [835/3000]: Train loss: 2.6052, Valid loss: 2.8354\n",
      "Epoch [836/3000]: Train loss: 2.2911, Valid loss: 2.2476\n",
      "Epoch [837/3000]: Train loss: 2.1151, Valid loss: 2.4924\n",
      "Epoch [838/3000]: Train loss: 2.2752, Valid loss: 2.3607\n",
      "Epoch [839/3000]: Train loss: 2.1072, Valid loss: 2.0801\n",
      "Epoch [840/3000]: Train loss: 2.3596, Valid loss: 2.6961\n",
      "Epoch [841/3000]: Train loss: 2.9077, Valid loss: 2.0845\n",
      "Epoch [842/3000]: Train loss: 2.3271, Valid loss: 2.9760\n",
      "Epoch [843/3000]: Train loss: 2.2963, Valid loss: 2.0881\n",
      "Epoch [844/3000]: Train loss: 2.0790, Valid loss: 2.4224\n",
      "Epoch [845/3000]: Train loss: 2.2909, Valid loss: 2.4020\n",
      "Epoch [846/3000]: Train loss: 3.3479, Valid loss: 2.4274\n",
      "Epoch [847/3000]: Train loss: 4.2056, Valid loss: 2.4684\n",
      "Epoch [848/3000]: Train loss: 3.3450, Valid loss: 4.5921\n",
      "Epoch [849/3000]: Train loss: 2.6221, Valid loss: 2.3086\n",
      "Epoch [850/3000]: Train loss: 2.2405, Valid loss: 4.7721\n",
      "Epoch [851/3000]: Train loss: 3.3427, Valid loss: 2.8570\n",
      "Epoch [852/3000]: Train loss: 2.5730, Valid loss: 2.2672\n",
      "Epoch [853/3000]: Train loss: 2.4366, Valid loss: 2.8171\n",
      "Epoch [854/3000]: Train loss: 2.4658, Valid loss: 3.2143\n",
      "Epoch [855/3000]: Train loss: 2.5087, Valid loss: 2.4725\n",
      "Epoch [856/3000]: Train loss: 2.3288, Valid loss: 2.4885\n",
      "Epoch [857/3000]: Train loss: 2.6512, Valid loss: 2.2300\n",
      "Epoch [858/3000]: Train loss: 2.4811, Valid loss: 1.8738\n",
      "Saving model with loss 1.874...\n",
      "Epoch [859/3000]: Train loss: 2.2896, Valid loss: 2.2099\n",
      "Epoch [860/3000]: Train loss: 2.2031, Valid loss: 2.1416\n",
      "Epoch [861/3000]: Train loss: 2.2958, Valid loss: 2.5903\n",
      "Epoch [862/3000]: Train loss: 2.5036, Valid loss: 2.5444\n",
      "Epoch [863/3000]: Train loss: 2.1875, Valid loss: 2.4640\n",
      "Epoch [864/3000]: Train loss: 2.1128, Valid loss: 3.1720\n",
      "Epoch [865/3000]: Train loss: 2.8527, Valid loss: 3.0184\n",
      "Epoch [866/3000]: Train loss: 2.8212, Valid loss: 2.3994\n",
      "Epoch [867/3000]: Train loss: 2.4992, Valid loss: 6.8272\n",
      "Epoch [868/3000]: Train loss: 3.8083, Valid loss: 2.9174\n",
      "Epoch [869/3000]: Train loss: 2.4168, Valid loss: 3.2207\n",
      "Epoch [870/3000]: Train loss: 2.3820, Valid loss: 2.0787\n",
      "Epoch [871/3000]: Train loss: 2.3705, Valid loss: 2.3277\n",
      "Epoch [872/3000]: Train loss: 2.2144, Valid loss: 1.9259\n",
      "Epoch [873/3000]: Train loss: 2.1671, Valid loss: 3.5116\n",
      "Epoch [874/3000]: Train loss: 2.6008, Valid loss: 2.5943\n",
      "Epoch [875/3000]: Train loss: 2.5754, Valid loss: 2.0355\n",
      "Epoch [876/3000]: Train loss: 2.0856, Valid loss: 2.6217\n",
      "Epoch [877/3000]: Train loss: 2.3121, Valid loss: 2.3527\n",
      "Epoch [878/3000]: Train loss: 2.0736, Valid loss: 2.2719\n",
      "Epoch [879/3000]: Train loss: 2.0939, Valid loss: 3.7329\n",
      "Epoch [880/3000]: Train loss: 2.2756, Valid loss: 1.9863\n",
      "Epoch [881/3000]: Train loss: 2.2350, Valid loss: 2.4868\n",
      "Epoch [882/3000]: Train loss: 2.4337, Valid loss: 3.2923\n",
      "Epoch [883/3000]: Train loss: 4.1027, Valid loss: 6.2412\n",
      "Epoch [884/3000]: Train loss: 3.3893, Valid loss: 2.7957\n",
      "Epoch [885/3000]: Train loss: 2.2385, Valid loss: 2.8482\n",
      "Epoch [886/3000]: Train loss: 2.4231, Valid loss: 2.4042\n",
      "Epoch [887/3000]: Train loss: 2.3169, Valid loss: 2.7869\n",
      "Epoch [888/3000]: Train loss: 2.8908, Valid loss: 3.5713\n",
      "Epoch [889/3000]: Train loss: 2.5552, Valid loss: 2.0312\n",
      "Epoch [890/3000]: Train loss: 2.1977, Valid loss: 2.5969\n",
      "Epoch [891/3000]: Train loss: 2.0989, Valid loss: 2.4059\n",
      "Epoch [892/3000]: Train loss: 2.2862, Valid loss: 2.7520\n",
      "Epoch [893/3000]: Train loss: 2.7971, Valid loss: 2.3716\n",
      "Epoch [894/3000]: Train loss: 2.1142, Valid loss: 2.7071\n",
      "Epoch [895/3000]: Train loss: 2.3284, Valid loss: 2.1885\n",
      "Epoch [896/3000]: Train loss: 2.1246, Valid loss: 2.5373\n",
      "Epoch [897/3000]: Train loss: 2.1485, Valid loss: 2.9615\n",
      "Epoch [898/3000]: Train loss: 2.1539, Valid loss: 2.1865\n",
      "Epoch [899/3000]: Train loss: 2.4064, Valid loss: 2.0683\n",
      "Epoch [900/3000]: Train loss: 2.6053, Valid loss: 3.0800\n",
      "Epoch [901/3000]: Train loss: 2.1995, Valid loss: 2.3545\n",
      "Epoch [902/3000]: Train loss: 2.2507, Valid loss: 2.2459\n",
      "Epoch [903/3000]: Train loss: 2.5769, Valid loss: 2.3744\n",
      "Epoch [904/3000]: Train loss: 2.1082, Valid loss: 2.2600\n",
      "Epoch [905/3000]: Train loss: 2.7244, Valid loss: 2.2614\n",
      "Epoch [906/3000]: Train loss: 2.9369, Valid loss: 2.2625\n",
      "Epoch [907/3000]: Train loss: 3.0300, Valid loss: 7.8431\n",
      "Epoch [908/3000]: Train loss: 3.6062, Valid loss: 2.9797\n",
      "Epoch [909/3000]: Train loss: 2.3600, Valid loss: 2.5622\n",
      "Epoch [910/3000]: Train loss: 2.1118, Valid loss: 2.5195\n",
      "Epoch [911/3000]: Train loss: 2.1530, Valid loss: 1.8011\n",
      "Saving model with loss 1.801...\n",
      "Epoch [912/3000]: Train loss: 2.6921, Valid loss: 2.0862\n",
      "Epoch [913/3000]: Train loss: 2.1502, Valid loss: 2.5969\n",
      "Epoch [914/3000]: Train loss: 2.1648, Valid loss: 2.1231\n",
      "Epoch [915/3000]: Train loss: 2.1024, Valid loss: 2.5113\n",
      "Epoch [916/3000]: Train loss: 2.0815, Valid loss: 2.9499\n",
      "Epoch [917/3000]: Train loss: 2.6746, Valid loss: 2.5969\n",
      "Epoch [918/3000]: Train loss: 2.5538, Valid loss: 2.9399\n",
      "Epoch [919/3000]: Train loss: 2.9691, Valid loss: 3.4638\n",
      "Epoch [920/3000]: Train loss: 2.5783, Valid loss: 2.6933\n",
      "Epoch [921/3000]: Train loss: 2.1311, Valid loss: 3.0461\n",
      "Epoch [922/3000]: Train loss: 2.7953, Valid loss: 2.0477\n",
      "Epoch [923/3000]: Train loss: 3.8429, Valid loss: 5.3360\n",
      "Epoch [924/3000]: Train loss: 4.1772, Valid loss: 3.4496\n",
      "Epoch [925/3000]: Train loss: 2.5830, Valid loss: 3.0866\n",
      "Epoch [926/3000]: Train loss: 2.7466, Valid loss: 2.7320\n",
      "Epoch [927/3000]: Train loss: 2.3938, Valid loss: 2.3754\n",
      "Epoch [928/3000]: Train loss: 2.4992, Valid loss: 3.8938\n",
      "Epoch [929/3000]: Train loss: 2.9322, Valid loss: 3.9682\n",
      "Epoch [930/3000]: Train loss: 2.3914, Valid loss: 2.5332\n",
      "Epoch [931/3000]: Train loss: 2.2054, Valid loss: 2.9511\n",
      "Epoch [932/3000]: Train loss: 2.3983, Valid loss: 3.4456\n",
      "Epoch [933/3000]: Train loss: 2.9986, Valid loss: 3.2854\n",
      "Epoch [934/3000]: Train loss: 2.4798, Valid loss: 3.7668\n",
      "Epoch [935/3000]: Train loss: 2.4774, Valid loss: 2.0832\n",
      "Epoch [936/3000]: Train loss: 2.5572, Valid loss: 2.3897\n",
      "Epoch [937/3000]: Train loss: 2.3679, Valid loss: 2.1291\n",
      "Epoch [938/3000]: Train loss: 2.6304, Valid loss: 4.9533\n",
      "Epoch [939/3000]: Train loss: 3.8521, Valid loss: 3.6587\n",
      "Epoch [940/3000]: Train loss: 2.5598, Valid loss: 2.4857\n",
      "Epoch [941/3000]: Train loss: 2.2632, Valid loss: 2.4262\n",
      "Epoch [942/3000]: Train loss: 2.2379, Valid loss: 2.1780\n",
      "Epoch [943/3000]: Train loss: 2.4270, Valid loss: 3.6784\n",
      "Epoch [944/3000]: Train loss: 3.0915, Valid loss: 3.9097\n",
      "Epoch [945/3000]: Train loss: 2.5198, Valid loss: 2.5247\n",
      "Epoch [946/3000]: Train loss: 2.6994, Valid loss: 4.2135\n",
      "Epoch [947/3000]: Train loss: 3.6066, Valid loss: 2.3764\n",
      "Epoch [948/3000]: Train loss: 3.0975, Valid loss: 5.4772\n",
      "Epoch [949/3000]: Train loss: 3.9148, Valid loss: 4.1084\n",
      "Epoch [950/3000]: Train loss: 2.6995, Valid loss: 2.9103\n",
      "Epoch [951/3000]: Train loss: 2.2047, Valid loss: 2.4383\n",
      "Epoch [952/3000]: Train loss: 2.1964, Valid loss: 2.6169\n",
      "Epoch [953/3000]: Train loss: 2.1689, Valid loss: 2.4158\n",
      "Epoch [954/3000]: Train loss: 2.6575, Valid loss: 2.6793\n",
      "Epoch [955/3000]: Train loss: 2.1416, Valid loss: 2.9847\n",
      "Epoch [956/3000]: Train loss: 2.4171, Valid loss: 2.0808\n",
      "Epoch [957/3000]: Train loss: 2.4671, Valid loss: 2.5764\n",
      "Epoch [958/3000]: Train loss: 2.0053, Valid loss: 2.1358\n",
      "Epoch [959/3000]: Train loss: 2.0806, Valid loss: 2.3697\n",
      "Epoch [960/3000]: Train loss: 2.6381, Valid loss: 2.8163\n",
      "Epoch [961/3000]: Train loss: 2.4043, Valid loss: 2.2162\n",
      "Epoch [962/3000]: Train loss: 2.4644, Valid loss: 2.5625\n",
      "Epoch [963/3000]: Train loss: 2.3517, Valid loss: 2.4038\n",
      "Epoch [964/3000]: Train loss: 2.0539, Valid loss: 2.2755\n",
      "Epoch [965/3000]: Train loss: 2.1031, Valid loss: 2.5714\n",
      "Epoch [966/3000]: Train loss: 2.3504, Valid loss: 2.6034\n",
      "Epoch [967/3000]: Train loss: 2.5243, Valid loss: 2.2694\n",
      "Epoch [968/3000]: Train loss: 2.2079, Valid loss: 2.1361\n",
      "Epoch [969/3000]: Train loss: 2.4666, Valid loss: 3.3904\n",
      "Epoch [970/3000]: Train loss: 2.5481, Valid loss: 2.1349\n",
      "Epoch [971/3000]: Train loss: 2.1511, Valid loss: 2.1404\n",
      "Epoch [972/3000]: Train loss: 2.1128, Valid loss: 2.4070\n",
      "Epoch [973/3000]: Train loss: 2.3316, Valid loss: 2.5231\n",
      "Epoch [974/3000]: Train loss: 2.1563, Valid loss: 2.5780\n",
      "Epoch [975/3000]: Train loss: 2.4241, Valid loss: 3.3286\n",
      "Epoch [976/3000]: Train loss: 2.5258, Valid loss: 2.2964\n",
      "Epoch [977/3000]: Train loss: 2.2263, Valid loss: 2.7174\n",
      "Epoch [978/3000]: Train loss: 2.1094, Valid loss: 2.0419\n",
      "Epoch [979/3000]: Train loss: 2.0472, Valid loss: 3.0084\n",
      "Epoch [980/3000]: Train loss: 2.4621, Valid loss: 2.3824\n",
      "Epoch [981/3000]: Train loss: 2.1328, Valid loss: 2.0813\n",
      "Epoch [982/3000]: Train loss: 2.1482, Valid loss: 4.1205\n",
      "Epoch [983/3000]: Train loss: 3.1694, Valid loss: 2.7589\n",
      "Epoch [984/3000]: Train loss: 3.7263, Valid loss: 9.3938\n",
      "Epoch [985/3000]: Train loss: 5.2038, Valid loss: 4.9864\n",
      "Epoch [986/3000]: Train loss: 3.5206, Valid loss: 5.7366\n",
      "Epoch [987/3000]: Train loss: 4.1490, Valid loss: 2.3662\n",
      "Epoch [988/3000]: Train loss: 2.4697, Valid loss: 2.4603\n",
      "Epoch [989/3000]: Train loss: 2.3932, Valid loss: 2.7634\n",
      "Epoch [990/3000]: Train loss: 2.5720, Valid loss: 2.1361\n",
      "Epoch [991/3000]: Train loss: 2.8086, Valid loss: 3.0526\n",
      "Epoch [992/3000]: Train loss: 2.5749, Valid loss: 3.3592\n",
      "Epoch [993/3000]: Train loss: 2.6841, Valid loss: 2.7357\n",
      "Epoch [994/3000]: Train loss: 2.3031, Valid loss: 2.2750\n",
      "Epoch [995/3000]: Train loss: 2.1567, Valid loss: 2.2781\n",
      "Epoch [996/3000]: Train loss: 2.0824, Valid loss: 2.9217\n",
      "Epoch [997/3000]: Train loss: 2.1001, Valid loss: 2.6739\n",
      "Epoch [998/3000]: Train loss: 2.1961, Valid loss: 2.1510\n",
      "Epoch [999/3000]: Train loss: 2.1683, Valid loss: 3.5218\n",
      "Epoch [1000/3000]: Train loss: 2.5774, Valid loss: 2.9047\n",
      "Epoch [1001/3000]: Train loss: 2.0253, Valid loss: 2.0216\n",
      "Epoch [1002/3000]: Train loss: 2.0666, Valid loss: 2.0076\n",
      "Epoch [1003/3000]: Train loss: 2.0938, Valid loss: 2.2767\n",
      "Epoch [1004/3000]: Train loss: 2.0010, Valid loss: 2.3532\n",
      "Epoch [1005/3000]: Train loss: 2.0897, Valid loss: 2.7126\n",
      "Epoch [1006/3000]: Train loss: 2.1031, Valid loss: 2.5693\n",
      "Epoch [1007/3000]: Train loss: 2.0071, Valid loss: 2.2591\n",
      "Epoch [1008/3000]: Train loss: 2.0734, Valid loss: 3.0857\n",
      "Epoch [1009/3000]: Train loss: 2.4954, Valid loss: 2.1808\n",
      "Epoch [1010/3000]: Train loss: 2.0839, Valid loss: 2.6488\n",
      "Epoch [1011/3000]: Train loss: 2.3724, Valid loss: 5.8529\n",
      "Epoch [1012/3000]: Train loss: 4.2402, Valid loss: 2.3301\n",
      "Epoch [1013/3000]: Train loss: 2.7804, Valid loss: 4.0455\n",
      "Epoch [1014/3000]: Train loss: 3.5723, Valid loss: 2.9228\n",
      "Epoch [1015/3000]: Train loss: 2.7657, Valid loss: 2.3011\n",
      "Epoch [1016/3000]: Train loss: 2.4274, Valid loss: 2.6338\n",
      "Epoch [1017/3000]: Train loss: 2.0690, Valid loss: 2.5082\n",
      "Epoch [1018/3000]: Train loss: 2.0669, Valid loss: 2.3261\n",
      "Epoch [1019/3000]: Train loss: 2.1013, Valid loss: 2.0974\n",
      "Epoch [1020/3000]: Train loss: 2.1062, Valid loss: 3.1136\n",
      "Epoch [1021/3000]: Train loss: 2.2467, Valid loss: 2.2580\n",
      "Epoch [1022/3000]: Train loss: 2.1815, Valid loss: 2.1880\n",
      "Epoch [1023/3000]: Train loss: 2.3366, Valid loss: 1.9696\n",
      "Epoch [1024/3000]: Train loss: 2.2025, Valid loss: 2.6159\n",
      "Epoch [1025/3000]: Train loss: 2.1104, Valid loss: 2.0628\n",
      "Epoch [1026/3000]: Train loss: 2.1937, Valid loss: 2.2427\n",
      "Epoch [1027/3000]: Train loss: 2.1759, Valid loss: 3.3064\n",
      "Epoch [1028/3000]: Train loss: 2.6195, Valid loss: 4.0425\n",
      "Epoch [1029/3000]: Train loss: 3.0592, Valid loss: 3.1483\n",
      "Epoch [1030/3000]: Train loss: 2.3471, Valid loss: 2.2658\n",
      "Epoch [1031/3000]: Train loss: 2.1073, Valid loss: 2.3747\n",
      "Epoch [1032/3000]: Train loss: 1.9687, Valid loss: 2.1108\n",
      "Epoch [1033/3000]: Train loss: 2.0529, Valid loss: 3.0864\n",
      "Epoch [1034/3000]: Train loss: 2.6436, Valid loss: 3.0515\n",
      "Epoch [1035/3000]: Train loss: 2.7138, Valid loss: 3.7164\n",
      "Epoch [1036/3000]: Train loss: 2.8970, Valid loss: 2.6196\n",
      "Epoch [1037/3000]: Train loss: 2.7326, Valid loss: 2.8098\n",
      "Epoch [1038/3000]: Train loss: 2.3896, Valid loss: 2.5133\n",
      "Epoch [1039/3000]: Train loss: 2.1952, Valid loss: 2.2559\n",
      "Epoch [1040/3000]: Train loss: 2.3840, Valid loss: 2.6305\n",
      "Epoch [1041/3000]: Train loss: 2.3063, Valid loss: 2.4368\n",
      "Epoch [1042/3000]: Train loss: 2.0292, Valid loss: 2.1241\n",
      "Epoch [1043/3000]: Train loss: 2.1182, Valid loss: 2.7989\n",
      "Epoch [1044/3000]: Train loss: 2.3704, Valid loss: 2.0910\n",
      "Epoch [1045/3000]: Train loss: 2.2599, Valid loss: 2.4163\n",
      "Epoch [1046/3000]: Train loss: 2.1062, Valid loss: 2.7256\n",
      "Epoch [1047/3000]: Train loss: 2.7242, Valid loss: 2.4677\n",
      "Epoch [1048/3000]: Train loss: 2.0085, Valid loss: 2.2499\n",
      "Epoch [1049/3000]: Train loss: 1.9990, Valid loss: 2.8021\n",
      "Epoch [1050/3000]: Train loss: 2.5110, Valid loss: 2.5263\n",
      "Epoch [1051/3000]: Train loss: 2.0820, Valid loss: 2.5224\n",
      "Epoch [1052/3000]: Train loss: 1.9924, Valid loss: 2.5534\n",
      "Epoch [1053/3000]: Train loss: 2.2356, Valid loss: 2.2994\n",
      "Epoch [1054/3000]: Train loss: 2.0408, Valid loss: 2.9344\n",
      "Epoch [1055/3000]: Train loss: 2.1558, Valid loss: 2.6615\n",
      "Epoch [1056/3000]: Train loss: 2.3839, Valid loss: 2.2318\n",
      "Epoch [1057/3000]: Train loss: 2.6587, Valid loss: 2.3852\n",
      "Epoch [1058/3000]: Train loss: 2.0761, Valid loss: 1.9726\n",
      "Epoch [1059/3000]: Train loss: 1.9840, Valid loss: 1.9498\n",
      "Epoch [1060/3000]: Train loss: 2.0163, Valid loss: 2.2767\n",
      "Epoch [1061/3000]: Train loss: 1.9998, Valid loss: 2.4331\n",
      "Epoch [1062/3000]: Train loss: 2.1036, Valid loss: 2.6969\n",
      "Epoch [1063/3000]: Train loss: 2.2927, Valid loss: 3.8067\n",
      "Epoch [1064/3000]: Train loss: 2.6157, Valid loss: 4.4943\n",
      "Epoch [1065/3000]: Train loss: 5.1355, Valid loss: 6.2479\n",
      "Epoch [1066/3000]: Train loss: 4.3925, Valid loss: 3.9847\n",
      "Epoch [1067/3000]: Train loss: 2.5075, Valid loss: 2.1992\n",
      "Epoch [1068/3000]: Train loss: 2.1404, Valid loss: 3.5528\n",
      "Epoch [1069/3000]: Train loss: 2.5831, Valid loss: 2.2561\n",
      "Epoch [1070/3000]: Train loss: 2.0871, Valid loss: 3.0743\n",
      "Epoch [1071/3000]: Train loss: 2.2964, Valid loss: 2.6538\n",
      "Epoch [1072/3000]: Train loss: 2.6031, Valid loss: 3.1705\n",
      "Epoch [1073/3000]: Train loss: 2.3811, Valid loss: 3.5754\n",
      "Epoch [1074/3000]: Train loss: 2.3338, Valid loss: 2.5149\n",
      "Epoch [1075/3000]: Train loss: 2.0323, Valid loss: 2.4006\n",
      "Epoch [1076/3000]: Train loss: 2.4129, Valid loss: 3.3629\n",
      "Epoch [1077/3000]: Train loss: 2.7812, Valid loss: 3.7437\n",
      "Epoch [1078/3000]: Train loss: 2.7771, Valid loss: 2.4369\n",
      "Epoch [1079/3000]: Train loss: 2.5746, Valid loss: 2.8407\n",
      "Epoch [1080/3000]: Train loss: 2.6238, Valid loss: 2.6202\n",
      "Epoch [1081/3000]: Train loss: 2.5740, Valid loss: 2.6901\n",
      "Epoch [1082/3000]: Train loss: 2.1500, Valid loss: 3.0527\n",
      "Epoch [1083/3000]: Train loss: 2.5414, Valid loss: 2.8105\n",
      "Epoch [1084/3000]: Train loss: 2.0695, Valid loss: 2.1008\n",
      "Epoch [1085/3000]: Train loss: 2.0289, Valid loss: 2.2416\n",
      "Epoch [1086/3000]: Train loss: 1.9596, Valid loss: 2.4783\n",
      "Epoch [1087/3000]: Train loss: 2.0077, Valid loss: 2.3750\n",
      "Epoch [1088/3000]: Train loss: 2.2414, Valid loss: 2.7130\n",
      "Epoch [1089/3000]: Train loss: 2.2149, Valid loss: 1.8672\n",
      "Epoch [1090/3000]: Train loss: 2.2895, Valid loss: 4.0578\n",
      "Epoch [1091/3000]: Train loss: 2.7731, Valid loss: 2.1426\n",
      "Epoch [1092/3000]: Train loss: 2.1794, Valid loss: 3.0234\n",
      "Epoch [1093/3000]: Train loss: 2.2764, Valid loss: 2.6386\n",
      "Epoch [1094/3000]: Train loss: 2.2216, Valid loss: 4.3091\n",
      "Epoch [1095/3000]: Train loss: 2.5341, Valid loss: 2.4442\n",
      "Epoch [1096/3000]: Train loss: 2.4245, Valid loss: 2.4180\n",
      "Epoch [1097/3000]: Train loss: 2.5489, Valid loss: 2.9436\n",
      "Epoch [1098/3000]: Train loss: 2.2354, Valid loss: 2.2257\n",
      "Epoch [1099/3000]: Train loss: 2.4876, Valid loss: 4.4012\n",
      "Epoch [1100/3000]: Train loss: 2.8037, Valid loss: 2.9144\n",
      "Epoch [1101/3000]: Train loss: 2.5217, Valid loss: 2.3963\n",
      "Epoch [1102/3000]: Train loss: 2.0083, Valid loss: 2.4146\n",
      "Epoch [1103/3000]: Train loss: 2.1009, Valid loss: 3.1627\n",
      "Epoch [1104/3000]: Train loss: 2.0886, Valid loss: 2.3854\n",
      "Epoch [1105/3000]: Train loss: 1.9912, Valid loss: 2.1624\n",
      "Epoch [1106/3000]: Train loss: 1.9950, Valid loss: 2.2738\n",
      "Epoch [1107/3000]: Train loss: 2.5635, Valid loss: 2.1641\n",
      "Epoch [1108/3000]: Train loss: 2.1844, Valid loss: 3.2639\n",
      "Epoch [1109/3000]: Train loss: 2.3622, Valid loss: 2.3880\n",
      "Epoch [1110/3000]: Train loss: 2.0891, Valid loss: 2.5596\n",
      "Epoch [1111/3000]: Train loss: 2.1210, Valid loss: 2.3464\n",
      "Epoch [1112/3000]: Train loss: 2.2207, Valid loss: 1.9662\n",
      "Epoch [1113/3000]: Train loss: 2.1400, Valid loss: 2.4067\n",
      "Epoch [1114/3000]: Train loss: 2.1486, Valid loss: 3.1808\n",
      "Epoch [1115/3000]: Train loss: 2.5125, Valid loss: 2.1135\n",
      "Epoch [1116/3000]: Train loss: 2.4206, Valid loss: 3.2472\n",
      "Epoch [1117/3000]: Train loss: 2.1319, Valid loss: 2.2422\n",
      "Epoch [1118/3000]: Train loss: 2.0268, Valid loss: 2.4650\n",
      "Epoch [1119/3000]: Train loss: 1.9736, Valid loss: 2.3776\n",
      "Epoch [1120/3000]: Train loss: 1.9765, Valid loss: 2.7017\n",
      "Epoch [1121/3000]: Train loss: 2.2011, Valid loss: 2.4061\n",
      "Epoch [1122/3000]: Train loss: 2.0581, Valid loss: 2.0306\n",
      "Epoch [1123/3000]: Train loss: 2.1444, Valid loss: 2.0242\n",
      "Epoch [1124/3000]: Train loss: 2.1568, Valid loss: 3.9788\n",
      "Epoch [1125/3000]: Train loss: 2.6089, Valid loss: 2.3429\n",
      "Epoch [1126/3000]: Train loss: 2.0912, Valid loss: 1.8726\n",
      "Epoch [1127/3000]: Train loss: 2.0163, Valid loss: 2.8334\n",
      "Epoch [1128/3000]: Train loss: 2.2921, Valid loss: 2.1295\n",
      "Epoch [1129/3000]: Train loss: 1.9939, Valid loss: 2.1489\n",
      "Epoch [1130/3000]: Train loss: 1.9763, Valid loss: 2.3477\n",
      "Epoch [1131/3000]: Train loss: 2.2039, Valid loss: 3.0238\n",
      "Epoch [1132/3000]: Train loss: 2.8877, Valid loss: 1.9458\n",
      "Epoch [1133/3000]: Train loss: 2.4383, Valid loss: 2.0648\n",
      "Epoch [1134/3000]: Train loss: 2.0468, Valid loss: 2.2433\n",
      "Epoch [1135/3000]: Train loss: 2.7298, Valid loss: 2.9432\n",
      "Epoch [1136/3000]: Train loss: 2.4189, Valid loss: 2.8696\n",
      "Epoch [1137/3000]: Train loss: 2.4965, Valid loss: 2.5108\n",
      "Epoch [1138/3000]: Train loss: 2.0993, Valid loss: 2.1252\n",
      "Epoch [1139/3000]: Train loss: 2.3558, Valid loss: 4.1982\n",
      "Epoch [1140/3000]: Train loss: 2.6822, Valid loss: 2.4816\n",
      "Epoch [1141/3000]: Train loss: 2.8784, Valid loss: 3.1223\n",
      "Epoch [1142/3000]: Train loss: 3.4562, Valid loss: 4.0391\n",
      "Epoch [1143/3000]: Train loss: 3.0859, Valid loss: 2.4762\n",
      "Epoch [1144/3000]: Train loss: 2.3356, Valid loss: 2.3055\n",
      "Epoch [1145/3000]: Train loss: 2.2703, Valid loss: 4.5222\n",
      "Epoch [1146/3000]: Train loss: 2.6330, Valid loss: 2.3162\n",
      "Epoch [1147/3000]: Train loss: 2.1795, Valid loss: 2.3250\n",
      "Epoch [1148/3000]: Train loss: 2.2653, Valid loss: 3.0726\n",
      "Epoch [1149/3000]: Train loss: 2.4376, Valid loss: 2.9980\n",
      "Epoch [1150/3000]: Train loss: 2.1913, Valid loss: 2.4398\n",
      "Epoch [1151/3000]: Train loss: 2.0473, Valid loss: 2.1599\n",
      "Epoch [1152/3000]: Train loss: 2.0185, Valid loss: 2.5881\n",
      "Epoch [1153/3000]: Train loss: 2.2290, Valid loss: 2.0842\n",
      "Epoch [1154/3000]: Train loss: 2.1346, Valid loss: 2.2932\n",
      "Epoch [1155/3000]: Train loss: 2.5427, Valid loss: 2.0657\n",
      "Epoch [1156/3000]: Train loss: 2.4195, Valid loss: 3.6058\n",
      "Epoch [1157/3000]: Train loss: 1.9781, Valid loss: 2.6775\n",
      "Epoch [1158/3000]: Train loss: 1.9396, Valid loss: 3.1262\n",
      "Epoch [1159/3000]: Train loss: 2.1020, Valid loss: 3.2912\n",
      "Epoch [1160/3000]: Train loss: 2.2706, Valid loss: 3.4436\n",
      "Epoch [1161/3000]: Train loss: 2.7187, Valid loss: 2.2202\n",
      "Epoch [1162/3000]: Train loss: 2.4728, Valid loss: 1.9788\n",
      "Epoch [1163/3000]: Train loss: 1.9278, Valid loss: 2.2611\n",
      "Epoch [1164/3000]: Train loss: 2.0799, Valid loss: 2.5835\n",
      "Epoch [1165/3000]: Train loss: 1.9485, Valid loss: 2.4187\n",
      "Epoch [1166/3000]: Train loss: 2.2970, Valid loss: 3.0347\n",
      "Epoch [1167/3000]: Train loss: 2.5592, Valid loss: 2.8496\n",
      "Epoch [1168/3000]: Train loss: 2.5628, Valid loss: 2.6113\n",
      "Epoch [1169/3000]: Train loss: 2.0664, Valid loss: 2.2543\n",
      "Epoch [1170/3000]: Train loss: 1.9884, Valid loss: 1.8583\n",
      "Epoch [1171/3000]: Train loss: 2.0053, Valid loss: 2.0733\n",
      "Epoch [1172/3000]: Train loss: 2.1276, Valid loss: 2.4844\n",
      "Epoch [1173/3000]: Train loss: 2.0512, Valid loss: 2.8203\n",
      "Epoch [1174/3000]: Train loss: 1.9789, Valid loss: 1.9675\n",
      "Epoch [1175/3000]: Train loss: 1.9811, Valid loss: 1.9284\n",
      "Epoch [1176/3000]: Train loss: 1.8867, Valid loss: 2.5092\n",
      "Epoch [1177/3000]: Train loss: 2.0006, Valid loss: 2.2648\n",
      "Epoch [1178/3000]: Train loss: 2.1568, Valid loss: 2.6330\n",
      "Epoch [1179/3000]: Train loss: 2.0004, Valid loss: 2.7534\n",
      "Epoch [1180/3000]: Train loss: 2.4682, Valid loss: 4.0588\n",
      "Epoch [1181/3000]: Train loss: 3.0686, Valid loss: 5.0562\n",
      "Epoch [1182/3000]: Train loss: 2.6722, Valid loss: 2.5042\n",
      "Epoch [1183/3000]: Train loss: 1.9395, Valid loss: 3.0201\n",
      "Epoch [1184/3000]: Train loss: 2.3789, Valid loss: 3.5954\n",
      "Epoch [1185/3000]: Train loss: 1.9894, Valid loss: 1.8342\n",
      "Epoch [1186/3000]: Train loss: 2.1679, Valid loss: 2.3028\n",
      "Epoch [1187/3000]: Train loss: 1.9794, Valid loss: 3.4748\n",
      "Epoch [1188/3000]: Train loss: 2.2583, Valid loss: 2.7255\n",
      "Epoch [1189/3000]: Train loss: 2.2603, Valid loss: 2.3784\n",
      "Epoch [1190/3000]: Train loss: 2.2025, Valid loss: 2.9110\n",
      "Epoch [1191/3000]: Train loss: 1.9851, Valid loss: 2.6505\n",
      "Epoch [1192/3000]: Train loss: 1.9648, Valid loss: 3.6448\n",
      "Epoch [1193/3000]: Train loss: 2.0041, Valid loss: 2.4717\n",
      "Epoch [1194/3000]: Train loss: 1.9502, Valid loss: 2.3358\n",
      "Epoch [1195/3000]: Train loss: 1.9928, Valid loss: 2.4663\n",
      "Epoch [1196/3000]: Train loss: 2.0845, Valid loss: 3.0182\n",
      "Epoch [1197/3000]: Train loss: 2.0189, Valid loss: 2.1790\n",
      "Epoch [1198/3000]: Train loss: 1.8738, Valid loss: 2.2304\n",
      "Epoch [1199/3000]: Train loss: 1.9800, Valid loss: 3.0244\n",
      "Epoch [1200/3000]: Train loss: 2.1201, Valid loss: 1.8665\n",
      "Epoch [1201/3000]: Train loss: 1.9961, Valid loss: 2.4948\n",
      "Epoch [1202/3000]: Train loss: 2.0496, Valid loss: 2.4171\n",
      "Epoch [1203/3000]: Train loss: 2.2275, Valid loss: 3.8535\n",
      "Epoch [1204/3000]: Train loss: 2.3245, Valid loss: 1.8336\n",
      "Epoch [1205/3000]: Train loss: 2.1732, Valid loss: 2.6939\n",
      "Epoch [1206/3000]: Train loss: 2.3001, Valid loss: 2.1296\n",
      "Epoch [1207/3000]: Train loss: 2.4423, Valid loss: 4.0316\n",
      "Epoch [1208/3000]: Train loss: 3.1909, Valid loss: 2.8433\n",
      "Epoch [1209/3000]: Train loss: 2.2050, Valid loss: 2.0465\n",
      "Epoch [1210/3000]: Train loss: 1.9973, Valid loss: 2.1250\n",
      "Epoch [1211/3000]: Train loss: 2.0167, Valid loss: 2.9308\n",
      "Epoch [1212/3000]: Train loss: 2.3231, Valid loss: 2.0340\n",
      "Epoch [1213/3000]: Train loss: 2.7064, Valid loss: 2.2619\n",
      "Epoch [1214/3000]: Train loss: 1.9916, Valid loss: 3.1884\n",
      "Epoch [1215/3000]: Train loss: 2.2052, Valid loss: 2.7161\n",
      "Epoch [1216/3000]: Train loss: 2.2215, Valid loss: 2.1200\n",
      "Epoch [1217/3000]: Train loss: 1.9353, Valid loss: 2.0676\n",
      "Epoch [1218/3000]: Train loss: 2.2676, Valid loss: 2.1298\n",
      "Epoch [1219/3000]: Train loss: 2.0642, Valid loss: 2.0436\n",
      "Epoch [1220/3000]: Train loss: 2.3142, Valid loss: 3.7588\n",
      "Epoch [1221/3000]: Train loss: 2.8618, Valid loss: 3.2003\n",
      "Epoch [1222/3000]: Train loss: 2.6132, Valid loss: 4.9004\n",
      "Epoch [1223/3000]: Train loss: 2.6239, Valid loss: 2.8468\n",
      "Epoch [1224/3000]: Train loss: 2.0826, Valid loss: 2.1771\n",
      "Epoch [1225/3000]: Train loss: 2.1151, Valid loss: 3.9721\n",
      "Epoch [1226/3000]: Train loss: 2.5760, Valid loss: 2.0440\n",
      "Epoch [1227/3000]: Train loss: 2.1337, Valid loss: 2.0091\n",
      "Epoch [1228/3000]: Train loss: 1.9313, Valid loss: 3.6818\n",
      "Epoch [1229/3000]: Train loss: 2.4043, Valid loss: 2.2318\n",
      "Epoch [1230/3000]: Train loss: 2.1807, Valid loss: 2.1230\n",
      "Epoch [1231/3000]: Train loss: 2.1423, Valid loss: 2.4836\n",
      "Epoch [1232/3000]: Train loss: 2.4481, Valid loss: 5.1449\n",
      "Epoch [1233/3000]: Train loss: 2.8611, Valid loss: 3.5961\n",
      "Epoch [1234/3000]: Train loss: 2.8290, Valid loss: 2.8885\n",
      "Epoch [1235/3000]: Train loss: 2.3992, Valid loss: 2.3082\n",
      "Epoch [1236/3000]: Train loss: 2.0603, Valid loss: 1.8994\n",
      "Epoch [1237/3000]: Train loss: 1.9678, Valid loss: 2.0367\n",
      "Epoch [1238/3000]: Train loss: 2.1693, Valid loss: 2.6303\n",
      "Epoch [1239/3000]: Train loss: 2.2220, Valid loss: 3.9007\n",
      "Epoch [1240/3000]: Train loss: 2.6522, Valid loss: 3.6312\n",
      "Epoch [1241/3000]: Train loss: 3.4356, Valid loss: 2.7354\n",
      "Epoch [1242/3000]: Train loss: 2.0456, Valid loss: 2.4301\n",
      "Epoch [1243/3000]: Train loss: 2.1760, Valid loss: 1.9162\n",
      "Epoch [1244/3000]: Train loss: 2.4429, Valid loss: 4.4271\n",
      "Epoch [1245/3000]: Train loss: 2.5506, Valid loss: 2.5370\n",
      "Epoch [1246/3000]: Train loss: 2.0227, Valid loss: 2.3744\n",
      "Epoch [1247/3000]: Train loss: 1.8812, Valid loss: 2.4272\n",
      "Epoch [1248/3000]: Train loss: 2.0991, Valid loss: 2.1001\n",
      "Epoch [1249/3000]: Train loss: 1.9627, Valid loss: 2.0013\n",
      "Epoch [1250/3000]: Train loss: 1.8400, Valid loss: 2.0825\n",
      "Epoch [1251/3000]: Train loss: 2.2149, Valid loss: 2.1686\n",
      "Epoch [1252/3000]: Train loss: 2.8609, Valid loss: 3.4733\n",
      "Epoch [1253/3000]: Train loss: 2.7550, Valid loss: 2.4700\n",
      "Epoch [1254/3000]: Train loss: 2.2505, Valid loss: 2.1158\n",
      "Epoch [1255/3000]: Train loss: 2.0373, Valid loss: 2.0456\n",
      "Epoch [1256/3000]: Train loss: 1.9816, Valid loss: 2.5814\n",
      "Epoch [1257/3000]: Train loss: 1.9612, Valid loss: 3.8919\n",
      "Epoch [1258/3000]: Train loss: 2.1154, Valid loss: 3.1104\n",
      "Epoch [1259/3000]: Train loss: 2.3592, Valid loss: 2.7148\n",
      "Epoch [1260/3000]: Train loss: 2.0458, Valid loss: 2.0830\n",
      "Epoch [1261/3000]: Train loss: 2.4258, Valid loss: 4.4937\n",
      "Epoch [1262/3000]: Train loss: 3.7125, Valid loss: 2.1710\n",
      "Epoch [1263/3000]: Train loss: 2.3215, Valid loss: 3.0838\n",
      "Epoch [1264/3000]: Train loss: 2.3269, Valid loss: 4.0484\n",
      "Epoch [1265/3000]: Train loss: 2.1787, Valid loss: 2.2546\n",
      "Epoch [1266/3000]: Train loss: 1.9763, Valid loss: 2.3110\n",
      "Epoch [1267/3000]: Train loss: 1.9709, Valid loss: 2.3327\n",
      "Epoch [1268/3000]: Train loss: 2.0537, Valid loss: 1.8252\n",
      "Epoch [1269/3000]: Train loss: 2.1807, Valid loss: 3.4366\n",
      "Epoch [1270/3000]: Train loss: 2.5707, Valid loss: 2.7185\n",
      "Epoch [1271/3000]: Train loss: 2.6722, Valid loss: 3.5009\n",
      "Epoch [1272/3000]: Train loss: 2.6479, Valid loss: 2.4692\n",
      "Epoch [1273/3000]: Train loss: 2.0909, Valid loss: 2.4284\n",
      "Epoch [1274/3000]: Train loss: 1.9490, Valid loss: 2.3420\n",
      "Epoch [1275/3000]: Train loss: 2.6311, Valid loss: 2.4829\n",
      "Epoch [1276/3000]: Train loss: 2.4438, Valid loss: 4.7495\n",
      "Epoch [1277/3000]: Train loss: 2.8445, Valid loss: 2.8841\n",
      "Epoch [1278/3000]: Train loss: 2.6951, Valid loss: 2.1999\n",
      "Epoch [1279/3000]: Train loss: 2.0455, Valid loss: 2.0619\n",
      "Epoch [1280/3000]: Train loss: 1.9339, Valid loss: 2.4277\n",
      "Epoch [1281/3000]: Train loss: 2.5987, Valid loss: 4.9802\n",
      "Epoch [1282/3000]: Train loss: 3.3682, Valid loss: 2.0352\n",
      "Epoch [1283/3000]: Train loss: 2.2287, Valid loss: 1.8329\n",
      "Epoch [1284/3000]: Train loss: 1.9795, Valid loss: 2.2386\n",
      "Epoch [1285/3000]: Train loss: 1.9850, Valid loss: 1.9688\n",
      "Epoch [1286/3000]: Train loss: 2.1809, Valid loss: 2.4475\n",
      "Epoch [1287/3000]: Train loss: 1.9388, Valid loss: 2.2112\n",
      "Epoch [1288/3000]: Train loss: 1.9253, Valid loss: 2.0450\n",
      "Epoch [1289/3000]: Train loss: 2.1939, Valid loss: 2.2456\n",
      "Epoch [1290/3000]: Train loss: 2.5538, Valid loss: 2.1758\n",
      "Epoch [1291/3000]: Train loss: 1.8736, Valid loss: 2.6307\n",
      "Epoch [1292/3000]: Train loss: 1.9105, Valid loss: 2.1220\n",
      "Epoch [1293/3000]: Train loss: 1.8948, Valid loss: 2.3051\n",
      "Epoch [1294/3000]: Train loss: 2.3249, Valid loss: 2.3040\n",
      "Epoch [1295/3000]: Train loss: 2.1575, Valid loss: 2.5502\n",
      "Epoch [1296/3000]: Train loss: 2.0880, Valid loss: 2.6909\n",
      "Epoch [1297/3000]: Train loss: 1.8567, Valid loss: 2.6464\n",
      "Epoch [1298/3000]: Train loss: 1.9034, Valid loss: 3.1079\n",
      "Epoch [1299/3000]: Train loss: 1.9533, Valid loss: 2.1151\n",
      "Epoch [1300/3000]: Train loss: 2.0906, Valid loss: 2.0815\n",
      "Epoch [1301/3000]: Train loss: 2.0066, Valid loss: 2.5609\n",
      "Epoch [1302/3000]: Train loss: 1.9323, Valid loss: 1.8594\n",
      "Epoch [1303/3000]: Train loss: 1.9848, Valid loss: 2.7409\n",
      "Epoch [1304/3000]: Train loss: 1.9208, Valid loss: 2.2210\n",
      "Epoch [1305/3000]: Train loss: 2.2325, Valid loss: 3.1456\n",
      "Epoch [1306/3000]: Train loss: 2.4688, Valid loss: 2.4695\n",
      "Epoch [1307/3000]: Train loss: 2.0741, Valid loss: 1.9324\n",
      "Epoch [1308/3000]: Train loss: 2.0003, Valid loss: 1.9427\n",
      "Epoch [1309/3000]: Train loss: 1.8061, Valid loss: 3.1669\n",
      "Epoch [1310/3000]: Train loss: 2.0176, Valid loss: 1.9542\n",
      "Epoch [1311/3000]: Train loss: 2.1814, Valid loss: 2.0602\n",
      "\n",
      "Model is not improving, so we halt the training session.\n"
     ]
    }
   ],
   "source": [
    "model = My_Model(input_dim=x_train.shape[1]).to(device) # put your model and data on the same computation device.\n",
    "trainer(train_loader, valid_loader, model, config, device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "Launching TensorBoard..."
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir=./runs/"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 227.89it/s]\n"
     ]
    }
   ],
   "source": [
    "def save_pred(preds, file):\n",
    "    ''' Save predictions to specified file '''\n",
    "    with open(file, 'w') as fp:\n",
    "        writer = csv.writer(fp)\n",
    "        writer.writerow(['id', 'tested_positive'])\n",
    "        for i, p in enumerate(preds):\n",
    "            writer.writerow([i, p])\n",
    "\n",
    "model = My_Model(input_dim=x_train.shape[1]).to(device)\n",
    "model.load_state_dict(torch.load(config['save_path']))\n",
    "preds = predict(test_loader, model, device)\n",
    "save_pred(preds, 'pred.csv')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-8c0dab32",
   "language": "python",
   "display_name": "PyCharm (deeplearning)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}