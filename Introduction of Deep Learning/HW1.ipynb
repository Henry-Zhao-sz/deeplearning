{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 数据Data，模型Network Structure，调优Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# Numerical Operations\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# Reading/Writing Data\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "\n",
    "# For Progress Bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "# For plotting learning curve\n",
    "from tensorboardX import SummaryWriter\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "def same_seed(seed):\n",
    "    '''Fixes random number generator seeds for reproducibility.'''\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def train_valid_split(data_set, valid_ratio, seed):\n",
    "    '''Split provided training data into training set and validation set'''\n",
    "    valid_set_size = int(valid_ratio * len(data_set))\n",
    "    train_set_size = len(data_set) - valid_set_size\n",
    "    train_set, valid_set = random_split(data_set, [train_set_size, valid_set_size], generator=torch.Generator().manual_seed(seed))\n",
    "    return np.array(train_set), np.array(valid_set)\n",
    "\n",
    "def predict(test_loader, model, device):\n",
    "    model.eval() # Set your model to evaluation mode.\n",
    "    preds = []\n",
    "    for x in tqdm(test_loader):\n",
    "        x = x.to(device)\n",
    "        with torch.no_grad():\n",
    "            pred = model(x)\n",
    "            preds.append(pred.detach().cpu())\n",
    "    preds = torch.cat(preds, dim=0).numpy()\n",
    "    return preds\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "class COVID19Dataset(Dataset):\n",
    "    '''\n",
    "    x: Features.\n",
    "    y: Targets, if none, do prediction.\n",
    "    '''\n",
    "    def __init__(self, x, y=None):\n",
    "        if y is None:\n",
    "            self.y = y\n",
    "        else:\n",
    "            self.y = torch.FloatTensor(y)\n",
    "        self.x = torch.FloatTensor(x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.y is None:\n",
    "            return self.x[idx]\n",
    "        else:\n",
    "            return self.x[idx], self.y[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "class My_Model(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(My_Model, self).__init__()\n",
    "        # TODO: modify model's structure, be aware of dimensions.\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(p = 0.35),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "        self.criterion = nn.MSELoss(reduction=\"mean\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        x = x.squeeze(1) # (B, 1) -> (B)\n",
    "        return x\n",
    "\n",
    "    def cal_loss(self, pred, target):\n",
    "        return torch.sqrt(self.criterion(pred, target))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40, 41, 53, 56, 57, 69, 72, 73, 85, 88, 89, 101, 103, 104, 105]\n"
     ]
    }
   ],
   "source": [
    "# feature engineering, to select the features that correlate with the prediction\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "df = pd.read_csv('./covid.train.csv')\n",
    "\n",
    "state_col = df.iloc[:,:40]\n",
    "data_col = df.iloc[:,40:-1]\n",
    "\n",
    "selector = SelectKBest(f_regression, k=15).fit(data_col, df.iloc[:,-1])\n",
    "\n",
    "feats_col = pd.concat([data_col.iloc[:,selector.get_support()]], axis=1)\n",
    "\n",
    "feats_selected = [df.columns.get_loc(col) for col in feats_col.columns]\n",
    "\n",
    "print(feats_selected)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "\n",
    "def select_feat(train_data, valid_data, test_data, feats_selected, select_all=True ):\n",
    "    '''Selects useful features to perform regression'''\n",
    "    y_train, y_valid = train_data[:,-1], valid_data[:,-1]\n",
    "    raw_x_train, raw_x_valid, raw_x_test = train_data[:,:-1], valid_data[:,:-1], test_data\n",
    "\n",
    "    if select_all:\n",
    "        feat_idx = list(range(raw_x_train.shape[1]))\n",
    "    else:\n",
    "        feat_idx = feats_selected # TODO: Select suitable feature columns.\n",
    "\n",
    "    return raw_x_train[:,feat_idx], raw_x_valid[:,feat_idx], raw_x_test[:,feat_idx], y_train, y_valid"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "def trainer(train_loader, valid_loader, model, config, device):\n",
    "\n",
    "    # criterion = nn.MSELoss(reduction='mean') # Define your loss function, do not modify this.\n",
    "\n",
    "    # Define your optimization algorithm.\n",
    "    # TODO: Please check https://pytorch.org/docs/stable/optim.html to get more available algorithms.\n",
    "    # TODO: L2 regularization (optimizer(weight decay...) or implement by your self).\n",
    "    # optimizer = torch.optim.SGD(model.parameters(), lr=config['learning_rate'], momentum=0.9)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "    writer = SummaryWriter() # Writer of tensoboard.\n",
    "\n",
    "    if not os.path.isdir('./models'):\n",
    "        os.mkdir('./models') # Create directory of saving models.\n",
    "\n",
    "    n_epochs, best_loss, step, early_stop_count = config['n_epochs'], math.inf, 0, 0\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train() # Set your model to train mode.\n",
    "        loss_record = []\n",
    "\n",
    "        # tqdm is a package to visualize your training progress.\n",
    "        train_pbar = tqdm(train_loader, position=0, leave=True)\n",
    "\n",
    "        for x, y in train_pbar:\n",
    "            optimizer.zero_grad()               # Set gradient to zero.\n",
    "            x, y = x.to(device), y.to(device)   # Move your data to device.\n",
    "            pred = model(x)\n",
    "            loss = model.cal_loss(pred, y)\n",
    "            loss.backward()                     # Compute gradient(backpropagation).\n",
    "            optimizer.step()                    # Update parameters.\n",
    "            step += 1\n",
    "            loss_record.append(loss.detach().item())\n",
    "\n",
    "            # Display current epoch number and loss on tqdm progress bar.\n",
    "            train_pbar.set_description(f'Epoch [{epoch+1}/{n_epochs}]')\n",
    "            train_pbar.set_postfix({'loss': loss.detach().item()})\n",
    "\n",
    "        mean_train_loss = sum(loss_record)/len(loss_record)\n",
    "        writer.add_scalar('Loss/train', mean_train_loss, step)\n",
    "\n",
    "        model.eval() # Set your model to evaluation mode.\n",
    "        loss_record = []\n",
    "        for x, y in valid_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            with torch.no_grad():\n",
    "                pred = model(x)\n",
    "                loss = model.cal_loss(pred, y)\n",
    "\n",
    "            loss_record.append(loss.item())\n",
    "\n",
    "        mean_valid_loss = sum(loss_record)/len(loss_record)\n",
    "        print(f'Epoch [{epoch+1}/{n_epochs}]: Train loss: {mean_train_loss:.4f}, Valid loss: {mean_valid_loss:.4f}')\n",
    "        writer.add_scalar('Loss/valid', mean_valid_loss, step)\n",
    "\n",
    "        if mean_valid_loss < best_loss:\n",
    "            best_loss = mean_valid_loss\n",
    "            torch.save(model.state_dict(), config['save_path']) # Save your best model\n",
    "            print('Saving model with loss {:.3f}...'.format(best_loss))\n",
    "            early_stop_count = 0\n",
    "        else:\n",
    "            early_stop_count += 1\n",
    "\n",
    "        if early_stop_count >= config['early_stop']:\n",
    "            print('\\nModel is not improving, so we halt the training session.')\n",
    "            return"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "config = {\n",
    "    'seed': 5201314,      # Your seed number, you can pick your lucky number. :)\n",
    "    'select_all': False,   # Whether to use all features.\n",
    "    'valid_ratio': 0.2,   # validation_size = train_size * valid_ratio\n",
    "    'n_epochs': 3000,     # Number of epochs.\n",
    "\n",
    "    'batch_size': 256,\n",
    "    'learning_rate': 1e-3,\n",
    "    'early_stop': 400,    # If model has not improved for this many consecutive epochs, stop training.\n",
    "    'save_path': './models/model.ckpt'  # Your model will be saved here.\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data size: (2160, 118)\n",
      "valid_data size: (539, 118)\n",
      "test_data size: (1078, 117)\n",
      "number of features: 15\n"
     ]
    }
   ],
   "source": [
    "# Set seed for reproducibility\n",
    "same_seed(config['seed'])\n",
    "\n",
    "\n",
    "# train_data size: 2699 x 118 (id + 37 states + 16 features x 5 days)\n",
    "# test_data size: 1078 x 117 (without last day's positive rate)\n",
    "train_data, test_data = pd.read_csv('./covid.train.csv').values, pd.read_csv('./covid.test.csv').values\n",
    "train_data, valid_data = train_valid_split(train_data, config['valid_ratio'], config['seed'])\n",
    "\n",
    "# Print out the data size.\n",
    "print(f\"\"\"train_data size: {train_data.shape}\n",
    "valid_data size: {valid_data.shape}\n",
    "test_data size: {test_data.shape}\"\"\")\n",
    "\n",
    "# Select features\n",
    "x_train, x_valid, x_test, y_train, y_valid = select_feat(train_data, valid_data, test_data, feats_selected, config['select_all'])\n",
    "\n",
    "# Print out the number of features.\n",
    "print(f'number of features: {x_train.shape[1]}')\n",
    "\n",
    "train_dataset, valid_dataset, test_dataset = COVID19Dataset(x_train, y_train), \\\n",
    "                                            COVID19Dataset(x_valid, y_valid), \\\n",
    "                                            COVID19Dataset(x_test)\n",
    "\n",
    "# Pytorch data loader loads pytorch dataset into batches.\n",
    "train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, pin_memory=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=config['batch_size'], shuffle=True, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False, pin_memory=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/3000]: 100%|██████████| 9/9 [00:01<00:00,  5.11it/s, loss=10.8]\n",
      "Epoch [2/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.10it/s, loss=11.6]\n",
      "Epoch [3/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.53it/s, loss=10.7]\n",
      "Epoch [4/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.09it/s, loss=9.74]\n",
      "Epoch [5/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.43it/s, loss=8.92]\n",
      "Epoch [6/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=9.39]\n",
      "Epoch [7/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=9.78]\n",
      "Epoch [8/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.53it/s, loss=9.44]\n",
      "Epoch [9/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.09it/s, loss=7.69]\n",
      "Epoch [10/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.53it/s, loss=6.86]\n",
      "Epoch [11/3000]: 100%|██████████| 9/9 [00:00<00:00, 213.22it/s, loss=6.97]\n",
      "Epoch [12/3000]: 100%|██████████| 9/9 [00:00<00:00, 214.86it/s, loss=6.82]\n",
      "Epoch [13/3000]: 100%|██████████| 9/9 [00:00<00:00, 220.10it/s, loss=5.95]\n",
      "Epoch [14/3000]: 100%|██████████| 9/9 [00:00<00:00, 214.86it/s, loss=5.68]\n",
      "Epoch [15/3000]: 100%|██████████| 9/9 [00:00<00:00, 214.81it/s, loss=5.44]\n",
      "Epoch [16/3000]: 100%|██████████| 9/9 [00:00<00:00, 209.86it/s, loss=5.28]\n",
      "Epoch [17/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=4.7]\n",
      "Epoch [18/3000]: 100%|██████████| 9/9 [00:00<00:00, 209.86it/s, loss=4.64]\n",
      "Epoch [19/3000]: 100%|██████████| 9/9 [00:00<00:00, 197.94it/s, loss=4.16]\n",
      "Epoch [20/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.17it/s, loss=4.36]\n",
      "Epoch [21/3000]: 100%|██████████| 9/9 [00:00<00:00, 209.84it/s, loss=4.85]\n",
      "Epoch [22/3000]: 100%|██████████| 9/9 [00:00<00:00, 214.86it/s, loss=2.89]\n",
      "Epoch [23/3000]: 100%|██████████| 9/9 [00:00<00:00, 217.41it/s, loss=3.76]\n",
      "Epoch [24/3000]: 100%|██████████| 9/9 [00:00<00:00, 225.60it/s, loss=4.39]\n",
      "Epoch [25/3000]: 100%|██████████| 9/9 [00:00<00:00, 207.41it/s, loss=3.24]\n",
      "Epoch [26/3000]: 100%|██████████| 9/9 [00:00<00:00, 170.27it/s, loss=3.49]\n",
      "Epoch [27/3000]: 100%|██████████| 9/9 [00:00<00:00, 178.66it/s, loss=3.12]\n",
      "Epoch [28/3000]: 100%|██████████| 9/9 [00:00<00:00, 155.59it/s, loss=2.91]\n",
      "Epoch [29/3000]: 100%|██████████| 9/9 [00:00<00:00, 155.58it/s, loss=2.59]\n",
      "Epoch [30/3000]: 100%|██████████| 9/9 [00:00<00:00, 138.83it/s, loss=2.31]\n",
      "Epoch [31/3000]: 100%|██████████| 9/9 [00:00<00:00, 170.26it/s, loss=2.47]\n",
      "Epoch [32/3000]: 100%|██████████| 9/9 [00:00<00:00, 158.32it/s, loss=2.62]\n",
      "Epoch [33/3000]: 100%|██████████| 9/9 [00:00<00:00, 155.58it/s, loss=2.42]\n",
      "Epoch [34/3000]: 100%|██████████| 9/9 [00:00<00:00, 138.83it/s, loss=2.93]\n",
      "Epoch [35/3000]: 100%|██████████| 9/9 [00:00<00:00, 170.26it/s, loss=2.95]\n",
      "Epoch [36/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=3.12]\n",
      "Epoch [37/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.54it/s, loss=3.02]\n",
      "Epoch [38/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=2.4]\n",
      "Epoch [39/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=3.01]\n",
      "Epoch [40/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=2.55]\n",
      "Epoch [41/3000]: 100%|██████████| 9/9 [00:00<00:00, 209.86it/s, loss=2.25]\n",
      "Epoch [42/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=2.42]\n",
      "Epoch [43/3000]: 100%|██████████| 9/9 [00:00<00:00, 193.80it/s, loss=2.7]\n",
      "Epoch [44/3000]: 100%|██████████| 9/9 [00:00<00:00, 209.86it/s, loss=2.4]\n",
      "Epoch [45/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.16it/s, loss=2.3]\n",
      "Epoch [46/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.09it/s, loss=2.37]\n",
      "Epoch [47/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.09it/s, loss=2.99]\n",
      "Epoch [48/3000]: 100%|██████████| 9/9 [00:00<00:00, 231.38it/s, loss=2.34]\n",
      "Epoch [49/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.17it/s, loss=2.37]\n",
      "Epoch [50/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.53it/s, loss=2.52]\n",
      "Epoch [51/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.26it/s, loss=2.29]\n",
      "Epoch [52/3000]: 100%|██████████| 9/9 [00:00<00:00, 194.95it/s, loss=2.46]\n",
      "Epoch [53/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=2.34]\n",
      "Epoch [54/3000]: 100%|██████████| 9/9 [00:00<00:00, 186.02it/s, loss=3.15]\n",
      "Epoch [55/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.01it/s, loss=2.25]\n",
      "Epoch [56/3000]: 100%|██████████| 9/9 [00:00<00:00, 194.04it/s, loss=2.23]\n",
      "Epoch [57/3000]: 100%|██████████| 9/9 [00:00<00:00, 214.86it/s, loss=2.62]\n",
      "Epoch [58/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.17it/s, loss=2.35]\n",
      "Epoch [59/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=2.52]\n",
      "Epoch [60/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=2.31]\n",
      "Epoch [61/3000]: 100%|██████████| 9/9 [00:00<00:00, 214.86it/s, loss=2.22]\n",
      "Epoch [62/3000]: 100%|██████████| 9/9 [00:00<00:00, 220.10it/s, loss=2.35]\n",
      "Epoch [63/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.09it/s, loss=1.77]\n",
      "Epoch [64/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.09it/s, loss=2.73]\n",
      "Epoch [65/3000]: 100%|██████████| 9/9 [00:00<00:00, 209.87it/s, loss=2.58]\n",
      "Epoch [66/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.09it/s, loss=2.29]\n",
      "Epoch [67/3000]: 100%|██████████| 9/9 [00:00<00:00, 214.85it/s, loss=2.72]\n",
      "Epoch [68/3000]: 100%|██████████| 9/9 [00:00<00:00, 191.90it/s, loss=2.07]\n",
      "Epoch [69/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=2.85]\n",
      "Epoch [70/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=2.49]\n",
      "Epoch [71/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.01it/s, loss=2.28]\n",
      "Epoch [72/3000]: 100%|██████████| 9/9 [00:00<00:00, 198.29it/s, loss=2.25]\n",
      "Epoch [73/3000]: 100%|██████████| 9/9 [00:00<00:00, 220.11it/s, loss=2.58]\n",
      "Epoch [74/3000]: 100%|██████████| 9/9 [00:00<00:00, 155.58it/s, loss=2.5]\n",
      "Epoch [75/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.09it/s, loss=2.49]\n",
      "Epoch [76/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.16it/s, loss=2.25]\n",
      "Epoch [77/3000]: 100%|██████████| 9/9 [00:00<00:00, 155.59it/s, loss=2.39]\n",
      "Epoch [78/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.95it/s, loss=1.99]\n",
      "Epoch [79/3000]: 100%|██████████| 9/9 [00:00<00:00, 151.24it/s, loss=2.37]\n",
      "Epoch [80/3000]: 100%|██████████| 9/9 [00:00<00:00, 156.44it/s, loss=2.39]\n",
      "Epoch [81/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=2.24]\n",
      "Epoch [82/3000]: 100%|██████████| 9/9 [00:00<00:00, 222.77it/s, loss=3.06]\n",
      "Epoch [83/3000]: 100%|██████████| 9/9 [00:00<00:00, 209.85it/s, loss=2.37]\n",
      "Epoch [84/3000]: 100%|██████████| 9/9 [00:00<00:00, 185.52it/s, loss=2.05]\n",
      "Epoch [85/3000]: 100%|██████████| 9/9 [00:00<00:00, 204.13it/s, loss=2.09]\n",
      "Epoch [86/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.53it/s, loss=1.98]\n",
      "Epoch [87/3000]: 100%|██████████| 9/9 [00:00<00:00, 156.91it/s, loss=2.17]\n",
      "Epoch [88/3000]: 100%|██████████| 9/9 [00:00<00:00, 167.11it/s, loss=2.02]\n",
      "Epoch [89/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=2.25]\n",
      "Epoch [90/3000]: 100%|██████████| 9/9 [00:00<00:00, 209.86it/s, loss=2.12]\n",
      "Epoch [91/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=2.73]\n",
      "Epoch [92/3000]: 100%|██████████| 9/9 [00:00<00:00, 141.00it/s, loss=2.18]\n",
      "Epoch [93/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.53it/s, loss=2.1]\n",
      "Epoch [94/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.94it/s, loss=1.83]\n",
      "Epoch [95/3000]: 100%|██████████| 9/9 [00:00<00:00, 145.55it/s, loss=2.22]\n",
      "Epoch [96/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.54it/s, loss=2.09]\n",
      "Epoch [97/3000]: 100%|██████████| 9/9 [00:00<00:00, 145.55it/s, loss=1.92]\n",
      "Epoch [98/3000]: 100%|██████████| 9/9 [00:00<00:00, 158.32it/s, loss=2.14]\n",
      "Epoch [99/3000]: 100%|██████████| 9/9 [00:00<00:00, 164.07it/s, loss=1.7]\n",
      "Epoch [100/3000]: 100%|██████████| 9/9 [00:00<00:00, 150.40it/s, loss=1.84]\n",
      "Epoch [101/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=2.01]\n",
      "Epoch [102/3000]: 100%|██████████| 9/9 [00:00<00:00, 129.87it/s, loss=2.54]\n",
      "Epoch [103/3000]: 100%|██████████| 9/9 [00:00<00:00, 145.54it/s, loss=2.42]\n",
      "Epoch [104/3000]: 100%|██████████| 9/9 [00:00<00:00, 170.26it/s, loss=2.41]\n",
      "Epoch [105/3000]: 100%|██████████| 9/9 [00:00<00:00, 170.28it/s, loss=1.87]\n",
      "Epoch [106/3000]: 100%|██████████| 9/9 [00:00<00:00, 158.31it/s, loss=2.54]\n",
      "Epoch [107/3000]: 100%|██████████| 9/9 [00:00<00:00, 127.10it/s, loss=1.67]\n",
      "Epoch [108/3000]: 100%|██████████| 9/9 [00:00<00:00, 147.94it/s, loss=1.84]\n",
      "Epoch [109/3000]: 100%|██████████| 9/9 [00:00<00:00, 159.64it/s, loss=1.66]\n",
      "Epoch [110/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.17it/s, loss=2.44]\n",
      "Epoch [111/3000]: 100%|██████████| 9/9 [00:00<00:00, 147.94it/s, loss=2.32]\n",
      "Epoch [112/3000]: 100%|██████████| 9/9 [00:00<00:00, 165.55it/s, loss=1.73]\n",
      "Epoch [113/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.20it/s, loss=2.33]\n",
      "Epoch [114/3000]: 100%|██████████| 9/9 [00:00<00:00, 136.73it/s, loss=1.94]\n",
      "Epoch [115/3000]: 100%|██████████| 9/9 [00:00<00:00, 155.59it/s, loss=2.19]\n",
      "Epoch [116/3000]: 100%|██████████| 9/9 [00:00<00:00, 170.27it/s, loss=1.89]\n",
      "Epoch [117/3000]: 100%|██████████| 9/9 [00:00<00:00, 145.55it/s, loss=1.95]\n",
      "Epoch [118/3000]: 100%|██████████| 9/9 [00:00<00:00, 170.27it/s, loss=2.03]\n",
      "Epoch [119/3000]: 100%|██████████| 9/9 [00:00<00:00, 145.55it/s, loss=2.08]\n",
      "Epoch [120/3000]: 100%|██████████| 9/9 [00:00<00:00, 147.93it/s, loss=2.02]\n",
      "Epoch [121/3000]: 100%|██████████| 9/9 [00:00<00:00, 134.69it/s, loss=2.17]\n",
      "Epoch [122/3000]: 100%|██████████| 9/9 [00:00<00:00, 158.31it/s, loss=2.13]\n",
      "Epoch [123/3000]: 100%|██████████| 9/9 [00:00<00:00, 155.59it/s, loss=1.68]\n",
      "Epoch [124/3000]: 100%|██████████| 9/9 [00:00<00:00, 167.16it/s, loss=2.09]\n",
      "Epoch [125/3000]: 100%|██████████| 9/9 [00:00<00:00, 167.12it/s, loss=2.03]\n",
      "Epoch [126/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=2.43]\n",
      "Epoch [127/3000]: 100%|██████████| 9/9 [00:00<00:00, 128.92it/s, loss=2.56]\n",
      "Epoch [128/3000]: 100%|██████████| 9/9 [00:00<00:00, 161.15it/s, loss=1.71]\n",
      "Epoch [129/3000]: 100%|██████████| 9/9 [00:00<00:00, 152.95it/s, loss=1.93]\n",
      "Epoch [130/3000]: 100%|██████████| 9/9 [00:00<00:00, 145.55it/s, loss=2.14]\n",
      "Epoch [131/3000]: 100%|██████████| 9/9 [00:00<00:00, 167.11it/s, loss=1.85]\n",
      "Epoch [132/3000]: 100%|██████████| 9/9 [00:00<00:00, 150.38it/s, loss=2.16]\n",
      "Epoch [133/3000]: 100%|██████████| 9/9 [00:00<00:00, 147.94it/s, loss=1.76]\n",
      "Epoch [134/3000]: 100%|██████████| 9/9 [00:00<00:00, 85.13it/s, loss=2.04]\n",
      "Epoch [135/3000]: 100%|██████████| 9/9 [00:00<00:00, 166.37it/s, loss=2.32]\n",
      "Epoch [136/3000]: 100%|██████████| 9/9 [00:00<00:00, 182.26it/s, loss=1.83]\n",
      "Epoch [137/3000]: 100%|██████████| 9/9 [00:00<00:00, 145.55it/s, loss=1.67]\n",
      "Epoch [138/3000]: 100%|██████████| 9/9 [00:00<00:00, 149.14it/s, loss=1.71]\n",
      "Epoch [139/3000]: 100%|██████████| 9/9 [00:00<00:00, 144.37it/s, loss=1.89]\n",
      "Epoch [140/3000]: 100%|██████████| 9/9 [00:00<00:00, 152.95it/s, loss=2.06]\n",
      "Epoch [141/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=2.08]\n",
      "Epoch [142/3000]: 100%|██████████| 9/9 [00:00<00:00, 167.12it/s, loss=1.66]\n",
      "Epoch [143/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=3.09]\n",
      "Epoch [144/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.94it/s, loss=1.53]\n",
      "Epoch [145/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.95it/s, loss=2.01]\n",
      "Epoch [146/3000]: 100%|██████████| 9/9 [00:00<00:00, 164.08it/s, loss=1.8]\n",
      "Epoch [147/3000]: 100%|██████████| 9/9 [00:00<00:00, 161.15it/s, loss=1.98]\n",
      "Epoch [148/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.54it/s, loss=1.5]\n",
      "Epoch [149/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.54it/s, loss=1.78]\n",
      "Epoch [150/3000]: 100%|██████████| 9/9 [00:00<00:00, 170.26it/s, loss=1.91]\n",
      "Epoch [151/3000]: 100%|██████████| 9/9 [00:00<00:00, 161.14it/s, loss=1.99]\n",
      "Epoch [152/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.43it/s, loss=1.58]\n",
      "Epoch [153/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=2.05]\n",
      "Epoch [154/3000]: 100%|██████████| 9/9 [00:00<00:00, 167.12it/s, loss=1.86]\n",
      "Epoch [155/3000]: 100%|██████████| 9/9 [00:00<00:00, 164.08it/s, loss=1.48]\n",
      "Epoch [156/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=2.24]\n",
      "Epoch [157/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.54it/s, loss=1.51]\n",
      "Epoch [158/3000]: 100%|██████████| 9/9 [00:00<00:00, 187.89it/s, loss=1.52]\n",
      "Epoch [159/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.15it/s, loss=2]\n",
      "Epoch [160/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.54it/s, loss=2.14]\n",
      "Epoch [161/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=1.88]\n",
      "Epoch [162/3000]: 100%|██████████| 9/9 [00:00<00:00, 162.58it/s, loss=2.23]\n",
      "Epoch [163/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=1.67]\n",
      "Epoch [164/3000]: 100%|██████████| 9/9 [00:00<00:00, 168.65it/s, loss=1.77]\n",
      "Epoch [165/3000]: 100%|██████████| 9/9 [00:00<00:00, 158.32it/s, loss=1.73]\n",
      "Epoch [166/3000]: 100%|██████████| 9/9 [00:00<00:00, 167.11it/s, loss=1.64]\n",
      "Epoch [167/3000]: 100%|██████████| 9/9 [00:00<00:00, 161.15it/s, loss=2.04]\n",
      "Epoch [168/3000]: 100%|██████████| 9/9 [00:00<00:00, 164.08it/s, loss=1.71]\n",
      "Epoch [169/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.54it/s, loss=1.66]\n",
      "Epoch [170/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.01it/s, loss=1.89]\n",
      "Epoch [171/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.54it/s, loss=1.68]\n",
      "Epoch [172/3000]: 100%|██████████| 9/9 [00:00<00:00, 164.08it/s, loss=1.87]\n",
      "Epoch [173/3000]: 100%|██████████| 9/9 [00:00<00:00, 147.94it/s, loss=1.67]\n",
      "Epoch [174/3000]: 100%|██████████| 9/9 [00:00<00:00, 170.27it/s, loss=2.05]\n",
      "Epoch [175/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=1.75]\n",
      "Epoch [176/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.54it/s, loss=2.58]\n",
      "Epoch [177/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.54it/s, loss=1.39]\n",
      "Epoch [178/3000]: 100%|██████████| 9/9 [00:00<00:00, 151.65it/s, loss=1.89]\n",
      "Epoch [179/3000]: 100%|██████████| 9/9 [00:00<00:00, 150.40it/s, loss=1.7]\n",
      "Epoch [180/3000]: 100%|██████████| 9/9 [00:00<00:00, 190.16it/s, loss=1.81]\n",
      "Epoch [181/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=1.74]\n",
      "Epoch [182/3000]: 100%|██████████| 9/9 [00:00<00:00, 138.83it/s, loss=1.68]\n",
      "Epoch [183/3000]: 100%|██████████| 9/9 [00:00<00:00, 158.32it/s, loss=1.91]\n",
      "Epoch [184/3000]: 100%|██████████| 9/9 [00:00<00:00, 164.08it/s, loss=1.81]\n",
      "Epoch [185/3000]: 100%|██████████| 9/9 [00:00<00:00, 164.07it/s, loss=1.81]\n",
      "Epoch [186/3000]: 100%|██████████| 9/9 [00:00<00:00, 178.78it/s, loss=1.59]\n",
      "Epoch [187/3000]: 100%|██████████| 9/9 [00:00<00:00, 156.90it/s, loss=1.66]\n",
      "Epoch [188/3000]: 100%|██████████| 9/9 [00:00<00:00, 155.59it/s, loss=1.76]\n",
      "Epoch [189/3000]: 100%|██████████| 9/9 [00:00<00:00, 198.21it/s, loss=1.74]\n",
      "Epoch [190/3000]: 100%|██████████| 9/9 [00:00<00:00, 191.99it/s, loss=1.73]\n",
      "Epoch [191/3000]: 100%|██████████| 9/9 [00:00<00:00, 150.40it/s, loss=2.08]\n",
      "Epoch [192/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=2.06]\n",
      "Epoch [193/3000]: 100%|██████████| 9/9 [00:00<00:00, 150.40it/s, loss=1.47]\n",
      "Epoch [194/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=1.43]\n",
      "Epoch [195/3000]: 100%|██████████| 9/9 [00:00<00:00, 170.27it/s, loss=1.47]\n",
      "Epoch [196/3000]: 100%|██████████| 9/9 [00:00<00:00, 209.85it/s, loss=1.53]\n",
      "Epoch [197/3000]: 100%|██████████| 9/9 [00:00<00:00, 141.00it/s, loss=1.57]\n",
      "Epoch [198/3000]: 100%|██████████| 9/9 [00:00<00:00, 147.94it/s, loss=1.71]\n",
      "Epoch [199/3000]: 100%|██████████| 9/9 [00:00<00:00, 164.07it/s, loss=1.57]\n",
      "Epoch [200/3000]: 100%|██████████| 9/9 [00:00<00:00, 72.19it/s, loss=1.47]\n",
      "Epoch [201/3000]: 100%|██████████| 9/9 [00:00<00:00, 155.59it/s, loss=1.55]\n",
      "Epoch [202/3000]: 100%|██████████| 9/9 [00:00<00:00, 175.19it/s, loss=1.68]\n",
      "Epoch [203/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.09it/s, loss=1.54]\n",
      "Epoch [204/3000]: 100%|██████████| 9/9 [00:00<00:00, 167.38it/s, loss=1.72]\n",
      "Epoch [205/3000]: 100%|██████████| 9/9 [00:00<00:00, 158.32it/s, loss=1.65]\n",
      "Epoch [206/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.15it/s, loss=1.63]\n",
      "Epoch [207/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.09it/s, loss=1.95]\n",
      "Epoch [208/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=1.6]\n",
      "Epoch [209/3000]: 100%|██████████| 9/9 [00:00<00:00, 155.58it/s, loss=1.71]\n",
      "Epoch [210/3000]: 100%|██████████| 9/9 [00:00<00:00, 147.93it/s, loss=1.75]\n",
      "Epoch [211/3000]: 100%|██████████| 9/9 [00:00<00:00, 139.89it/s, loss=1.47]\n",
      "Epoch [212/3000]: 100%|██████████| 9/9 [00:00<00:00, 165.55it/s, loss=2.02]\n",
      "Epoch [213/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=1.54]\n",
      "Epoch [214/3000]: 100%|██████████| 9/9 [00:00<00:00, 159.69it/s, loss=1.67]\n",
      "Epoch [215/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.01it/s, loss=1.73]\n",
      "Epoch [216/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.54it/s, loss=1.5]\n",
      "Epoch [217/3000]: 100%|██████████| 9/9 [00:00<00:00, 170.26it/s, loss=1.52]\n",
      "Epoch [218/3000]: 100%|██████████| 9/9 [00:00<00:00, 134.75it/s, loss=1.5]\n",
      "Epoch [219/3000]: 100%|██████████| 9/9 [00:00<00:00, 150.41it/s, loss=1.8]\n",
      "Epoch [220/3000]: 100%|██████████| 9/9 [00:00<00:00, 170.27it/s, loss=1.76]\n",
      "Epoch [221/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=1.52]\n",
      "Epoch [222/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.54it/s, loss=1.28]\n",
      "Epoch [223/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=2.07]\n",
      "Epoch [224/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=1.73]\n",
      "Epoch [225/3000]: 100%|██████████| 9/9 [00:00<00:00, 167.11it/s, loss=2.77]\n",
      "Epoch [226/3000]: 100%|██████████| 9/9 [00:00<00:00, 164.08it/s, loss=1.61]\n",
      "Epoch [227/3000]: 100%|██████████| 9/9 [00:00<00:00, 152.95it/s, loss=1.95]\n",
      "Epoch [228/3000]: 100%|██████████| 9/9 [00:00<00:00, 182.25it/s, loss=1.7]\n",
      "Epoch [229/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.54it/s, loss=1.76]\n",
      "Epoch [230/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=1.54]\n",
      "Epoch [231/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.09it/s, loss=1.5]\n",
      "Epoch [232/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.41it/s, loss=1.32]\n",
      "Epoch [233/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.54it/s, loss=2.05]\n",
      "Epoch [234/3000]: 100%|██████████| 9/9 [00:00<00:00, 158.32it/s, loss=1.53]\n",
      "Epoch [235/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.46it/s, loss=1.41]\n",
      "Epoch [236/3000]: 100%|██████████| 9/9 [00:00<00:00, 147.94it/s, loss=1.53]\n",
      "Epoch [237/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.46it/s, loss=1.69]\n",
      "Epoch [238/3000]: 100%|██████████| 9/9 [00:00<00:00, 146.71it/s, loss=1.62]\n",
      "Epoch [239/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.95it/s, loss=1.77]\n",
      "Epoch [240/3000]: 100%|██████████| 9/9 [00:00<00:00, 214.86it/s, loss=1.73]\n",
      "Epoch [241/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.94it/s, loss=1.77]\n",
      "Epoch [242/3000]: 100%|██████████| 9/9 [00:00<00:00, 150.40it/s, loss=1.44]\n",
      "Epoch [243/3000]: 100%|██████████| 9/9 [00:00<00:00, 164.07it/s, loss=1.57]\n",
      "Epoch [244/3000]: 100%|██████████| 9/9 [00:00<00:00, 132.71it/s, loss=1.77]\n",
      "Epoch [245/3000]: 100%|██████████| 9/9 [00:00<00:00, 129.01it/s, loss=1.98]\n",
      "Epoch [246/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=1.82]\n",
      "Epoch [247/3000]: 100%|██████████| 9/9 [00:00<00:00, 164.08it/s, loss=1.33]\n",
      "Epoch [248/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.94it/s, loss=1.39]\n",
      "Epoch [249/3000]: 100%|██████████| 9/9 [00:00<00:00, 170.27it/s, loss=1.81]\n",
      "Epoch [250/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.54it/s, loss=1.76]\n",
      "Epoch [251/3000]: 100%|██████████| 9/9 [00:00<00:00, 164.07it/s, loss=1.51]\n",
      "Epoch [252/3000]: 100%|██████████| 9/9 [00:00<00:00, 167.11it/s, loss=1.41]\n",
      "Epoch [253/3000]: 100%|██████████| 9/9 [00:00<00:00, 170.27it/s, loss=1.69]\n",
      "Epoch [254/3000]: 100%|██████████| 9/9 [00:00<00:00, 151.64it/s, loss=1.5]\n",
      "Epoch [255/3000]: 100%|██████████| 9/9 [00:00<00:00, 161.15it/s, loss=1.71]\n",
      "Epoch [256/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.54it/s, loss=1.59]\n",
      "Epoch [257/3000]: 100%|██████████| 9/9 [00:00<00:00, 171.79it/s, loss=1.76]\n",
      "Epoch [258/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.09it/s, loss=1.66]\n",
      "Epoch [259/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=1.78]\n",
      "Epoch [260/3000]: 100%|██████████| 9/9 [00:00<00:00, 152.95it/s, loss=1.63]\n",
      "Epoch [261/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.61]\n",
      "Epoch [262/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.16it/s, loss=1.37]\n",
      "Epoch [263/3000]: 100%|██████████| 9/9 [00:00<00:00, 161.14it/s, loss=1.35]\n",
      "Epoch [264/3000]: 100%|██████████| 9/9 [00:00<00:00, 178.66it/s, loss=1.5]\n",
      "Epoch [265/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.53it/s, loss=1.86]\n",
      "Epoch [266/3000]: 100%|██████████| 9/9 [00:00<00:00, 70.50it/s, loss=1.42]\n",
      "Epoch [267/3000]: 100%|██████████| 9/9 [00:00<00:00, 147.94it/s, loss=1.4]\n",
      "Epoch [268/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=1.59]\n",
      "Epoch [269/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=1.78]\n",
      "Epoch [270/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=1.48]\n",
      "Epoch [271/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.01it/s, loss=1.52]\n",
      "Epoch [272/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.54it/s, loss=1.4]\n",
      "Epoch [273/3000]: 100%|██████████| 9/9 [00:00<00:00, 158.32it/s, loss=1.23]\n",
      "Epoch [274/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.94it/s, loss=1.81]\n",
      "Epoch [275/3000]: 100%|██████████| 9/9 [00:00<00:00, 170.26it/s, loss=1.62]\n",
      "Epoch [276/3000]: 100%|██████████| 9/9 [00:00<00:00, 167.12it/s, loss=1.33]\n",
      "Epoch [277/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.59]\n",
      "Epoch [278/3000]: 100%|██████████| 9/9 [00:00<00:00, 161.15it/s, loss=1.51]\n",
      "Epoch [279/3000]: 100%|██████████| 9/9 [00:00<00:00, 152.89it/s, loss=1.68]\n",
      "Epoch [280/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.94it/s, loss=1.53]\n",
      "Epoch [281/3000]: 100%|██████████| 9/9 [00:00<00:00, 128.92it/s, loss=1.71]\n",
      "Epoch [282/3000]: 100%|██████████| 9/9 [00:00<00:00, 134.69it/s, loss=1.62]\n",
      "Epoch [283/3000]: 100%|██████████| 9/9 [00:00<00:00, 220.10it/s, loss=1.29]\n",
      "Epoch [284/3000]: 100%|██████████| 9/9 [00:00<00:00, 214.86it/s, loss=1.43]\n",
      "Epoch [285/3000]: 100%|██████████| 9/9 [00:00<00:00, 167.10it/s, loss=1.45]\n",
      "Epoch [286/3000]: 100%|██████████| 9/9 [00:00<00:00, 167.11it/s, loss=1.52]\n",
      "Epoch [287/3000]: 100%|██████████| 9/9 [00:00<00:00, 181.31it/s, loss=2.27]\n",
      "Epoch [288/3000]: 100%|██████████| 9/9 [00:00<00:00, 209.85it/s, loss=1.74]\n",
      "Epoch [289/3000]: 100%|██████████| 9/9 [00:00<00:00, 194.77it/s, loss=1.51]\n",
      "Epoch [290/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=1.41]\n",
      "Epoch [291/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.94it/s, loss=1.63]\n",
      "Epoch [292/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=1.69]\n",
      "Epoch [293/3000]: 100%|██████████| 9/9 [00:00<00:00, 161.15it/s, loss=1.47]\n",
      "Epoch [294/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=2.02]\n",
      "Epoch [295/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.10it/s, loss=1.71]\n",
      "Epoch [296/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.54it/s, loss=1.5]\n",
      "Epoch [297/3000]: 100%|██████████| 9/9 [00:00<00:00, 214.86it/s, loss=1.61]\n",
      "Epoch [298/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.94it/s, loss=1.35]\n",
      "Epoch [299/3000]: 100%|██████████| 9/9 [00:00<00:00, 209.86it/s, loss=1.48]\n",
      "Epoch [300/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.12it/s, loss=1.68]\n",
      "Epoch [301/3000]: 100%|██████████| 9/9 [00:00<00:00, 220.10it/s, loss=1.56]\n",
      "Epoch [302/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.01it/s, loss=1.61]\n",
      "Epoch [303/3000]: 100%|██████████| 9/9 [00:00<00:00, 214.86it/s, loss=1.44]\n",
      "Epoch [304/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.8]\n",
      "Epoch [305/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=1.34]\n",
      "Epoch [306/3000]: 100%|██████████| 9/9 [00:00<00:00, 220.09it/s, loss=1.57]\n",
      "Epoch [307/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.49it/s, loss=1.61]\n",
      "Epoch [308/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.09it/s, loss=1.84]\n",
      "Epoch [309/3000]: 100%|██████████| 9/9 [00:00<00:00, 212.30it/s, loss=1.79]\n",
      "Epoch [310/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.04it/s, loss=1.5]\n",
      "Epoch [311/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.17it/s, loss=1.5]\n",
      "Epoch [312/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.44it/s, loss=1.41]\n",
      "Epoch [313/3000]: 100%|██████████| 9/9 [00:00<00:00, 164.04it/s, loss=1.55]\n",
      "Epoch [314/3000]: 100%|██████████| 9/9 [00:00<00:00, 209.88it/s, loss=1.53]\n",
      "Epoch [315/3000]: 100%|██████████| 9/9 [00:00<00:00, 209.86it/s, loss=1.66]\n",
      "Epoch [316/3000]: 100%|██████████| 9/9 [00:00<00:00, 157.14it/s, loss=1.6]\n",
      "Epoch [317/3000]: 100%|██████████| 9/9 [00:00<00:00, 151.64it/s, loss=1.82]\n",
      "Epoch [318/3000]: 100%|██████████| 9/9 [00:00<00:00, 209.85it/s, loss=1.65]\n",
      "Epoch [319/3000]: 100%|██████████| 9/9 [00:00<00:00, 129.83it/s, loss=1.65]\n",
      "Epoch [320/3000]: 100%|██████████| 9/9 [00:00<00:00, 115.70it/s, loss=1.67]\n",
      "Epoch [321/3000]: 100%|██████████| 9/9 [00:00<00:00, 152.95it/s, loss=1.44]\n",
      "Epoch [322/3000]: 100%|██████████| 9/9 [00:00<00:00, 150.40it/s, loss=1.91]\n",
      "Epoch [323/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.54it/s, loss=1.58]\n",
      "Epoch [324/3000]: 100%|██████████| 9/9 [00:00<00:00, 167.12it/s, loss=1.41]\n",
      "Epoch [325/3000]: 100%|██████████| 9/9 [00:00<00:00, 167.11it/s, loss=1.43]\n",
      "Epoch [326/3000]: 100%|██████████| 9/9 [00:00<00:00, 164.07it/s, loss=1.84]\n",
      "Epoch [327/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.94it/s, loss=1.54]\n",
      "Epoch [328/3000]: 100%|██████████| 9/9 [00:00<00:00, 202.39it/s, loss=1.6]\n",
      "Epoch [329/3000]: 100%|██████████| 9/9 [00:00<00:00, 140.38it/s, loss=1.68]\n",
      "Epoch [330/3000]: 100%|██████████| 9/9 [00:00<00:00, 147.10it/s, loss=1.59]\n",
      "Epoch [331/3000]: 100%|██████████| 9/9 [00:00<00:00, 189.69it/s, loss=1.67]\n",
      "Epoch [332/3000]: 100%|██████████| 9/9 [00:00<00:00, 77.13it/s, loss=1.63]\n",
      "Epoch [333/3000]: 100%|██████████| 9/9 [00:00<00:00, 155.59it/s, loss=1.72]\n",
      "Epoch [334/3000]: 100%|██████████| 9/9 [00:00<00:00, 128.92it/s, loss=1.76]\n",
      "Epoch [335/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=1.48]\n",
      "Epoch [336/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.53it/s, loss=1.44]\n",
      "Epoch [337/3000]: 100%|██████████| 9/9 [00:00<00:00, 158.32it/s, loss=2.01]\n",
      "Epoch [338/3000]: 100%|██████████| 9/9 [00:00<00:00, 152.94it/s, loss=1.38]\n",
      "Epoch [339/3000]: 100%|██████████| 9/9 [00:00<00:00, 161.13it/s, loss=1.41]\n",
      "Epoch [340/3000]: 100%|██████████| 9/9 [00:00<00:00, 152.95it/s, loss=1.75]\n",
      "Epoch [341/3000]: 100%|██████████| 9/9 [00:00<00:00, 150.41it/s, loss=1.93]\n",
      "Epoch [342/3000]: 100%|██████████| 9/9 [00:00<00:00, 121.95it/s, loss=1.47]\n",
      "Epoch [343/3000]: 100%|██████████| 9/9 [00:00<00:00, 164.07it/s, loss=1.54]\n",
      "Epoch [344/3000]: 100%|██████████| 9/9 [00:00<00:00, 118.74it/s, loss=1.46]\n",
      "Epoch [345/3000]: 100%|██████████| 9/9 [00:00<00:00, 141.00it/s, loss=1.75]\n",
      "Epoch [346/3000]: 100%|██████████| 9/9 [00:00<00:00, 214.87it/s, loss=1.44]\n",
      "Epoch [347/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.01it/s, loss=1.71]\n",
      "Epoch [348/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=1.29]\n",
      "Epoch [349/3000]: 100%|██████████| 9/9 [00:00<00:00, 164.07it/s, loss=1.63]\n",
      "Epoch [350/3000]: 100%|██████████| 9/9 [00:00<00:00, 214.86it/s, loss=1.38]\n",
      "Epoch [351/3000]: 100%|██████████| 9/9 [00:00<00:00, 214.87it/s, loss=1.51]\n",
      "Epoch [352/3000]: 100%|██████████| 9/9 [00:00<00:00, 220.10it/s, loss=2.34]\n",
      "Epoch [353/3000]: 100%|██████████| 9/9 [00:00<00:00, 170.27it/s, loss=1.45]\n",
      "Epoch [354/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=1.62]\n",
      "Epoch [355/3000]: 100%|██████████| 9/9 [00:00<00:00, 225.61it/s, loss=1.65]\n",
      "Epoch [356/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.47it/s, loss=1.67]\n",
      "Epoch [357/3000]: 100%|██████████| 9/9 [00:00<00:00, 214.86it/s, loss=2.45]\n",
      "Epoch [358/3000]: 100%|██████████| 9/9 [00:00<00:00, 209.77it/s, loss=1.44]\n",
      "Epoch [359/3000]: 100%|██████████| 9/9 [00:00<00:00, 158.32it/s, loss=2.02]\n",
      "Epoch [360/3000]: 100%|██████████| 9/9 [00:00<00:00, 214.86it/s, loss=1.44]\n",
      "Epoch [361/3000]: 100%|██████████| 9/9 [00:00<00:00, 214.86it/s, loss=1.43]\n",
      "Epoch [362/3000]: 100%|██████████| 9/9 [00:00<00:00, 225.60it/s, loss=1.65]\n",
      "Epoch [363/3000]: 100%|██████████| 9/9 [00:00<00:00, 186.03it/s, loss=1.72]\n",
      "Epoch [364/3000]: 100%|██████████| 9/9 [00:00<00:00, 214.86it/s, loss=1.63]\n",
      "Epoch [365/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=1.41]\n",
      "Epoch [366/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=1.55]\n",
      "Epoch [367/3000]: 100%|██████████| 9/9 [00:00<00:00, 214.86it/s, loss=1.66]\n",
      "Epoch [368/3000]: 100%|██████████| 9/9 [00:00<00:00, 209.85it/s, loss=1.47]\n",
      "Epoch [369/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.95it/s, loss=1.46]\n",
      "Epoch [370/3000]: 100%|██████████| 9/9 [00:00<00:00, 225.61it/s, loss=1.78]\n",
      "Epoch [371/3000]: 100%|██████████| 9/9 [00:00<00:00, 220.10it/s, loss=1.52]\n",
      "Epoch [372/3000]: 100%|██████████| 9/9 [00:00<00:00, 220.10it/s, loss=1.47]\n",
      "Epoch [373/3000]: 100%|██████████| 9/9 [00:00<00:00, 220.10it/s, loss=1.32]\n",
      "Epoch [374/3000]: 100%|██████████| 9/9 [00:00<00:00, 225.60it/s, loss=1.65]\n",
      "Epoch [375/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.54it/s, loss=1.51]\n",
      "Epoch [376/3000]: 100%|██████████| 9/9 [00:00<00:00, 220.10it/s, loss=1.88]\n",
      "Epoch [377/3000]: 100%|██████████| 9/9 [00:00<00:00, 220.11it/s, loss=1.34]\n",
      "Epoch [378/3000]: 100%|██████████| 9/9 [00:00<00:00, 214.86it/s, loss=1.6]\n",
      "Epoch [379/3000]: 100%|██████████| 9/9 [00:00<00:00, 220.10it/s, loss=1.38]\n",
      "Epoch [380/3000]: 100%|██████████| 9/9 [00:00<00:00, 214.87it/s, loss=1.31]\n",
      "Epoch [381/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.09it/s, loss=1.78]\n",
      "Epoch [382/3000]: 100%|██████████| 9/9 [00:00<00:00, 214.86it/s, loss=1.63]\n",
      "Epoch [383/3000]: 100%|██████████| 9/9 [00:00<00:00, 208.84it/s, loss=1.62]\n",
      "Epoch [384/3000]: 100%|██████████| 9/9 [00:00<00:00, 150.40it/s, loss=1.77]\n",
      "Epoch [385/3000]: 100%|██████████| 9/9 [00:00<00:00, 217.41it/s, loss=1.79]\n",
      "Epoch [386/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=1.7]\n",
      "Epoch [387/3000]: 100%|██████████| 9/9 [00:00<00:00, 187.90it/s, loss=1.43]\n",
      "Epoch [388/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.17it/s, loss=1.34]\n",
      "Epoch [389/3000]: 100%|██████████| 9/9 [00:00<00:00, 214.86it/s, loss=1.63]\n",
      "Epoch [390/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.01it/s, loss=1.62]\n",
      "Epoch [391/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=2.42]\n",
      "Epoch [392/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.06it/s, loss=1.52]\n",
      "Epoch [393/3000]: 100%|██████████| 9/9 [00:00<00:00, 132.71it/s, loss=1.46]\n",
      "Epoch [394/3000]: 100%|██████████| 9/9 [00:00<00:00, 170.27it/s, loss=1.45]\n",
      "Epoch [395/3000]: 100%|██████████| 9/9 [00:00<00:00, 182.29it/s, loss=1.67]\n",
      "Epoch [396/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.01it/s, loss=1.55]\n",
      "Epoch [397/3000]: 100%|██████████| 9/9 [00:00<00:00, 178.67it/s, loss=1.54]\n",
      "Epoch [398/3000]: 100%|██████████| 9/9 [00:00<00:00, 82.79it/s, loss=1.44]\n",
      "Epoch [399/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=1.29]\n",
      "Epoch [400/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.07it/s, loss=1.48]\n",
      "Epoch [401/3000]: 100%|██████████| 9/9 [00:00<00:00, 108.72it/s, loss=2.65]\n",
      "Epoch [402/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=1.75]\n",
      "Epoch [403/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.54it/s, loss=1.45]\n",
      "Epoch [404/3000]: 100%|██████████| 9/9 [00:00<00:00, 161.14it/s, loss=1.5]\n",
      "Epoch [405/3000]: 100%|██████████| 9/9 [00:00<00:00, 164.07it/s, loss=1.74]\n",
      "Epoch [406/3000]: 100%|██████████| 9/9 [00:00<00:00, 158.32it/s, loss=1.56]\n",
      "Epoch [407/3000]: 100%|██████████| 9/9 [00:00<00:00, 114.23it/s, loss=1.52]\n",
      "Epoch [408/3000]: 100%|██████████| 9/9 [00:00<00:00, 155.59it/s, loss=1.51]\n",
      "Epoch [409/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.94it/s, loss=1.46]\n",
      "Epoch [410/3000]: 100%|██████████| 9/9 [00:00<00:00, 167.11it/s, loss=1.24]\n",
      "Epoch [411/3000]: 100%|██████████| 9/9 [00:00<00:00, 167.09it/s, loss=1.42]\n",
      "Epoch [412/3000]: 100%|██████████| 9/9 [00:00<00:00, 121.94it/s, loss=1.57]\n",
      "Epoch [413/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.54it/s, loss=1.78]\n",
      "Epoch [414/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=1.51]\n",
      "Epoch [415/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.61]\n",
      "Epoch [416/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.06it/s, loss=1.78]\n",
      "Epoch [417/3000]: 100%|██████████| 9/9 [00:00<00:00, 112.15it/s, loss=1.3]\n",
      "Epoch [418/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=1.42]\n",
      "Epoch [419/3000]: 100%|██████████| 9/9 [00:00<00:00, 189.95it/s, loss=1.41]\n",
      "Epoch [420/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.46]\n",
      "Epoch [421/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=1.68]\n",
      "Epoch [422/3000]: 100%|██████████| 9/9 [00:00<00:00, 150.40it/s, loss=1.42]\n",
      "Epoch [423/3000]: 100%|██████████| 9/9 [00:00<00:00, 134.69it/s, loss=1.36]\n",
      "Epoch [424/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=1.14]\n",
      "Epoch [425/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=1.38]\n",
      "Epoch [426/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.43]\n",
      "Epoch [427/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.01it/s, loss=1.57]\n",
      "Epoch [428/3000]: 100%|██████████| 9/9 [00:00<00:00, 104.93it/s, loss=1.59]\n",
      "Epoch [429/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.94it/s, loss=1.32]\n",
      "Epoch [430/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.81]\n",
      "Epoch [431/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=1.18]\n",
      "Epoch [432/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=1.74]\n",
      "Epoch [433/3000]: 100%|██████████| 9/9 [00:00<00:00, 189.95it/s, loss=2.11]\n",
      "Epoch [434/3000]: 100%|██████████| 9/9 [00:00<00:00, 118.74it/s, loss=1.85]\n",
      "Epoch [435/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=1.68]\n",
      "Epoch [436/3000]: 100%|██████████| 9/9 [00:00<00:00, 191.99it/s, loss=1.67]\n",
      "Epoch [437/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.01it/s, loss=1.57]\n",
      "Epoch [438/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.54it/s, loss=1.34]\n",
      "Epoch [439/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=1.77]\n",
      "Epoch [440/3000]: 100%|██████████| 9/9 [00:00<00:00, 120.32it/s, loss=1.83]\n",
      "Epoch [441/3000]: 100%|██████████| 9/9 [00:00<00:00, 178.05it/s, loss=1.49]\n",
      "Epoch [442/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=1.51]\n",
      "Epoch [443/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.43]\n",
      "Epoch [444/3000]: 100%|██████████| 9/9 [00:00<00:00, 182.21it/s, loss=2.26]\n",
      "Epoch [445/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=1.59]\n",
      "Epoch [446/3000]: 100%|██████████| 9/9 [00:00<00:00, 187.14it/s, loss=1.78]\n",
      "Epoch [447/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.62]\n",
      "Epoch [448/3000]: 100%|██████████| 9/9 [00:00<00:00, 125.32it/s, loss=1.31]\n",
      "Epoch [449/3000]: 100%|██████████| 9/9 [00:00<00:00, 182.27it/s, loss=1.42]\n",
      "Epoch [450/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.01it/s, loss=1.63]\n",
      "Epoch [451/3000]: 100%|██████████| 9/9 [00:00<00:00, 182.28it/s, loss=1.32]\n",
      "Epoch [452/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=1.63]\n",
      "Epoch [453/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.94it/s, loss=1.73]\n",
      "Epoch [454/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.92]\n",
      "Epoch [455/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=1.77]\n",
      "Epoch [456/3000]: 100%|██████████| 9/9 [00:00<00:00, 102.55it/s, loss=1.87]\n",
      "Epoch [457/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=1.57]\n",
      "Epoch [458/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=1.44]\n",
      "Epoch [459/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=1.51]\n",
      "Epoch [460/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=1.49]\n",
      "Epoch [461/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.01it/s, loss=1.36]\n",
      "Epoch [462/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=1.81]\n",
      "Epoch [463/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=2.61]\n",
      "Epoch [464/3000]: 100%|██████████| 9/9 [00:00<00:00, 68.10it/s, loss=1.74]\n",
      "Epoch [465/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.54it/s, loss=1.82]\n",
      "Epoch [466/3000]: 100%|██████████| 9/9 [00:00<00:00, 183.86it/s, loss=1.78]\n",
      "Epoch [467/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.93it/s, loss=1.65]\n",
      "Epoch [468/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.94it/s, loss=1.96]\n",
      "Epoch [469/3000]: 100%|██████████| 9/9 [00:00<00:00, 190.88it/s, loss=1.76]\n",
      "Epoch [470/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=1.39]\n",
      "Epoch [471/3000]: 100%|██████████| 9/9 [00:00<00:00, 187.99it/s, loss=1.66]\n",
      "Epoch [472/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.63]\n",
      "Epoch [473/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.94it/s, loss=1.6]\n",
      "Epoch [474/3000]: 100%|██████████| 9/9 [00:00<00:00, 102.54it/s, loss=1.56]\n",
      "Epoch [475/3000]: 100%|██████████| 9/9 [00:00<00:00, 178.67it/s, loss=1.38]\n",
      "Epoch [476/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=2.42]\n",
      "Epoch [477/3000]: 100%|██████████| 9/9 [00:00<00:00, 175.19it/s, loss=1.62]\n",
      "Epoch [478/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=1.32]\n",
      "Epoch [479/3000]: 100%|██████████| 9/9 [00:00<00:00, 194.03it/s, loss=1.89]\n",
      "Epoch [480/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.94it/s, loss=1.58]\n",
      "Epoch [481/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.94it/s, loss=1.55]\n",
      "Epoch [482/3000]: 100%|██████████| 9/9 [00:00<00:00, 182.28it/s, loss=1.9]\n",
      "Epoch [483/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.01it/s, loss=1.74]\n",
      "Epoch [484/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.02it/s, loss=1.65]\n",
      "Epoch [485/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.72]\n",
      "Epoch [486/3000]: 100%|██████████| 9/9 [00:00<00:00, 130.78it/s, loss=1.73]\n",
      "Epoch [487/3000]: 100%|██████████| 9/9 [00:00<00:00, 164.08it/s, loss=1.48]\n",
      "Epoch [488/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.5]\n",
      "Epoch [489/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=2.08]\n",
      "Epoch [490/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.46]\n",
      "Epoch [491/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.94it/s, loss=1.61]\n",
      "Epoch [492/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.54it/s, loss=1.46]\n",
      "Epoch [493/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=1.34]\n",
      "Epoch [494/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.49it/s, loss=1.59]\n",
      "Epoch [495/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=1.39]\n",
      "Epoch [496/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=1.7]\n",
      "Epoch [497/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=1.6]\n",
      "Epoch [498/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.94it/s, loss=1.42]\n",
      "Epoch [499/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.15it/s, loss=1.68]\n",
      "Epoch [500/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.73]\n",
      "Epoch [501/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.54it/s, loss=1.41]\n",
      "Epoch [502/3000]: 100%|██████████| 9/9 [00:00<00:00, 191.99it/s, loss=1.47]\n",
      "Epoch [503/3000]: 100%|██████████| 9/9 [00:00<00:00, 104.93it/s, loss=1.48]\n",
      "Epoch [504/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=1.61]\n",
      "Epoch [505/3000]: 100%|██████████| 9/9 [00:00<00:00, 186.04it/s, loss=2.06]\n",
      "Epoch [506/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=1.61]\n",
      "Epoch [507/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.50it/s, loss=1.5]\n",
      "Epoch [508/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=1.85]\n",
      "Epoch [509/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=1.79]\n",
      "Epoch [510/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=1.48]\n",
      "Epoch [511/3000]: 100%|██████████| 9/9 [00:00<00:00, 170.26it/s, loss=1.4]\n",
      "Epoch [512/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=1.41]\n",
      "Epoch [513/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.95it/s, loss=1.59]\n",
      "Epoch [514/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=1.64]\n",
      "Epoch [515/3000]: 100%|██████████| 9/9 [00:00<00:00, 187.91it/s, loss=1.45]\n",
      "Epoch [516/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=1.94]\n",
      "Epoch [517/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.54it/s, loss=2.14]\n",
      "Epoch [518/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=1.66]\n",
      "Epoch [519/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.98]\n",
      "Epoch [520/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=1.55]\n",
      "Epoch [521/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.71]\n",
      "Epoch [522/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.53it/s, loss=1.33]\n",
      "Epoch [523/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=2.05]\n",
      "Epoch [524/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.93it/s, loss=1.54]\n",
      "Epoch [525/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=1.56]\n",
      "Epoch [526/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.52]\n",
      "Epoch [527/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.52it/s, loss=1.48]\n",
      "Epoch [528/3000]: 100%|██████████| 9/9 [00:00<00:00, 99.75it/s, loss=1.36]\n",
      "Epoch [529/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=1.54]\n",
      "Epoch [530/3000]: 100%|██████████| 9/9 [00:00<00:00, 82.04it/s, loss=1.83]\n",
      "Epoch [531/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.29]\n",
      "Epoch [532/3000]: 100%|██████████| 9/9 [00:00<00:00, 161.14it/s, loss=1.71]\n",
      "Epoch [533/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.54it/s, loss=1.71]\n",
      "Epoch [534/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.95it/s, loss=1.93]\n",
      "Epoch [535/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=1.58]\n",
      "Epoch [536/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.54it/s, loss=1.32]\n",
      "Epoch [537/3000]: 100%|██████████| 9/9 [00:00<00:00, 170.27it/s, loss=1.34]\n",
      "Epoch [538/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.54it/s, loss=1.67]\n",
      "Epoch [539/3000]: 100%|██████████| 9/9 [00:00<00:00, 150.40it/s, loss=1.34]\n",
      "Epoch [540/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.94it/s, loss=1.48]\n",
      "Epoch [541/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=1.5]\n",
      "Epoch [542/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.16it/s, loss=1.77]\n",
      "Epoch [543/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.01it/s, loss=1.61]\n",
      "Epoch [544/3000]: 100%|██████████| 9/9 [00:00<00:00, 187.99it/s, loss=2.08]\n",
      "Epoch [545/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.94it/s, loss=1.56]\n",
      "Epoch [546/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=1.43]\n",
      "Epoch [547/3000]: 100%|██████████| 9/9 [00:00<00:00, 187.98it/s, loss=1.41]\n",
      "Epoch [548/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.5]\n",
      "Epoch [549/3000]: 100%|██████████| 9/9 [00:00<00:00, 182.53it/s, loss=2.41]\n",
      "Epoch [550/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=1.43]\n",
      "Epoch [551/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=1.57]\n",
      "Epoch [552/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.48]\n",
      "Epoch [553/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.49it/s, loss=1.57]\n",
      "Epoch [554/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=1.4]\n",
      "Epoch [555/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.94it/s, loss=1.58]\n",
      "Epoch [556/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.4]\n",
      "Epoch [557/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=1.36]\n",
      "Epoch [558/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=1.63]\n",
      "Epoch [559/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=1.44]\n",
      "Epoch [560/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=1.45]\n",
      "Epoch [561/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=1.41]\n",
      "Epoch [562/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.62]\n",
      "Epoch [563/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=1.61]\n",
      "Epoch [564/3000]: 100%|██████████| 9/9 [00:00<00:00, 168.64it/s, loss=1.41]\n",
      "Epoch [565/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=1.52]\n",
      "Epoch [566/3000]: 100%|██████████| 9/9 [00:00<00:00, 158.31it/s, loss=1.44]\n",
      "Epoch [567/3000]: 100%|██████████| 9/9 [00:00<00:00, 124.44it/s, loss=1.55]\n",
      "Epoch [568/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.62]\n",
      "Epoch [569/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=1.49]\n",
      "Epoch [570/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.26]\n",
      "Epoch [571/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.54it/s, loss=1.07]\n",
      "Epoch [572/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.15it/s, loss=1.9]\n",
      "Epoch [573/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.54it/s, loss=1.61]\n",
      "Epoch [574/3000]: 100%|██████████| 9/9 [00:00<00:00, 186.03it/s, loss=1.26]\n",
      "Epoch [575/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=1.49]\n",
      "Epoch [576/3000]: 100%|██████████| 9/9 [00:00<00:00, 170.27it/s, loss=1.46]\n",
      "Epoch [577/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=1.66]\n",
      "Epoch [578/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=1.15]\n",
      "Epoch [579/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.61]\n",
      "Epoch [580/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.38]\n",
      "Epoch [581/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=1.73]\n",
      "Epoch [582/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=1.57]\n",
      "Epoch [583/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.94it/s, loss=1.45]\n",
      "Epoch [584/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=1.78]\n",
      "Epoch [585/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=1.63]\n",
      "Epoch [586/3000]: 100%|██████████| 9/9 [00:00<00:00, 167.12it/s, loss=1.7]\n",
      "Epoch [587/3000]: 100%|██████████| 9/9 [00:00<00:00, 138.83it/s, loss=1.69]\n",
      "Epoch [588/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.54it/s, loss=1.73]\n",
      "Epoch [589/3000]: 100%|██████████| 9/9 [00:00<00:00, 170.26it/s, loss=1.55]\n",
      "Epoch [590/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=1.52]\n",
      "Epoch [591/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.43]\n",
      "Epoch [592/3000]: 100%|██████████| 9/9 [00:00<00:00, 187.99it/s, loss=2.38]\n",
      "Epoch [593/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=1.74]\n",
      "Epoch [594/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=1.41]\n",
      "Epoch [595/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=1.87]\n",
      "Epoch [596/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=1.51]\n",
      "Epoch [597/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=1.65]\n",
      "Epoch [598/3000]: 100%|██████████| 9/9 [00:00<00:00, 186.23it/s, loss=1.27]\n",
      "Epoch [599/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=2.26]\n",
      "Epoch [600/3000]: 100%|██████████| 9/9 [00:00<00:00, 175.96it/s, loss=1.4]\n",
      "Epoch [601/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=1.34]\n",
      "Epoch [602/3000]: 100%|██████████| 9/9 [00:00<00:00, 171.95it/s, loss=1.62]\n",
      "Epoch [603/3000]: 100%|██████████| 9/9 [00:00<00:00, 191.35it/s, loss=1.65]\n",
      "Epoch [604/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.17it/s, loss=1.47]\n",
      "Epoch [605/3000]: 100%|██████████| 9/9 [00:00<00:00, 182.67it/s, loss=1.46]\n",
      "Epoch [606/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.63]\n",
      "Epoch [607/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.84]\n",
      "Epoch [608/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=1.35]\n",
      "Epoch [609/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.01it/s, loss=1.25]\n",
      "Epoch [610/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.39]\n",
      "Epoch [611/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=2.07]\n",
      "Epoch [612/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.47it/s, loss=1.83]\n",
      "Epoch [613/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.94it/s, loss=1.64]\n",
      "Epoch [614/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=1.55]\n",
      "Epoch [615/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.49it/s, loss=1.71]\n",
      "Epoch [616/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.01it/s, loss=1.56]\n",
      "Epoch [617/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=1.57]\n",
      "Epoch [618/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=2.08]\n",
      "Epoch [619/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.94it/s, loss=1.29]\n",
      "Epoch [620/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=2.04]\n",
      "Epoch [621/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.95it/s, loss=1.48]\n",
      "Epoch [622/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.52it/s, loss=1.48]\n",
      "Epoch [623/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.49it/s, loss=1.57]\n",
      "Epoch [624/3000]: 100%|██████████| 9/9 [00:00<00:00, 191.99it/s, loss=1.33]\n",
      "Epoch [625/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=1.44]\n",
      "Epoch [626/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=1.31]\n",
      "Epoch [627/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.49]\n",
      "Epoch [628/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.69]\n",
      "Epoch [629/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.94it/s, loss=1.61]\n",
      "Epoch [630/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.58]\n",
      "Epoch [631/3000]: 100%|██████████| 9/9 [00:00<00:00, 189.95it/s, loss=1.47]\n",
      "Epoch [632/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=1.58]\n",
      "Epoch [633/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.42]\n",
      "Epoch [634/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=2.33]\n",
      "Epoch [635/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.57]\n",
      "Epoch [636/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.01it/s, loss=1.62]\n",
      "Epoch [637/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=1.55]\n",
      "Epoch [638/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.83]\n",
      "Epoch [639/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.94it/s, loss=1.6]\n",
      "Epoch [640/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.8]\n",
      "Epoch [641/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=1.66]\n",
      "Epoch [642/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.22]\n",
      "Epoch [643/3000]: 100%|██████████| 9/9 [00:00<00:00, 143.24it/s, loss=1.76]\n",
      "Epoch [644/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.7]\n",
      "Epoch [645/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.92]\n",
      "Epoch [646/3000]: 100%|██████████| 9/9 [00:00<00:00, 182.28it/s, loss=1.34]\n",
      "Epoch [647/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.47]\n",
      "Epoch [648/3000]: 100%|██████████| 9/9 [00:00<00:00, 190.35it/s, loss=1.83]\n",
      "Epoch [649/3000]: 100%|██████████| 9/9 [00:00<00:00, 109.62it/s, loss=1.72]\n",
      "Epoch [650/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.53]\n",
      "Epoch [651/3000]: 100%|██████████| 9/9 [00:00<00:00, 187.24it/s, loss=1.5]\n",
      "Epoch [652/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.52]\n",
      "Epoch [653/3000]: 100%|██████████| 9/9 [00:00<00:00, 186.09it/s, loss=1.86]\n",
      "Epoch [654/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.89it/s, loss=1.26]\n",
      "Epoch [655/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=1.02]\n",
      "Epoch [656/3000]: 100%|██████████| 9/9 [00:00<00:00, 190.03it/s, loss=1.53]\n",
      "Epoch [657/3000]: 100%|██████████| 9/9 [00:00<00:00, 145.55it/s, loss=1.5]\n",
      "Epoch [658/3000]: 100%|██████████| 9/9 [00:00<00:00, 187.98it/s, loss=1.41]\n",
      "Epoch [659/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=1.71]\n",
      "Epoch [660/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.01it/s, loss=1.34]\n",
      "Epoch [661/3000]: 100%|██████████| 9/9 [00:00<00:00, 187.60it/s, loss=1.4]\n",
      "Epoch [662/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=1.98]\n",
      "Epoch [663/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.48]\n",
      "Epoch [664/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.56]\n",
      "Epoch [665/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.61]\n",
      "Epoch [666/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=1.45]\n",
      "Epoch [667/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=1.44]\n",
      "Epoch [668/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=1.51]\n",
      "Epoch [669/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.47]\n",
      "Epoch [670/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=1.45]\n",
      "Epoch [671/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.67]\n",
      "Epoch [672/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=1.87]\n",
      "Epoch [673/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=1.5]\n",
      "Epoch [674/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.51]\n",
      "Epoch [675/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=1.72]\n",
      "Epoch [676/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=1.32]\n",
      "Epoch [677/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=1.88]\n",
      "Epoch [678/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=1.81]\n",
      "Epoch [679/3000]: 100%|██████████| 9/9 [00:00<00:00, 171.86it/s, loss=1.57]\n",
      "Epoch [680/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=1.67]\n",
      "Epoch [681/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=1.39]\n",
      "Epoch [682/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.27it/s, loss=1.4]\n",
      "Epoch [683/3000]: 100%|██████████| 9/9 [00:00<00:00, 191.30it/s, loss=1.65]\n",
      "Epoch [684/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=1.57]\n",
      "Epoch [685/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=1.52]\n",
      "Epoch [686/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=1.78]\n",
      "Epoch [687/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=1.73]\n",
      "Epoch [688/3000]: 100%|██████████| 9/9 [00:00<00:00, 187.99it/s, loss=1.74]\n",
      "Epoch [689/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=1.36]\n",
      "Epoch [690/3000]: 100%|██████████| 9/9 [00:00<00:00, 182.27it/s, loss=1.98]\n",
      "Epoch [691/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=1.84]\n",
      "Epoch [692/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=1.22]\n",
      "Epoch [693/3000]: 100%|██████████| 9/9 [00:00<00:00, 189.95it/s, loss=1.44]\n",
      "Epoch [694/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=1.76]\n",
      "Epoch [695/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=1.45]\n",
      "Epoch [696/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=1.66]\n",
      "Epoch [697/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.53it/s, loss=1.31]\n",
      "Epoch [698/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.17it/s, loss=1.54]\n",
      "Epoch [699/3000]: 100%|██████████| 9/9 [00:00<00:00, 205.09it/s, loss=1.47]\n",
      "Epoch [700/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.49]\n",
      "Epoch [701/3000]: 100%|██████████| 9/9 [00:00<00:00, 175.05it/s, loss=1.8]\n",
      "Epoch [702/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=1.56]\n",
      "Epoch [703/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=1.67]\n",
      "Epoch [704/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=1.57]\n",
      "Epoch [705/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.59]\n",
      "Epoch [706/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.59]\n",
      "Epoch [707/3000]: 100%|██████████| 9/9 [00:00<00:00, 167.11it/s, loss=1.37]\n",
      "Epoch [708/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=1.48]\n",
      "Epoch [709/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.17it/s, loss=1.45]\n",
      "Epoch [710/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.85]\n",
      "Epoch [711/3000]: 100%|██████████| 9/9 [00:00<00:00, 191.01it/s, loss=1.64]\n",
      "Epoch [712/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=1.49]\n",
      "Epoch [713/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=1.31]\n",
      "Epoch [714/3000]: 100%|██████████| 9/9 [00:00<00:00, 179.07it/s, loss=1.49]\n",
      "Epoch [715/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.54it/s, loss=1.47]\n",
      "Epoch [716/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.34it/s, loss=1.39]\n",
      "Epoch [717/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=1.59]\n",
      "Epoch [718/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.04it/s, loss=1.34]\n",
      "Epoch [719/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=1.9]\n",
      "Epoch [720/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.49]\n",
      "Epoch [721/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=1.57]\n",
      "Epoch [722/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.28]\n",
      "Epoch [723/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.15it/s, loss=1.65]\n",
      "Epoch [724/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=1.23]\n",
      "Epoch [725/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=1.65]\n",
      "Epoch [726/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=1.17]\n",
      "Epoch [727/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=1.42]\n",
      "Epoch [728/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.36]\n",
      "Epoch [729/3000]: 100%|██████████| 9/9 [00:00<00:00, 83.56it/s, loss=1.22]\n",
      "Epoch [730/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.54it/s, loss=1.23]\n",
      "Epoch [731/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.01it/s, loss=1.74]\n",
      "Epoch [732/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.95it/s, loss=1.94]\n",
      "Epoch [733/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=1.42]\n",
      "Epoch [734/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=2.05]\n",
      "Epoch [735/3000]: 100%|██████████| 9/9 [00:00<00:00, 164.07it/s, loss=1.94]\n",
      "Epoch [736/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=1.52]\n",
      "Epoch [737/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=1.53]\n",
      "Epoch [738/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.63]\n",
      "Epoch [739/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=1.42]\n",
      "Epoch [740/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=1.97]\n",
      "Epoch [741/3000]: 100%|██████████| 9/9 [00:00<00:00, 187.39it/s, loss=1.14]\n",
      "Epoch [742/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.47it/s, loss=1.21]\n",
      "Epoch [743/3000]: 100%|██████████| 9/9 [00:00<00:00, 191.90it/s, loss=1.77]\n",
      "Epoch [744/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=1.48]\n",
      "Epoch [745/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=1.45]\n",
      "Epoch [746/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.15it/s, loss=1.36]\n",
      "Epoch [747/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=1.2]\n",
      "Epoch [748/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=1.58]\n",
      "Epoch [749/3000]: 100%|██████████| 9/9 [00:00<00:00, 187.20it/s, loss=1.3]\n",
      "Epoch [750/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.36it/s, loss=1.85]\n",
      "Epoch [751/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.54it/s, loss=1.48]\n",
      "Epoch [752/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=1.66]\n",
      "Epoch [753/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.74]\n",
      "Epoch [754/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=1.95]\n",
      "Epoch [755/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=1.68]\n",
      "Epoch [756/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.18it/s, loss=1.63]\n",
      "Epoch [757/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.63]\n",
      "Epoch [758/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=1.44]\n",
      "Epoch [759/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.6]\n",
      "Epoch [760/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=1.92]\n",
      "Epoch [761/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.01it/s, loss=1.54]\n",
      "Epoch [762/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.35]\n",
      "Epoch [763/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.84]\n",
      "Epoch [764/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=1.22]\n",
      "Epoch [765/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.45]\n",
      "Epoch [766/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=1.67]\n",
      "Epoch [767/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.28it/s, loss=1.63]\n",
      "Epoch [768/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=1.37]\n",
      "Epoch [769/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.27]\n",
      "Epoch [770/3000]: 100%|██████████| 9/9 [00:00<00:00, 178.05it/s, loss=1.28]\n",
      "Epoch [771/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=1.41]\n",
      "Epoch [772/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.68]\n",
      "Epoch [773/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.94it/s, loss=1.18]\n",
      "Epoch [774/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=1.77]\n",
      "Epoch [775/3000]: 100%|██████████| 9/9 [00:00<00:00, 189.95it/s, loss=1.72]\n",
      "Epoch [776/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.73]\n",
      "Epoch [777/3000]: 100%|██████████| 9/9 [00:00<00:00, 187.23it/s, loss=1.66]\n",
      "Epoch [778/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=1.56]\n",
      "Epoch [779/3000]: 100%|██████████| 9/9 [00:00<00:00, 182.74it/s, loss=1.51]\n",
      "Epoch [780/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.47]\n",
      "Epoch [781/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.44]\n",
      "Epoch [782/3000]: 100%|██████████| 9/9 [00:00<00:00, 175.35it/s, loss=1.3]\n",
      "Epoch [783/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=2.01]\n",
      "Epoch [784/3000]: 100%|██████████| 9/9 [00:00<00:00, 189.87it/s, loss=1.46]\n",
      "Epoch [785/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=1.82]\n",
      "Epoch [786/3000]: 100%|██████████| 9/9 [00:00<00:00, 178.67it/s, loss=1.68]\n",
      "Epoch [787/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.57]\n",
      "Epoch [788/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.95it/s, loss=1.4]\n",
      "Epoch [789/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.94it/s, loss=1.29]\n",
      "Epoch [790/3000]: 100%|██████████| 9/9 [00:00<00:00, 164.07it/s, loss=1.91]\n",
      "Epoch [791/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.94it/s, loss=1.46]\n",
      "Epoch [792/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=1.79]\n",
      "Epoch [793/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=1.75]\n",
      "Epoch [794/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.43]\n",
      "Epoch [795/3000]: 100%|██████████| 9/9 [00:00<00:00, 84.34it/s, loss=1.42]\n",
      "Epoch [796/3000]: 100%|██████████| 9/9 [00:00<00:00, 187.99it/s, loss=1.36]\n",
      "Epoch [797/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=2.61]\n",
      "Epoch [798/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=1.5]\n",
      "Epoch [799/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.49it/s, loss=1.45]\n",
      "Epoch [800/3000]: 100%|██████████| 9/9 [00:00<00:00, 185.94it/s, loss=1.35]\n",
      "Epoch [801/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=1.5]\n",
      "Epoch [802/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.15it/s, loss=1.99]\n",
      "Epoch [803/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.15it/s, loss=1.31]\n",
      "Epoch [804/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=1.54]\n",
      "Epoch [805/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=1.79]\n",
      "Epoch [806/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.42]\n",
      "Epoch [807/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.93it/s, loss=1.33]\n",
      "Epoch [808/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.54it/s, loss=1.6]\n",
      "Epoch [809/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.94it/s, loss=2.01]\n",
      "Epoch [810/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.15it/s, loss=1.74]\n",
      "Epoch [811/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=1.45]\n",
      "Epoch [812/3000]: 100%|██████████| 9/9 [00:00<00:00, 191.53it/s, loss=1.44]\n",
      "Epoch [813/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=1.35]\n",
      "Epoch [814/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.7]\n",
      "Epoch [815/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=1.86]\n",
      "Epoch [816/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.47]\n",
      "Epoch [817/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=1.18]\n",
      "Epoch [818/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.85]\n",
      "Epoch [819/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=1.27]\n",
      "Epoch [820/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=1.46]\n",
      "Epoch [821/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.35]\n",
      "Epoch [822/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=1.78]\n",
      "Epoch [823/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=1.57]\n",
      "Epoch [824/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.01it/s, loss=1.61]\n",
      "Epoch [825/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.57]\n",
      "Epoch [826/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=1.22]\n",
      "Epoch [827/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.94it/s, loss=1.21]\n",
      "Epoch [828/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=1.28]\n",
      "Epoch [829/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.10it/s, loss=2.14]\n",
      "Epoch [830/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=1.72]\n",
      "Epoch [831/3000]: 100%|██████████| 9/9 [00:00<00:00, 186.24it/s, loss=1.98]\n",
      "Epoch [832/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=1.45]\n",
      "Epoch [833/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.49it/s, loss=2.25]\n",
      "Epoch [834/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.15it/s, loss=1.54]\n",
      "Epoch [835/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=1.72]\n",
      "Epoch [836/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=1.35]\n",
      "Epoch [837/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.59]\n",
      "Epoch [838/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.16it/s, loss=2.34]\n",
      "Epoch [839/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=1.32]\n",
      "Epoch [840/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=1.51]\n",
      "Epoch [841/3000]: 100%|██████████| 9/9 [00:00<00:00, 179.43it/s, loss=1.62]\n",
      "Epoch [842/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.94it/s, loss=1.82]\n",
      "Epoch [843/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.49]\n",
      "Epoch [844/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.35]\n",
      "Epoch [845/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=1.29]\n",
      "Epoch [846/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.95it/s, loss=1.35]\n",
      "Epoch [847/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.17]\n",
      "Epoch [848/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.48]\n",
      "Epoch [849/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=1.39]\n",
      "Epoch [850/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.42]\n",
      "Epoch [851/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.47]\n",
      "Epoch [852/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=1.7]\n",
      "Epoch [853/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=1.28]\n",
      "Epoch [854/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=1.32]\n",
      "Epoch [855/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.33]\n",
      "Epoch [856/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.17it/s, loss=1.53]\n",
      "Epoch [857/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.54it/s, loss=1.29]\n",
      "Epoch [858/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.23it/s, loss=1.4]\n",
      "Epoch [859/3000]: 100%|██████████| 9/9 [00:00<00:00, 182.56it/s, loss=2.17]\n",
      "Epoch [860/3000]: 100%|██████████| 9/9 [00:00<00:00, 185.14it/s, loss=1.48]\n",
      "Epoch [861/3000]: 100%|██████████| 9/9 [00:00<00:00, 79.57it/s, loss=1.55]\n",
      "Epoch [862/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=1.48]\n",
      "Epoch [863/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=1.48]\n",
      "Epoch [864/3000]: 100%|██████████| 9/9 [00:00<00:00, 170.30it/s, loss=1.5]\n",
      "Epoch [865/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.54it/s, loss=2.19]\n",
      "Epoch [866/3000]: 100%|██████████| 9/9 [00:00<00:00, 191.54it/s, loss=1.21]\n",
      "Epoch [867/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.34]\n",
      "Epoch [868/3000]: 100%|██████████| 9/9 [00:00<00:00, 186.04it/s, loss=1.49]\n",
      "Epoch [869/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=1.56]\n",
      "Epoch [870/3000]: 100%|██████████| 9/9 [00:00<00:00, 191.93it/s, loss=1.61]\n",
      "Epoch [871/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.56]\n",
      "Epoch [872/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=1.81]\n",
      "Epoch [873/3000]: 100%|██████████| 9/9 [00:00<00:00, 103.73it/s, loss=2.04]\n",
      "Epoch [874/3000]: 100%|██████████| 9/9 [00:00<00:00, 147.94it/s, loss=1.54]\n",
      "Epoch [875/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=1.36]\n",
      "Epoch [876/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.54it/s, loss=1.87]\n",
      "Epoch [877/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.17it/s, loss=1.74]\n",
      "Epoch [878/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.17it/s, loss=1.2]\n",
      "Epoch [879/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=1.44]\n",
      "Epoch [880/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=1.28]\n",
      "Epoch [881/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=1.26]\n",
      "Epoch [882/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.54it/s, loss=1.77]\n",
      "Epoch [883/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=1.5]\n",
      "Epoch [884/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=1.6]\n",
      "Epoch [885/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=1.78]\n",
      "Epoch [886/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.43it/s, loss=1.37]\n",
      "Epoch [887/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.45]\n",
      "Epoch [888/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.72]\n",
      "Epoch [889/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=2.01]\n",
      "Epoch [890/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.62]\n",
      "Epoch [891/3000]: 100%|██████████| 9/9 [00:00<00:00, 191.99it/s, loss=1.83]\n",
      "Epoch [892/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=1.35]\n",
      "Epoch [893/3000]: 100%|██████████| 9/9 [00:00<00:00, 189.95it/s, loss=1.92]\n",
      "Epoch [894/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.66]\n",
      "Epoch [895/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.77]\n",
      "Epoch [896/3000]: 100%|██████████| 9/9 [00:00<00:00, 200.54it/s, loss=1.54]\n",
      "Epoch [897/3000]: 100%|██████████| 9/9 [00:00<00:00, 196.17it/s, loss=1.64]\n",
      "Epoch [898/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.52]\n",
      "Epoch [899/3000]: 100%|██████████| 9/9 [00:00<00:00, 164.10it/s, loss=2.61]\n",
      "Epoch [900/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=1.37]\n",
      "Epoch [901/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.47]\n",
      "Epoch [902/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=1.74]\n",
      "Epoch [903/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=1.6]\n",
      "Epoch [904/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.52]\n",
      "Epoch [905/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=2.25]\n",
      "Epoch [906/3000]: 100%|██████████| 9/9 [00:00<00:00, 164.07it/s, loss=1.38]\n",
      "Epoch [907/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=1.45]\n",
      "Epoch [908/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=1.41]\n",
      "Epoch [909/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=1.85]\n",
      "Epoch [910/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=1.4]\n",
      "Epoch [911/3000]: 100%|██████████| 9/9 [00:00<00:00, 191.93it/s, loss=1.44]\n",
      "Epoch [912/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.20it/s, loss=1.36]\n",
      "Epoch [913/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=1.33]\n",
      "Epoch [914/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.4]\n",
      "Epoch [915/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.39]\n",
      "Epoch [916/3000]: 100%|██████████| 9/9 [00:00<00:00, 178.71it/s, loss=1.65]\n",
      "Epoch [917/3000]: 100%|██████████| 9/9 [00:00<00:00, 189.95it/s, loss=1.74]\n",
      "Epoch [918/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=1.48]\n",
      "Epoch [919/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.35it/s, loss=1.62]\n",
      "Epoch [920/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.47it/s, loss=1.23]\n",
      "Epoch [921/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.54it/s, loss=1.4]\n",
      "Epoch [922/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=2.12]\n",
      "Epoch [923/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=1.51]\n",
      "Epoch [924/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=1.39]\n",
      "Epoch [925/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=1.43]\n",
      "Epoch [926/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.17]\n",
      "Epoch [927/3000]: 100%|██████████| 9/9 [00:00<00:00, 82.79it/s, loss=1.22]\n",
      "Epoch [928/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=1.78]\n",
      "Epoch [929/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=1.64]\n",
      "Epoch [930/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.55]\n",
      "Epoch [931/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=1.47]\n",
      "Epoch [932/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=1.22]\n",
      "Epoch [933/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=1.19]\n",
      "Epoch [934/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=1.32]\n",
      "Epoch [935/3000]: 100%|██████████| 9/9 [00:00<00:00, 187.99it/s, loss=1.45]\n",
      "Epoch [936/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=1.45]\n",
      "Epoch [937/3000]: 100%|██████████| 9/9 [00:00<00:00, 186.48it/s, loss=1.94]\n",
      "Epoch [938/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.54it/s, loss=1.3]\n",
      "Epoch [939/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.54it/s, loss=1.39]\n",
      "Epoch [940/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=1.34]\n",
      "Epoch [941/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.54it/s, loss=1.34]\n",
      "Epoch [942/3000]: 100%|██████████| 9/9 [00:00<00:00, 151.50it/s, loss=1.42]\n",
      "Epoch [943/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=2.06]\n",
      "Epoch [944/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.03it/s, loss=1.35]\n",
      "Epoch [945/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.48]\n",
      "Epoch [946/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=1.39]\n",
      "Epoch [947/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.54it/s, loss=1.28]\n",
      "Epoch [948/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=1.61]\n",
      "Epoch [949/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.45]\n",
      "Epoch [950/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.91]\n",
      "Epoch [951/3000]: 100%|██████████| 9/9 [00:00<00:00, 177.06it/s, loss=1.46]\n",
      "Epoch [952/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.63]\n",
      "Epoch [953/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.14it/s, loss=1.38]\n",
      "Epoch [954/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.49it/s, loss=1.51]\n",
      "Epoch [955/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.5]\n",
      "Epoch [956/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=1.49]\n",
      "Epoch [957/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.94it/s, loss=1.56]\n",
      "Epoch [958/3000]: 100%|██████████| 9/9 [00:00<00:00, 170.27it/s, loss=1.49]\n",
      "Epoch [959/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.94it/s, loss=1.54]\n",
      "Epoch [960/3000]: 100%|██████████| 9/9 [00:00<00:00, 187.98it/s, loss=1.4]\n",
      "Epoch [961/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=1.61]\n",
      "Epoch [962/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=1.37]\n",
      "Epoch [963/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.94it/s, loss=1.38]\n",
      "Epoch [964/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=1.33]\n",
      "Epoch [965/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=1.42]\n",
      "Epoch [966/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.53it/s, loss=1.15]\n",
      "Epoch [967/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.39]\n",
      "Epoch [968/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.62]\n",
      "Epoch [969/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.71]\n",
      "Epoch [970/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.64]\n",
      "Epoch [971/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.11it/s, loss=1.31]\n",
      "Epoch [972/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.36]\n",
      "Epoch [973/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=1.4]\n",
      "Epoch [974/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=1.53]\n",
      "Epoch [975/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=1.95]\n",
      "Epoch [976/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.47it/s, loss=1.61]\n",
      "Epoch [977/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=1.74]\n",
      "Epoch [978/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.94it/s, loss=2.01]\n",
      "Epoch [979/3000]: 100%|██████████| 9/9 [00:00<00:00, 152.95it/s, loss=1.39]\n",
      "Epoch [980/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=1.46]\n",
      "Epoch [981/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.6]\n",
      "Epoch [982/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=2.14]\n",
      "Epoch [983/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.46it/s, loss=1.33]\n",
      "Epoch [984/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.49it/s, loss=1.43]\n",
      "Epoch [985/3000]: 100%|██████████| 9/9 [00:00<00:00, 170.27it/s, loss=1.73]\n",
      "Epoch [986/3000]: 100%|██████████| 9/9 [00:00<00:00, 187.87it/s, loss=2.08]\n",
      "Epoch [987/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.95it/s, loss=1.4]\n",
      "Epoch [988/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=1.54]\n",
      "Epoch [989/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.01it/s, loss=1.54]\n",
      "Epoch [990/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.56it/s, loss=1.86]\n",
      "Epoch [991/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.94it/s, loss=1.43]\n",
      "Epoch [992/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=1.52]\n",
      "Epoch [993/3000]: 100%|██████████| 9/9 [00:00<00:00, 82.79it/s, loss=1.4] \n",
      "Epoch [994/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=1.97]\n",
      "Epoch [995/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.54it/s, loss=1.51]\n",
      "Epoch [996/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=1.97]\n",
      "Epoch [997/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.51]\n",
      "Epoch [998/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.01it/s, loss=1.38]\n",
      "Epoch [999/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.16it/s, loss=2.24]\n",
      "Epoch [1000/3000]: 100%|██████████| 9/9 [00:00<00:00, 192.00it/s, loss=1.57]\n",
      "Epoch [1001/3000]: 100%|██████████| 9/9 [00:00<00:00, 188.00it/s, loss=1.38]\n",
      "Epoch [1002/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.49it/s, loss=1.48]\n",
      "Epoch [1003/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=1.6]\n",
      "Epoch [1004/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=1.6]\n",
      "Epoch [1005/3000]: 100%|██████████| 9/9 [00:00<00:00, 184.17it/s, loss=1.24]\n",
      "Epoch [1006/3000]: 100%|██████████| 9/9 [00:00<00:00, 182.27it/s, loss=1.26]\n",
      "Epoch [1007/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=1.66]\n",
      "Epoch [1008/3000]: 100%|██████████| 9/9 [00:00<00:00, 175.20it/s, loss=1.83]\n",
      "Epoch [1009/3000]: 100%|██████████| 9/9 [00:00<00:00, 170.27it/s, loss=1.69]\n",
      "Epoch [1010/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=1.23]\n",
      "Epoch [1011/3000]: 100%|██████████| 9/9 [00:00<00:00, 176.73it/s, loss=1.48]\n",
      "Epoch [1012/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.54it/s, loss=1.35]\n",
      "Epoch [1013/3000]: 100%|██████████| 9/9 [00:00<00:00, 170.27it/s, loss=1.73]\n",
      "Epoch [1014/3000]: 100%|██████████| 9/9 [00:00<00:00, 158.80it/s, loss=1.63]\n",
      "Epoch [1015/3000]: 100%|██████████| 9/9 [00:00<00:00, 167.11it/s, loss=1.32]\n",
      "Epoch [1016/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.56it/s, loss=1.28]\n",
      "Epoch [1017/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.54it/s, loss=1.32]\n",
      "Epoch [1018/3000]: 100%|██████████| 9/9 [00:00<00:00, 180.48it/s, loss=1.37]\n",
      "Epoch [1019/3000]: 100%|██████████| 9/9 [00:00<00:00, 186.09it/s, loss=1.47]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3000]: Train loss: 11.1909, Valid loss: 9.9936\n",
      "Saving model with loss 9.994...\n",
      "Epoch [2/3000]: Train loss: 10.7677, Valid loss: 9.6532\n",
      "Saving model with loss 9.653...\n",
      "Epoch [3/3000]: Train loss: 10.2812, Valid loss: 9.0969\n",
      "Saving model with loss 9.097...\n",
      "Epoch [4/3000]: Train loss: 9.8599, Valid loss: 8.9209\n",
      "Saving model with loss 8.921...\n",
      "Epoch [5/3000]: Train loss: 9.4278, Valid loss: 9.0346\n",
      "Epoch [6/3000]: Train loss: 9.1335, Valid loss: 9.0417\n",
      "Epoch [7/3000]: Train loss: 8.8332, Valid loss: 8.8460\n",
      "Saving model with loss 8.846...\n",
      "Epoch [8/3000]: Train loss: 8.4810, Valid loss: 8.0892\n",
      "Saving model with loss 8.089...\n",
      "Epoch [9/3000]: Train loss: 7.9962, Valid loss: 7.8202\n",
      "Saving model with loss 7.820...\n",
      "Epoch [10/3000]: Train loss: 7.5873, Valid loss: 7.9082\n",
      "Epoch [11/3000]: Train loss: 7.2837, Valid loss: 7.4451\n",
      "Saving model with loss 7.445...\n",
      "Epoch [12/3000]: Train loss: 6.9602, Valid loss: 6.9098\n",
      "Saving model with loss 6.910...\n",
      "Epoch [13/3000]: Train loss: 6.5320, Valid loss: 6.4790\n",
      "Saving model with loss 6.479...\n",
      "Epoch [14/3000]: Train loss: 6.1879, Valid loss: 6.3900\n",
      "Saving model with loss 6.390...\n",
      "Epoch [15/3000]: Train loss: 5.8531, Valid loss: 5.7958\n",
      "Saving model with loss 5.796...\n",
      "Epoch [16/3000]: Train loss: 5.5455, Valid loss: 5.5110\n",
      "Saving model with loss 5.511...\n",
      "Epoch [17/3000]: Train loss: 5.1818, Valid loss: 5.1450\n",
      "Saving model with loss 5.145...\n",
      "Epoch [18/3000]: Train loss: 4.8799, Valid loss: 4.6541\n",
      "Saving model with loss 4.654...\n",
      "Epoch [19/3000]: Train loss: 4.5994, Valid loss: 4.0914\n",
      "Saving model with loss 4.091...\n",
      "Epoch [20/3000]: Train loss: 4.2986, Valid loss: 4.6277\n",
      "Epoch [21/3000]: Train loss: 4.1388, Valid loss: 3.7891\n",
      "Saving model with loss 3.789...\n",
      "Epoch [22/3000]: Train loss: 3.7759, Valid loss: 4.1603\n",
      "Epoch [23/3000]: Train loss: 3.5765, Valid loss: 3.5170\n",
      "Saving model with loss 3.517...\n",
      "Epoch [24/3000]: Train loss: 3.4464, Valid loss: 2.3497\n",
      "Saving model with loss 2.350...\n",
      "Epoch [25/3000]: Train loss: 3.2313, Valid loss: 3.5977\n",
      "Epoch [26/3000]: Train loss: 3.0675, Valid loss: 2.2264\n",
      "Saving model with loss 2.226...\n",
      "Epoch [27/3000]: Train loss: 2.9539, Valid loss: 2.5535\n",
      "Epoch [28/3000]: Train loss: 2.8596, Valid loss: 2.0779\n",
      "Saving model with loss 2.078...\n",
      "Epoch [29/3000]: Train loss: 2.8011, Valid loss: 2.4458\n",
      "Epoch [30/3000]: Train loss: 2.7953, Valid loss: 2.4930\n",
      "Epoch [31/3000]: Train loss: 2.6721, Valid loss: 2.7104\n",
      "Epoch [32/3000]: Train loss: 2.7141, Valid loss: 2.0847\n",
      "Epoch [33/3000]: Train loss: 2.6918, Valid loss: 2.2070\n",
      "Epoch [34/3000]: Train loss: 2.7322, Valid loss: 2.1658\n",
      "Epoch [35/3000]: Train loss: 2.7337, Valid loss: 3.1628\n",
      "Epoch [36/3000]: Train loss: 2.8380, Valid loss: 3.0926\n",
      "Epoch [37/3000]: Train loss: 2.7401, Valid loss: 1.7660\n",
      "Saving model with loss 1.766...\n",
      "Epoch [38/3000]: Train loss: 2.5928, Valid loss: 2.0042\n",
      "Epoch [39/3000]: Train loss: 2.6148, Valid loss: 1.8797\n",
      "Epoch [40/3000]: Train loss: 2.6078, Valid loss: 1.9803\n",
      "Epoch [41/3000]: Train loss: 2.5083, Valid loss: 2.3331\n",
      "Epoch [42/3000]: Train loss: 2.5288, Valid loss: 1.8710\n",
      "Epoch [43/3000]: Train loss: 2.5536, Valid loss: 2.0044\n",
      "Epoch [44/3000]: Train loss: 2.5254, Valid loss: 2.4906\n",
      "Epoch [45/3000]: Train loss: 2.6139, Valid loss: 1.9777\n",
      "Epoch [46/3000]: Train loss: 2.5718, Valid loss: 1.8447\n",
      "Epoch [47/3000]: Train loss: 2.5405, Valid loss: 1.9485\n",
      "Epoch [48/3000]: Train loss: 2.5040, Valid loss: 2.1650\n",
      "Epoch [49/3000]: Train loss: 2.4694, Valid loss: 2.1513\n",
      "Epoch [50/3000]: Train loss: 2.4640, Valid loss: 2.4092\n",
      "Epoch [51/3000]: Train loss: 2.4555, Valid loss: 1.7232\n",
      "Saving model with loss 1.723...\n",
      "Epoch [52/3000]: Train loss: 2.4939, Valid loss: 1.8639\n",
      "Epoch [53/3000]: Train loss: 2.4368, Valid loss: 1.9858\n",
      "Epoch [54/3000]: Train loss: 2.4713, Valid loss: 2.0698\n",
      "Epoch [55/3000]: Train loss: 2.4678, Valid loss: 1.6146\n",
      "Saving model with loss 1.615...\n",
      "Epoch [56/3000]: Train loss: 2.4176, Valid loss: 1.8401\n",
      "Epoch [57/3000]: Train loss: 2.4102, Valid loss: 2.1325\n",
      "Epoch [58/3000]: Train loss: 2.4992, Valid loss: 2.0718\n",
      "Epoch [59/3000]: Train loss: 2.5192, Valid loss: 2.8618\n",
      "Epoch [60/3000]: Train loss: 2.4165, Valid loss: 1.8607\n",
      "Epoch [61/3000]: Train loss: 2.3259, Valid loss: 1.8456\n",
      "Epoch [62/3000]: Train loss: 2.4640, Valid loss: 1.7450\n",
      "Epoch [63/3000]: Train loss: 2.3264, Valid loss: 1.8194\n",
      "Epoch [64/3000]: Train loss: 2.4924, Valid loss: 2.8069\n",
      "Epoch [65/3000]: Train loss: 2.4322, Valid loss: 2.6912\n",
      "Epoch [66/3000]: Train loss: 2.4046, Valid loss: 1.8231\n",
      "Epoch [67/3000]: Train loss: 2.3108, Valid loss: 1.7872\n",
      "Epoch [68/3000]: Train loss: 2.3588, Valid loss: 1.8045\n",
      "Epoch [69/3000]: Train loss: 2.3738, Valid loss: 1.9087\n",
      "Epoch [70/3000]: Train loss: 2.3926, Valid loss: 1.6582\n",
      "Epoch [71/3000]: Train loss: 2.2520, Valid loss: 1.8361\n",
      "Epoch [72/3000]: Train loss: 2.2871, Valid loss: 1.8185\n",
      "Epoch [73/3000]: Train loss: 2.2953, Valid loss: 1.9679\n",
      "Epoch [74/3000]: Train loss: 2.3374, Valid loss: 2.2739\n",
      "Epoch [75/3000]: Train loss: 2.3053, Valid loss: 1.7884\n",
      "Epoch [76/3000]: Train loss: 2.3145, Valid loss: 1.7466\n",
      "Epoch [77/3000]: Train loss: 2.2531, Valid loss: 1.7295\n",
      "Epoch [78/3000]: Train loss: 2.1441, Valid loss: 1.6189\n",
      "Epoch [79/3000]: Train loss: 2.2359, Valid loss: 1.9649\n",
      "Epoch [80/3000]: Train loss: 2.2349, Valid loss: 1.5877\n",
      "Saving model with loss 1.588...\n",
      "Epoch [81/3000]: Train loss: 2.3024, Valid loss: 1.6518\n",
      "Epoch [82/3000]: Train loss: 2.3793, Valid loss: 1.6317\n",
      "Epoch [83/3000]: Train loss: 2.2446, Valid loss: 1.8257\n",
      "Epoch [84/3000]: Train loss: 2.2322, Valid loss: 1.5714\n",
      "Saving model with loss 1.571...\n",
      "Epoch [85/3000]: Train loss: 2.1677, Valid loss: 1.5693\n",
      "Saving model with loss 1.569...\n",
      "Epoch [86/3000]: Train loss: 2.1756, Valid loss: 1.7952\n",
      "Epoch [87/3000]: Train loss: 2.2189, Valid loss: 1.5346\n",
      "Saving model with loss 1.535...\n",
      "Epoch [88/3000]: Train loss: 2.2362, Valid loss: 1.5948\n",
      "Epoch [89/3000]: Train loss: 2.0619, Valid loss: 1.7057\n",
      "Epoch [90/3000]: Train loss: 2.1205, Valid loss: 1.6908\n",
      "Epoch [91/3000]: Train loss: 2.1791, Valid loss: 1.6020\n",
      "Epoch [92/3000]: Train loss: 2.1205, Valid loss: 1.6985\n",
      "Epoch [93/3000]: Train loss: 2.1616, Valid loss: 1.5696\n",
      "Epoch [94/3000]: Train loss: 2.0731, Valid loss: 1.4509\n",
      "Saving model with loss 1.451...\n",
      "Epoch [95/3000]: Train loss: 2.0666, Valid loss: 1.4808\n",
      "Epoch [96/3000]: Train loss: 2.0281, Valid loss: 1.9227\n",
      "Epoch [97/3000]: Train loss: 2.0216, Valid loss: 1.5706\n",
      "Epoch [98/3000]: Train loss: 2.0234, Valid loss: 1.8533\n",
      "Epoch [99/3000]: Train loss: 2.0245, Valid loss: 1.6196\n",
      "Epoch [100/3000]: Train loss: 2.1243, Valid loss: 2.0347\n",
      "Epoch [101/3000]: Train loss: 2.0284, Valid loss: 2.1842\n",
      "Epoch [102/3000]: Train loss: 2.1731, Valid loss: 2.1941\n",
      "Epoch [103/3000]: Train loss: 2.0841, Valid loss: 1.6295\n",
      "Epoch [104/3000]: Train loss: 2.1130, Valid loss: 1.5161\n",
      "Epoch [105/3000]: Train loss: 2.0188, Valid loss: 1.4554\n",
      "Epoch [106/3000]: Train loss: 2.0362, Valid loss: 1.4827\n",
      "Epoch [107/3000]: Train loss: 1.9800, Valid loss: 1.6004\n",
      "Epoch [108/3000]: Train loss: 1.9350, Valid loss: 1.2885\n",
      "Saving model with loss 1.288...\n",
      "Epoch [109/3000]: Train loss: 2.0335, Valid loss: 1.6074\n",
      "Epoch [110/3000]: Train loss: 2.0069, Valid loss: 2.0135\n",
      "Epoch [111/3000]: Train loss: 2.0393, Valid loss: 1.6210\n",
      "Epoch [112/3000]: Train loss: 1.9476, Valid loss: 2.0912\n",
      "Epoch [113/3000]: Train loss: 2.0405, Valid loss: 2.0913\n",
      "Epoch [114/3000]: Train loss: 1.9320, Valid loss: 1.4935\n",
      "Epoch [115/3000]: Train loss: 1.9248, Valid loss: 1.4064\n",
      "Epoch [116/3000]: Train loss: 2.0561, Valid loss: 1.9163\n",
      "Epoch [117/3000]: Train loss: 1.9905, Valid loss: 1.5729\n",
      "Epoch [118/3000]: Train loss: 1.9642, Valid loss: 1.9965\n",
      "Epoch [119/3000]: Train loss: 1.9526, Valid loss: 1.5350\n",
      "Epoch [120/3000]: Train loss: 1.8840, Valid loss: 1.6338\n",
      "Epoch [121/3000]: Train loss: 1.9803, Valid loss: 1.2892\n",
      "Epoch [122/3000]: Train loss: 1.9339, Valid loss: 1.3333\n",
      "Epoch [123/3000]: Train loss: 1.9825, Valid loss: 1.3422\n",
      "Epoch [124/3000]: Train loss: 1.9695, Valid loss: 1.6312\n",
      "Epoch [125/3000]: Train loss: 1.8840, Valid loss: 1.4980\n",
      "Epoch [126/3000]: Train loss: 1.8875, Valid loss: 1.4959\n",
      "Epoch [127/3000]: Train loss: 1.9093, Valid loss: 1.4591\n",
      "Epoch [128/3000]: Train loss: 1.8753, Valid loss: 1.7256\n",
      "Epoch [129/3000]: Train loss: 1.9756, Valid loss: 1.3401\n",
      "Epoch [130/3000]: Train loss: 1.8865, Valid loss: 1.4052\n",
      "Epoch [131/3000]: Train loss: 1.8352, Valid loss: 1.2903\n",
      "Epoch [132/3000]: Train loss: 1.9194, Valid loss: 1.4222\n",
      "Epoch [133/3000]: Train loss: 1.8279, Valid loss: 1.2247\n",
      "Saving model with loss 1.225...\n",
      "Epoch [134/3000]: Train loss: 1.9063, Valid loss: 1.5239\n",
      "Epoch [135/3000]: Train loss: 1.9196, Valid loss: 1.5276\n",
      "Epoch [136/3000]: Train loss: 1.8616, Valid loss: 1.2132\n",
      "Saving model with loss 1.213...\n",
      "Epoch [137/3000]: Train loss: 1.9222, Valid loss: 1.2245\n",
      "Epoch [138/3000]: Train loss: 1.8366, Valid loss: 1.2830\n",
      "Epoch [139/3000]: Train loss: 1.8216, Valid loss: 1.3034\n",
      "Epoch [140/3000]: Train loss: 1.8530, Valid loss: 1.1919\n",
      "Saving model with loss 1.192...\n",
      "Epoch [141/3000]: Train loss: 1.8750, Valid loss: 1.2918\n",
      "Epoch [142/3000]: Train loss: 1.7885, Valid loss: 1.1673\n",
      "Saving model with loss 1.167...\n",
      "Epoch [143/3000]: Train loss: 2.0062, Valid loss: 1.1964\n",
      "Epoch [144/3000]: Train loss: 1.6916, Valid loss: 1.2849\n",
      "Epoch [145/3000]: Train loss: 1.7552, Valid loss: 1.1989\n",
      "Epoch [146/3000]: Train loss: 1.7545, Valid loss: 1.2372\n",
      "Epoch [147/3000]: Train loss: 1.8344, Valid loss: 1.2950\n",
      "Epoch [148/3000]: Train loss: 1.6854, Valid loss: 1.1671\n",
      "Saving model with loss 1.167...\n",
      "Epoch [149/3000]: Train loss: 1.7921, Valid loss: 1.1943\n",
      "Epoch [150/3000]: Train loss: 1.8266, Valid loss: 1.3141\n",
      "Epoch [151/3000]: Train loss: 1.7985, Valid loss: 1.4158\n",
      "Epoch [152/3000]: Train loss: 1.7896, Valid loss: 1.3505\n",
      "Epoch [153/3000]: Train loss: 1.7805, Valid loss: 1.1964\n",
      "Epoch [154/3000]: Train loss: 1.8073, Valid loss: 1.1589\n",
      "Saving model with loss 1.159...\n",
      "Epoch [155/3000]: Train loss: 1.7335, Valid loss: 1.2313\n",
      "Epoch [156/3000]: Train loss: 1.8098, Valid loss: 1.3895\n",
      "Epoch [157/3000]: Train loss: 1.6910, Valid loss: 1.3731\n",
      "Epoch [158/3000]: Train loss: 1.7382, Valid loss: 1.4653\n",
      "Epoch [159/3000]: Train loss: 1.7933, Valid loss: 1.1569\n",
      "Saving model with loss 1.157...\n",
      "Epoch [160/3000]: Train loss: 1.7744, Valid loss: 1.2030\n",
      "Epoch [161/3000]: Train loss: 1.7386, Valid loss: 1.2673\n",
      "Epoch [162/3000]: Train loss: 1.7798, Valid loss: 1.3078\n",
      "Epoch [163/3000]: Train loss: 1.7716, Valid loss: 1.1251\n",
      "Saving model with loss 1.125...\n",
      "Epoch [164/3000]: Train loss: 1.7324, Valid loss: 1.1817\n",
      "Epoch [165/3000]: Train loss: 1.7218, Valid loss: 1.1573\n",
      "Epoch [166/3000]: Train loss: 1.7138, Valid loss: 1.1450\n",
      "Epoch [167/3000]: Train loss: 1.7112, Valid loss: 1.2593\n",
      "Epoch [168/3000]: Train loss: 1.7050, Valid loss: 1.1437\n",
      "Epoch [169/3000]: Train loss: 1.8273, Valid loss: 1.1985\n",
      "Epoch [170/3000]: Train loss: 1.7265, Valid loss: 1.1243\n",
      "Saving model with loss 1.124...\n",
      "Epoch [171/3000]: Train loss: 1.7083, Valid loss: 1.0062\n",
      "Saving model with loss 1.006...\n",
      "Epoch [172/3000]: Train loss: 1.7483, Valid loss: 1.1747\n",
      "Epoch [173/3000]: Train loss: 1.6821, Valid loss: 1.3292\n",
      "Epoch [174/3000]: Train loss: 1.7279, Valid loss: 1.1716\n",
      "Epoch [175/3000]: Train loss: 1.7021, Valid loss: 1.1186\n",
      "Epoch [176/3000]: Train loss: 1.8011, Valid loss: 1.1293\n",
      "Epoch [177/3000]: Train loss: 1.6521, Valid loss: 1.1863\n",
      "Epoch [178/3000]: Train loss: 1.7189, Valid loss: 1.3051\n",
      "Epoch [179/3000]: Train loss: 1.7433, Valid loss: 1.3086\n",
      "Epoch [180/3000]: Train loss: 1.5950, Valid loss: 1.0949\n",
      "Epoch [181/3000]: Train loss: 1.7301, Valid loss: 1.1272\n",
      "Epoch [182/3000]: Train loss: 1.6476, Valid loss: 1.1131\n",
      "Epoch [183/3000]: Train loss: 1.6400, Valid loss: 1.2270\n",
      "Epoch [184/3000]: Train loss: 1.6370, Valid loss: 1.1459\n",
      "Epoch [185/3000]: Train loss: 1.6219, Valid loss: 1.3487\n",
      "Epoch [186/3000]: Train loss: 1.6217, Valid loss: 1.1385\n",
      "Epoch [187/3000]: Train loss: 1.7029, Valid loss: 1.2778\n",
      "Epoch [188/3000]: Train loss: 1.6897, Valid loss: 1.2838\n",
      "Epoch [189/3000]: Train loss: 1.7066, Valid loss: 1.1520\n",
      "Epoch [190/3000]: Train loss: 1.5968, Valid loss: 1.1637\n",
      "Epoch [191/3000]: Train loss: 1.6588, Valid loss: 1.2282\n",
      "Epoch [192/3000]: Train loss: 1.6993, Valid loss: 1.1751\n",
      "Epoch [193/3000]: Train loss: 1.6088, Valid loss: 1.1067\n",
      "Epoch [194/3000]: Train loss: 1.6671, Valid loss: 1.3029\n",
      "Epoch [195/3000]: Train loss: 1.6629, Valid loss: 1.2795\n",
      "Epoch [196/3000]: Train loss: 1.5923, Valid loss: 1.1592\n",
      "Epoch [197/3000]: Train loss: 1.6891, Valid loss: 1.0500\n",
      "Epoch [198/3000]: Train loss: 1.6527, Valid loss: 1.1798\n",
      "Epoch [199/3000]: Train loss: 1.6862, Valid loss: 1.0469\n",
      "Epoch [200/3000]: Train loss: 1.6708, Valid loss: 1.1183\n",
      "Epoch [201/3000]: Train loss: 1.5934, Valid loss: 1.1193\n",
      "Epoch [202/3000]: Train loss: 1.6275, Valid loss: 1.1171\n",
      "Epoch [203/3000]: Train loss: 1.6083, Valid loss: 1.0559\n",
      "Epoch [204/3000]: Train loss: 1.5909, Valid loss: 1.4018\n",
      "Epoch [205/3000]: Train loss: 1.6560, Valid loss: 1.1264\n",
      "Epoch [206/3000]: Train loss: 1.6381, Valid loss: 1.3002\n",
      "Epoch [207/3000]: Train loss: 1.6585, Valid loss: 1.0664\n",
      "Epoch [208/3000]: Train loss: 1.6025, Valid loss: 1.1661\n",
      "Epoch [209/3000]: Train loss: 1.6786, Valid loss: 1.3436\n",
      "Epoch [210/3000]: Train loss: 1.5868, Valid loss: 1.1266\n",
      "Epoch [211/3000]: Train loss: 1.5485, Valid loss: 1.0715\n",
      "Epoch [212/3000]: Train loss: 1.6624, Valid loss: 1.0815\n",
      "Epoch [213/3000]: Train loss: 1.8258, Valid loss: 1.3103\n",
      "Epoch [214/3000]: Train loss: 1.6294, Valid loss: 1.1312\n",
      "Epoch [215/3000]: Train loss: 1.5743, Valid loss: 1.2184\n",
      "Epoch [216/3000]: Train loss: 1.5610, Valid loss: 1.3420\n",
      "Epoch [217/3000]: Train loss: 1.5770, Valid loss: 1.0958\n",
      "Epoch [218/3000]: Train loss: 1.5742, Valid loss: 1.1591\n",
      "Epoch [219/3000]: Train loss: 1.5817, Valid loss: 1.2074\n",
      "Epoch [220/3000]: Train loss: 1.6571, Valid loss: 1.1176\n",
      "Epoch [221/3000]: Train loss: 1.5990, Valid loss: 1.1379\n",
      "Epoch [222/3000]: Train loss: 1.5611, Valid loss: 1.0225\n",
      "Epoch [223/3000]: Train loss: 1.6144, Valid loss: 1.0621\n",
      "Epoch [224/3000]: Train loss: 1.5996, Valid loss: 0.9960\n",
      "Saving model with loss 0.996...\n",
      "Epoch [225/3000]: Train loss: 1.8075, Valid loss: 1.1406\n",
      "Epoch [226/3000]: Train loss: 1.5766, Valid loss: 1.1315\n",
      "Epoch [227/3000]: Train loss: 1.6694, Valid loss: 1.1393\n",
      "Epoch [228/3000]: Train loss: 1.5797, Valid loss: 1.1570\n",
      "Epoch [229/3000]: Train loss: 1.6273, Valid loss: 1.1080\n",
      "Epoch [230/3000]: Train loss: 1.5944, Valid loss: 1.1540\n",
      "Epoch [231/3000]: Train loss: 1.6430, Valid loss: 1.0727\n",
      "Epoch [232/3000]: Train loss: 1.5283, Valid loss: 1.0782\n",
      "Epoch [233/3000]: Train loss: 1.6293, Valid loss: 1.1910\n",
      "Epoch [234/3000]: Train loss: 1.5758, Valid loss: 1.0854\n",
      "Epoch [235/3000]: Train loss: 1.5780, Valid loss: 1.1511\n",
      "Epoch [236/3000]: Train loss: 1.5457, Valid loss: 1.1582\n",
      "Epoch [237/3000]: Train loss: 1.6945, Valid loss: 1.1530\n",
      "Epoch [238/3000]: Train loss: 1.6076, Valid loss: 1.0916\n",
      "Epoch [239/3000]: Train loss: 1.5584, Valid loss: 1.0450\n",
      "Epoch [240/3000]: Train loss: 1.5802, Valid loss: 1.1733\n",
      "Epoch [241/3000]: Train loss: 1.5713, Valid loss: 1.1809\n",
      "Epoch [242/3000]: Train loss: 1.6329, Valid loss: 1.0953\n",
      "Epoch [243/3000]: Train loss: 1.5610, Valid loss: 1.2665\n",
      "Epoch [244/3000]: Train loss: 1.6112, Valid loss: 1.2129\n",
      "Epoch [245/3000]: Train loss: 1.5840, Valid loss: 1.2287\n",
      "Epoch [246/3000]: Train loss: 1.6227, Valid loss: 1.3826\n",
      "Epoch [247/3000]: Train loss: 1.5770, Valid loss: 1.0942\n",
      "Epoch [248/3000]: Train loss: 1.5278, Valid loss: 1.1337\n",
      "Epoch [249/3000]: Train loss: 1.6091, Valid loss: 1.0673\n",
      "Epoch [250/3000]: Train loss: 1.5828, Valid loss: 1.0659\n",
      "Epoch [251/3000]: Train loss: 1.5588, Valid loss: 1.1260\n",
      "Epoch [252/3000]: Train loss: 1.5610, Valid loss: 1.0714\n",
      "Epoch [253/3000]: Train loss: 1.5350, Valid loss: 1.1784\n",
      "Epoch [254/3000]: Train loss: 1.5643, Valid loss: 1.1796\n",
      "Epoch [255/3000]: Train loss: 1.5757, Valid loss: 1.1961\n",
      "Epoch [256/3000]: Train loss: 1.5540, Valid loss: 1.1060\n",
      "Epoch [257/3000]: Train loss: 1.5832, Valid loss: 1.0818\n",
      "Epoch [258/3000]: Train loss: 1.5799, Valid loss: 1.0651\n",
      "Epoch [259/3000]: Train loss: 1.5552, Valid loss: 1.1019\n",
      "Epoch [260/3000]: Train loss: 1.6477, Valid loss: 1.1529\n",
      "Epoch [261/3000]: Train loss: 1.5784, Valid loss: 1.0427\n",
      "Epoch [262/3000]: Train loss: 1.5448, Valid loss: 1.2361\n",
      "Epoch [263/3000]: Train loss: 1.6098, Valid loss: 1.0865\n",
      "Epoch [264/3000]: Train loss: 1.6135, Valid loss: 1.1150\n",
      "Epoch [265/3000]: Train loss: 1.6417, Valid loss: 1.1303\n",
      "Epoch [266/3000]: Train loss: 1.6393, Valid loss: 1.2301\n",
      "Epoch [267/3000]: Train loss: 1.5616, Valid loss: 1.1435\n",
      "Epoch [268/3000]: Train loss: 1.5199, Valid loss: 1.1883\n",
      "Epoch [269/3000]: Train loss: 1.5938, Valid loss: 1.1539\n",
      "Epoch [270/3000]: Train loss: 1.5139, Valid loss: 1.1417\n",
      "Epoch [271/3000]: Train loss: 1.5742, Valid loss: 1.0963\n",
      "Epoch [272/3000]: Train loss: 1.4612, Valid loss: 1.0870\n",
      "Epoch [273/3000]: Train loss: 1.5385, Valid loss: 1.0939\n",
      "Epoch [274/3000]: Train loss: 1.5569, Valid loss: 1.1405\n",
      "Epoch [275/3000]: Train loss: 1.5873, Valid loss: 1.1798\n",
      "Epoch [276/3000]: Train loss: 1.5207, Valid loss: 1.0912\n",
      "Epoch [277/3000]: Train loss: 1.5698, Valid loss: 1.1110\n",
      "Epoch [278/3000]: Train loss: 1.6062, Valid loss: 1.0653\n",
      "Epoch [279/3000]: Train loss: 1.5577, Valid loss: 1.1706\n",
      "Epoch [280/3000]: Train loss: 1.5816, Valid loss: 1.0714\n",
      "Epoch [281/3000]: Train loss: 1.5601, Valid loss: 1.0904\n",
      "Epoch [282/3000]: Train loss: 1.5946, Valid loss: 1.2644\n",
      "Epoch [283/3000]: Train loss: 1.5095, Valid loss: 1.0748\n",
      "Epoch [284/3000]: Train loss: 1.5836, Valid loss: 1.0717\n",
      "Epoch [285/3000]: Train loss: 1.5458, Valid loss: 1.1124\n",
      "Epoch [286/3000]: Train loss: 1.6015, Valid loss: 1.2047\n",
      "Epoch [287/3000]: Train loss: 1.6152, Valid loss: 1.0854\n",
      "Epoch [288/3000]: Train loss: 1.5853, Valid loss: 1.0424\n",
      "Epoch [289/3000]: Train loss: 1.5056, Valid loss: 1.0139\n",
      "Epoch [290/3000]: Train loss: 1.5151, Valid loss: 1.1211\n",
      "Epoch [291/3000]: Train loss: 1.5397, Valid loss: 1.0245\n",
      "Epoch [292/3000]: Train loss: 1.5561, Valid loss: 1.0904\n",
      "Epoch [293/3000]: Train loss: 1.5547, Valid loss: 1.0670\n",
      "Epoch [294/3000]: Train loss: 1.6250, Valid loss: 1.3321\n",
      "Epoch [295/3000]: Train loss: 1.7199, Valid loss: 1.0657\n",
      "Epoch [296/3000]: Train loss: 1.5349, Valid loss: 1.1287\n",
      "Epoch [297/3000]: Train loss: 1.5232, Valid loss: 1.1250\n",
      "Epoch [298/3000]: Train loss: 1.5317, Valid loss: 1.1485\n",
      "Epoch [299/3000]: Train loss: 1.6262, Valid loss: 1.1670\n",
      "Epoch [300/3000]: Train loss: 1.5132, Valid loss: 1.0187\n",
      "Epoch [301/3000]: Train loss: 1.6021, Valid loss: 1.1709\n",
      "Epoch [302/3000]: Train loss: 1.5277, Valid loss: 1.1347\n",
      "Epoch [303/3000]: Train loss: 1.5722, Valid loss: 1.0490\n",
      "Epoch [304/3000]: Train loss: 1.5403, Valid loss: 1.0587\n",
      "Epoch [305/3000]: Train loss: 1.5495, Valid loss: 1.0953\n",
      "Epoch [306/3000]: Train loss: 1.5743, Valid loss: 1.0913\n",
      "Epoch [307/3000]: Train loss: 1.5279, Valid loss: 1.1526\n",
      "Epoch [308/3000]: Train loss: 1.6259, Valid loss: 1.0111\n",
      "Epoch [309/3000]: Train loss: 1.5316, Valid loss: 1.0868\n",
      "Epoch [310/3000]: Train loss: 1.5612, Valid loss: 1.0632\n",
      "Epoch [311/3000]: Train loss: 1.5351, Valid loss: 1.0378\n",
      "Epoch [312/3000]: Train loss: 1.4727, Valid loss: 1.1673\n",
      "Epoch [313/3000]: Train loss: 1.4863, Valid loss: 1.0318\n",
      "Epoch [314/3000]: Train loss: 1.5408, Valid loss: 1.1157\n",
      "Epoch [315/3000]: Train loss: 1.5664, Valid loss: 1.0327\n",
      "Epoch [316/3000]: Train loss: 1.5212, Valid loss: 1.0680\n",
      "Epoch [317/3000]: Train loss: 1.5592, Valid loss: 1.2854\n",
      "Epoch [318/3000]: Train loss: 1.5850, Valid loss: 1.1620\n",
      "Epoch [319/3000]: Train loss: 1.5927, Valid loss: 1.3181\n",
      "Epoch [320/3000]: Train loss: 1.5589, Valid loss: 1.1019\n",
      "Epoch [321/3000]: Train loss: 1.5847, Valid loss: 1.0570\n",
      "Epoch [322/3000]: Train loss: 1.5566, Valid loss: 1.1941\n",
      "Epoch [323/3000]: Train loss: 1.5699, Valid loss: 1.1137\n",
      "Epoch [324/3000]: Train loss: 1.5612, Valid loss: 1.1877\n",
      "Epoch [325/3000]: Train loss: 1.5461, Valid loss: 1.1858\n",
      "Epoch [326/3000]: Train loss: 1.5952, Valid loss: 1.2148\n",
      "Epoch [327/3000]: Train loss: 1.5487, Valid loss: 1.1414\n",
      "Epoch [328/3000]: Train loss: 1.5913, Valid loss: 1.0560\n",
      "Epoch [329/3000]: Train loss: 1.5637, Valid loss: 1.1415\n",
      "Epoch [330/3000]: Train loss: 1.5687, Valid loss: 1.0600\n",
      "Epoch [331/3000]: Train loss: 1.5021, Valid loss: 1.2323\n",
      "Epoch [332/3000]: Train loss: 1.5589, Valid loss: 1.0065\n",
      "Epoch [333/3000]: Train loss: 1.6776, Valid loss: 0.9621\n",
      "Saving model with loss 0.962...\n",
      "Epoch [334/3000]: Train loss: 1.5147, Valid loss: 1.0439\n",
      "Epoch [335/3000]: Train loss: 1.5554, Valid loss: 1.1444\n",
      "Epoch [336/3000]: Train loss: 1.5364, Valid loss: 1.1046\n",
      "Epoch [337/3000]: Train loss: 1.6018, Valid loss: 1.0682\n",
      "Epoch [338/3000]: Train loss: 1.4481, Valid loss: 1.1940\n",
      "Epoch [339/3000]: Train loss: 1.5280, Valid loss: 1.0627\n",
      "Epoch [340/3000]: Train loss: 1.4813, Valid loss: 1.1017\n",
      "Epoch [341/3000]: Train loss: 1.5938, Valid loss: 1.0972\n",
      "Epoch [342/3000]: Train loss: 1.6019, Valid loss: 1.0907\n",
      "Epoch [343/3000]: Train loss: 1.5699, Valid loss: 1.1302\n",
      "Epoch [344/3000]: Train loss: 1.5427, Valid loss: 1.2179\n",
      "Epoch [345/3000]: Train loss: 1.5208, Valid loss: 1.1100\n",
      "Epoch [346/3000]: Train loss: 1.5139, Valid loss: 1.0535\n",
      "Epoch [347/3000]: Train loss: 1.5135, Valid loss: 1.0479\n",
      "Epoch [348/3000]: Train loss: 1.4881, Valid loss: 1.0802\n",
      "Epoch [349/3000]: Train loss: 1.4730, Valid loss: 1.1067\n",
      "Epoch [350/3000]: Train loss: 1.5628, Valid loss: 1.0974\n",
      "Epoch [351/3000]: Train loss: 1.5121, Valid loss: 1.0308\n",
      "Epoch [352/3000]: Train loss: 1.6727, Valid loss: 1.0619\n",
      "Epoch [353/3000]: Train loss: 1.4831, Valid loss: 1.0584\n",
      "Epoch [354/3000]: Train loss: 1.4872, Valid loss: 1.0245\n",
      "Epoch [355/3000]: Train loss: 1.5071, Valid loss: 1.2455\n",
      "Epoch [356/3000]: Train loss: 1.5275, Valid loss: 1.1729\n",
      "Epoch [357/3000]: Train loss: 1.6565, Valid loss: 1.0992\n",
      "Epoch [358/3000]: Train loss: 1.4859, Valid loss: 1.0835\n",
      "Epoch [359/3000]: Train loss: 1.6271, Valid loss: 1.0707\n",
      "Epoch [360/3000]: Train loss: 1.5125, Valid loss: 1.0278\n",
      "Epoch [361/3000]: Train loss: 1.4908, Valid loss: 1.1851\n",
      "Epoch [362/3000]: Train loss: 1.6018, Valid loss: 1.2092\n",
      "Epoch [363/3000]: Train loss: 1.4699, Valid loss: 1.1014\n",
      "Epoch [364/3000]: Train loss: 1.5546, Valid loss: 1.1289\n",
      "Epoch [365/3000]: Train loss: 1.5249, Valid loss: 1.1094\n",
      "Epoch [366/3000]: Train loss: 1.4990, Valid loss: 1.0110\n",
      "Epoch [367/3000]: Train loss: 1.5731, Valid loss: 1.0640\n",
      "Epoch [368/3000]: Train loss: 1.5016, Valid loss: 1.1998\n",
      "Epoch [369/3000]: Train loss: 1.5422, Valid loss: 1.1573\n",
      "Epoch [370/3000]: Train loss: 1.5218, Valid loss: 1.1101\n",
      "Epoch [371/3000]: Train loss: 1.5004, Valid loss: 1.0503\n",
      "Epoch [372/3000]: Train loss: 1.5599, Valid loss: 1.2918\n",
      "Epoch [373/3000]: Train loss: 1.4948, Valid loss: 1.1075\n",
      "Epoch [374/3000]: Train loss: 1.4895, Valid loss: 1.1700\n",
      "Epoch [375/3000]: Train loss: 1.4941, Valid loss: 1.1495\n",
      "Epoch [376/3000]: Train loss: 1.5139, Valid loss: 1.0348\n",
      "Epoch [377/3000]: Train loss: 1.4848, Valid loss: 1.1568\n",
      "Epoch [378/3000]: Train loss: 1.6037, Valid loss: 1.1444\n",
      "Epoch [379/3000]: Train loss: 1.5172, Valid loss: 1.1053\n",
      "Epoch [380/3000]: Train loss: 1.5566, Valid loss: 1.1410\n",
      "Epoch [381/3000]: Train loss: 1.5862, Valid loss: 1.3669\n",
      "Epoch [382/3000]: Train loss: 1.4951, Valid loss: 1.0042\n",
      "Epoch [383/3000]: Train loss: 1.5167, Valid loss: 1.1899\n",
      "Epoch [384/3000]: Train loss: 1.5868, Valid loss: 1.1983\n",
      "Epoch [385/3000]: Train loss: 1.5444, Valid loss: 1.0856\n",
      "Epoch [386/3000]: Train loss: 1.5282, Valid loss: 1.0785\n",
      "Epoch [387/3000]: Train loss: 1.5525, Valid loss: 1.0480\n",
      "Epoch [388/3000]: Train loss: 1.4922, Valid loss: 1.1306\n",
      "Epoch [389/3000]: Train loss: 1.5536, Valid loss: 1.0915\n",
      "Epoch [390/3000]: Train loss: 1.5152, Valid loss: 1.0571\n",
      "Epoch [391/3000]: Train loss: 1.6566, Valid loss: 1.1182\n",
      "Epoch [392/3000]: Train loss: 1.5288, Valid loss: 1.0600\n",
      "Epoch [393/3000]: Train loss: 1.5243, Valid loss: 1.1139\n",
      "Epoch [394/3000]: Train loss: 1.5354, Valid loss: 1.0902\n",
      "Epoch [395/3000]: Train loss: 1.5241, Valid loss: 1.1264\n",
      "Epoch [396/3000]: Train loss: 1.5569, Valid loss: 1.2308\n",
      "Epoch [397/3000]: Train loss: 1.4753, Valid loss: 1.1193\n",
      "Epoch [398/3000]: Train loss: 1.5080, Valid loss: 1.2281\n",
      "Epoch [399/3000]: Train loss: 1.5863, Valid loss: 1.0510\n",
      "Epoch [400/3000]: Train loss: 1.4659, Valid loss: 1.1497\n",
      "Epoch [401/3000]: Train loss: 1.6027, Valid loss: 1.0623\n",
      "Epoch [402/3000]: Train loss: 1.5772, Valid loss: 1.2655\n",
      "Epoch [403/3000]: Train loss: 1.4530, Valid loss: 1.3044\n",
      "Epoch [404/3000]: Train loss: 1.4953, Valid loss: 1.1380\n",
      "Epoch [405/3000]: Train loss: 1.5425, Valid loss: 1.0331\n",
      "Epoch [406/3000]: Train loss: 1.5203, Valid loss: 1.1694\n",
      "Epoch [407/3000]: Train loss: 1.4758, Valid loss: 0.9480\n",
      "Saving model with loss 0.948...\n",
      "Epoch [408/3000]: Train loss: 1.5631, Valid loss: 1.1256\n",
      "Epoch [409/3000]: Train loss: 1.5133, Valid loss: 1.0368\n",
      "Epoch [410/3000]: Train loss: 1.4903, Valid loss: 1.2085\n",
      "Epoch [411/3000]: Train loss: 1.5420, Valid loss: 1.1010\n",
      "Epoch [412/3000]: Train loss: 1.6174, Valid loss: 1.1446\n",
      "Epoch [413/3000]: Train loss: 1.5870, Valid loss: 1.0773\n",
      "Epoch [414/3000]: Train loss: 1.5723, Valid loss: 1.1103\n",
      "Epoch [415/3000]: Train loss: 1.5338, Valid loss: 1.0737\n",
      "Epoch [416/3000]: Train loss: 1.6338, Valid loss: 1.3034\n",
      "Epoch [417/3000]: Train loss: 1.5670, Valid loss: 1.0960\n",
      "Epoch [418/3000]: Train loss: 1.5132, Valid loss: 1.0636\n",
      "Epoch [419/3000]: Train loss: 1.5766, Valid loss: 1.1123\n",
      "Epoch [420/3000]: Train loss: 1.4732, Valid loss: 1.1277\n",
      "Epoch [421/3000]: Train loss: 1.6226, Valid loss: 1.3151\n",
      "Epoch [422/3000]: Train loss: 1.4623, Valid loss: 1.1082\n",
      "Epoch [423/3000]: Train loss: 1.5214, Valid loss: 1.0866\n",
      "Epoch [424/3000]: Train loss: 1.4875, Valid loss: 1.4316\n",
      "Epoch [425/3000]: Train loss: 1.4615, Valid loss: 1.1043\n",
      "Epoch [426/3000]: Train loss: 1.4696, Valid loss: 1.0607\n",
      "Epoch [427/3000]: Train loss: 1.5583, Valid loss: 1.1207\n",
      "Epoch [428/3000]: Train loss: 1.5295, Valid loss: 1.0315\n",
      "Epoch [429/3000]: Train loss: 1.5727, Valid loss: 1.1131\n",
      "Epoch [430/3000]: Train loss: 1.5423, Valid loss: 1.0819\n",
      "Epoch [431/3000]: Train loss: 1.4671, Valid loss: 1.2258\n",
      "Epoch [432/3000]: Train loss: 1.5879, Valid loss: 1.0644\n",
      "Epoch [433/3000]: Train loss: 1.5556, Valid loss: 1.0957\n",
      "Epoch [434/3000]: Train loss: 1.6299, Valid loss: 1.1005\n",
      "Epoch [435/3000]: Train loss: 1.5332, Valid loss: 1.0984\n",
      "Epoch [436/3000]: Train loss: 1.5687, Valid loss: 1.1938\n",
      "Epoch [437/3000]: Train loss: 1.4914, Valid loss: 1.1199\n",
      "Epoch [438/3000]: Train loss: 1.5166, Valid loss: 1.0732\n",
      "Epoch [439/3000]: Train loss: 1.5241, Valid loss: 1.1070\n",
      "Epoch [440/3000]: Train loss: 1.6255, Valid loss: 1.1724\n",
      "Epoch [441/3000]: Train loss: 1.4635, Valid loss: 1.0476\n",
      "Epoch [442/3000]: Train loss: 1.5220, Valid loss: 1.2318\n",
      "Epoch [443/3000]: Train loss: 1.5065, Valid loss: 1.0846\n",
      "Epoch [444/3000]: Train loss: 1.5948, Valid loss: 1.0444\n",
      "Epoch [445/3000]: Train loss: 1.5511, Valid loss: 1.0336\n",
      "Epoch [446/3000]: Train loss: 1.5423, Valid loss: 1.1582\n",
      "Epoch [447/3000]: Train loss: 1.4840, Valid loss: 1.1575\n",
      "Epoch [448/3000]: Train loss: 1.5071, Valid loss: 1.0462\n",
      "Epoch [449/3000]: Train loss: 1.5120, Valid loss: 1.0715\n",
      "Epoch [450/3000]: Train loss: 1.5227, Valid loss: 1.0900\n",
      "Epoch [451/3000]: Train loss: 1.5162, Valid loss: 1.1956\n",
      "Epoch [452/3000]: Train loss: 1.5899, Valid loss: 1.2797\n",
      "Epoch [453/3000]: Train loss: 1.5085, Valid loss: 1.3124\n",
      "Epoch [454/3000]: Train loss: 1.5275, Valid loss: 1.0219\n",
      "Epoch [455/3000]: Train loss: 1.5222, Valid loss: 1.1718\n",
      "Epoch [456/3000]: Train loss: 1.5176, Valid loss: 1.0580\n",
      "Epoch [457/3000]: Train loss: 1.5721, Valid loss: 1.0393\n",
      "Epoch [458/3000]: Train loss: 1.4944, Valid loss: 1.0452\n",
      "Epoch [459/3000]: Train loss: 1.5448, Valid loss: 1.0888\n",
      "Epoch [460/3000]: Train loss: 1.4853, Valid loss: 1.2128\n",
      "Epoch [461/3000]: Train loss: 1.4599, Valid loss: 1.0541\n",
      "Epoch [462/3000]: Train loss: 1.5406, Valid loss: 1.2757\n",
      "Epoch [463/3000]: Train loss: 1.6416, Valid loss: 1.0372\n",
      "Epoch [464/3000]: Train loss: 1.5516, Valid loss: 1.1114\n",
      "Epoch [465/3000]: Train loss: 1.6291, Valid loss: 1.1105\n",
      "Epoch [466/3000]: Train loss: 1.5412, Valid loss: 1.0986\n",
      "Epoch [467/3000]: Train loss: 1.4886, Valid loss: 1.0395\n",
      "Epoch [468/3000]: Train loss: 1.5722, Valid loss: 1.2876\n",
      "Epoch [469/3000]: Train loss: 1.5390, Valid loss: 1.0278\n",
      "Epoch [470/3000]: Train loss: 1.5611, Valid loss: 1.1153\n",
      "Epoch [471/3000]: Train loss: 1.5593, Valid loss: 1.2079\n",
      "Epoch [472/3000]: Train loss: 1.5089, Valid loss: 1.0498\n",
      "Epoch [473/3000]: Train loss: 1.4878, Valid loss: 1.0933\n",
      "Epoch [474/3000]: Train loss: 1.4639, Valid loss: 1.1224\n",
      "Epoch [475/3000]: Train loss: 1.4592, Valid loss: 1.0343\n",
      "Epoch [476/3000]: Train loss: 1.6333, Valid loss: 1.0640\n",
      "Epoch [477/3000]: Train loss: 1.5406, Valid loss: 1.0609\n",
      "Epoch [478/3000]: Train loss: 1.4677, Valid loss: 1.1484\n",
      "Epoch [479/3000]: Train loss: 1.5444, Valid loss: 1.1352\n",
      "Epoch [480/3000]: Train loss: 1.5017, Valid loss: 1.0736\n",
      "Epoch [481/3000]: Train loss: 1.5245, Valid loss: 1.2269\n",
      "Epoch [482/3000]: Train loss: 1.5612, Valid loss: 1.0722\n",
      "Epoch [483/3000]: Train loss: 1.6000, Valid loss: 1.0954\n",
      "Epoch [484/3000]: Train loss: 1.5154, Valid loss: 1.0018\n",
      "Epoch [485/3000]: Train loss: 1.4920, Valid loss: 1.0652\n",
      "Epoch [486/3000]: Train loss: 1.5729, Valid loss: 1.0594\n",
      "Epoch [487/3000]: Train loss: 1.5083, Valid loss: 1.0312\n",
      "Epoch [488/3000]: Train loss: 1.5378, Valid loss: 1.0401\n",
      "Epoch [489/3000]: Train loss: 1.5674, Valid loss: 1.0666\n",
      "Epoch [490/3000]: Train loss: 1.4862, Valid loss: 1.1855\n",
      "Epoch [491/3000]: Train loss: 1.4884, Valid loss: 1.0456\n",
      "Epoch [492/3000]: Train loss: 1.4792, Valid loss: 1.0447\n",
      "Epoch [493/3000]: Train loss: 1.4694, Valid loss: 1.0632\n",
      "Epoch [494/3000]: Train loss: 1.4970, Valid loss: 1.1068\n",
      "Epoch [495/3000]: Train loss: 1.5042, Valid loss: 1.0995\n",
      "Epoch [496/3000]: Train loss: 1.5988, Valid loss: 1.0706\n",
      "Epoch [497/3000]: Train loss: 1.5028, Valid loss: 1.0727\n",
      "Epoch [498/3000]: Train loss: 1.5111, Valid loss: 1.0504\n",
      "Epoch [499/3000]: Train loss: 1.5059, Valid loss: 1.0838\n",
      "Epoch [500/3000]: Train loss: 1.5807, Valid loss: 1.0750\n",
      "Epoch [501/3000]: Train loss: 1.5540, Valid loss: 1.1179\n",
      "Epoch [502/3000]: Train loss: 1.5186, Valid loss: 1.0373\n",
      "Epoch [503/3000]: Train loss: 1.4865, Valid loss: 1.0326\n",
      "Epoch [504/3000]: Train loss: 1.5237, Valid loss: 1.0392\n",
      "Epoch [505/3000]: Train loss: 1.5586, Valid loss: 1.0305\n",
      "Epoch [506/3000]: Train loss: 1.4928, Valid loss: 1.2725\n",
      "Epoch [507/3000]: Train loss: 1.5550, Valid loss: 1.1282\n",
      "Epoch [508/3000]: Train loss: 1.5067, Valid loss: 1.0275\n",
      "Epoch [509/3000]: Train loss: 1.5264, Valid loss: 1.0502\n",
      "Epoch [510/3000]: Train loss: 1.4981, Valid loss: 1.1111\n",
      "Epoch [511/3000]: Train loss: 1.5022, Valid loss: 1.0783\n",
      "Epoch [512/3000]: Train loss: 1.4560, Valid loss: 1.2059\n",
      "Epoch [513/3000]: Train loss: 1.5453, Valid loss: 1.0545\n",
      "Epoch [514/3000]: Train loss: 1.4535, Valid loss: 1.0423\n",
      "Epoch [515/3000]: Train loss: 1.5366, Valid loss: 1.2282\n",
      "Epoch [516/3000]: Train loss: 1.4997, Valid loss: 1.1090\n",
      "Epoch [517/3000]: Train loss: 1.5395, Valid loss: 1.0682\n",
      "Epoch [518/3000]: Train loss: 1.4980, Valid loss: 0.9762\n",
      "Epoch [519/3000]: Train loss: 1.5392, Valid loss: 1.1604\n",
      "Epoch [520/3000]: Train loss: 1.4990, Valid loss: 1.0195\n",
      "Epoch [521/3000]: Train loss: 1.5112, Valid loss: 1.2276\n",
      "Epoch [522/3000]: Train loss: 1.4961, Valid loss: 1.1602\n",
      "Epoch [523/3000]: Train loss: 1.5215, Valid loss: 1.0551\n",
      "Epoch [524/3000]: Train loss: 1.5663, Valid loss: 1.0983\n",
      "Epoch [525/3000]: Train loss: 1.5428, Valid loss: 1.2187\n",
      "Epoch [526/3000]: Train loss: 1.5277, Valid loss: 1.1028\n",
      "Epoch [527/3000]: Train loss: 1.4855, Valid loss: 1.1629\n",
      "Epoch [528/3000]: Train loss: 1.5166, Valid loss: 1.1304\n",
      "Epoch [529/3000]: Train loss: 1.4912, Valid loss: 1.1321\n",
      "Epoch [530/3000]: Train loss: 1.5801, Valid loss: 1.0802\n",
      "Epoch [531/3000]: Train loss: 1.4791, Valid loss: 1.0420\n",
      "Epoch [532/3000]: Train loss: 1.5447, Valid loss: 1.1271\n",
      "Epoch [533/3000]: Train loss: 1.5346, Valid loss: 1.0831\n",
      "Epoch [534/3000]: Train loss: 1.4761, Valid loss: 1.1192\n",
      "Epoch [535/3000]: Train loss: 1.4991, Valid loss: 1.3368\n",
      "Epoch [536/3000]: Train loss: 1.4845, Valid loss: 1.1773\n",
      "Epoch [537/3000]: Train loss: 1.4798, Valid loss: 1.1178\n",
      "Epoch [538/3000]: Train loss: 1.6265, Valid loss: 1.1578\n",
      "Epoch [539/3000]: Train loss: 1.4993, Valid loss: 1.1858\n",
      "Epoch [540/3000]: Train loss: 1.4725, Valid loss: 1.1489\n",
      "Epoch [541/3000]: Train loss: 1.5158, Valid loss: 1.0600\n",
      "Epoch [542/3000]: Train loss: 1.5030, Valid loss: 1.1275\n",
      "Epoch [543/3000]: Train loss: 1.4969, Valid loss: 1.0960\n",
      "Epoch [544/3000]: Train loss: 1.5339, Valid loss: 1.0931\n",
      "Epoch [545/3000]: Train loss: 1.5686, Valid loss: 1.1277\n",
      "Epoch [546/3000]: Train loss: 1.5506, Valid loss: 1.0505\n",
      "Epoch [547/3000]: Train loss: 1.4532, Valid loss: 0.9982\n",
      "Epoch [548/3000]: Train loss: 1.4651, Valid loss: 1.1296\n",
      "Epoch [549/3000]: Train loss: 1.6327, Valid loss: 1.1161\n",
      "Epoch [550/3000]: Train loss: 1.5349, Valid loss: 1.1066\n",
      "Epoch [551/3000]: Train loss: 1.4939, Valid loss: 1.0591\n",
      "Epoch [552/3000]: Train loss: 1.4907, Valid loss: 1.1809\n",
      "Epoch [553/3000]: Train loss: 1.5178, Valid loss: 1.0588\n",
      "Epoch [554/3000]: Train loss: 1.5193, Valid loss: 1.1412\n",
      "Epoch [555/3000]: Train loss: 1.5432, Valid loss: 1.1948\n",
      "Epoch [556/3000]: Train loss: 1.4957, Valid loss: 1.1681\n",
      "Epoch [557/3000]: Train loss: 1.5238, Valid loss: 1.1493\n",
      "Epoch [558/3000]: Train loss: 1.4893, Valid loss: 1.2860\n",
      "Epoch [559/3000]: Train loss: 1.4546, Valid loss: 1.0808\n",
      "Epoch [560/3000]: Train loss: 1.5554, Valid loss: 1.2725\n",
      "Epoch [561/3000]: Train loss: 1.4945, Valid loss: 0.9806\n",
      "Epoch [562/3000]: Train loss: 1.4351, Valid loss: 1.0733\n",
      "Epoch [563/3000]: Train loss: 1.5792, Valid loss: 1.1403\n",
      "Epoch [564/3000]: Train loss: 1.5308, Valid loss: 1.1310\n",
      "Epoch [565/3000]: Train loss: 1.5257, Valid loss: 1.0561\n",
      "Epoch [566/3000]: Train loss: 1.4748, Valid loss: 1.1684\n",
      "Epoch [567/3000]: Train loss: 1.5042, Valid loss: 1.0489\n",
      "Epoch [568/3000]: Train loss: 1.4765, Valid loss: 1.0645\n",
      "Epoch [569/3000]: Train loss: 1.5317, Valid loss: 1.0188\n",
      "Epoch [570/3000]: Train loss: 1.4736, Valid loss: 1.0807\n",
      "Epoch [571/3000]: Train loss: 1.4605, Valid loss: 1.0852\n",
      "Epoch [572/3000]: Train loss: 1.5914, Valid loss: 1.1014\n",
      "Epoch [573/3000]: Train loss: 1.5146, Valid loss: 1.1109\n",
      "Epoch [574/3000]: Train loss: 1.4688, Valid loss: 1.0726\n",
      "Epoch [575/3000]: Train loss: 1.4773, Valid loss: 1.1213\n",
      "Epoch [576/3000]: Train loss: 1.4714, Valid loss: 1.1307\n",
      "Epoch [577/3000]: Train loss: 1.4765, Valid loss: 1.0548\n",
      "Epoch [578/3000]: Train loss: 1.5193, Valid loss: 1.0570\n",
      "Epoch [579/3000]: Train loss: 1.5983, Valid loss: 1.0306\n",
      "Epoch [580/3000]: Train loss: 1.4817, Valid loss: 1.1635\n",
      "Epoch [581/3000]: Train loss: 1.4876, Valid loss: 1.0747\n",
      "Epoch [582/3000]: Train loss: 1.4992, Valid loss: 1.0859\n",
      "Epoch [583/3000]: Train loss: 1.4607, Valid loss: 1.2062\n",
      "Epoch [584/3000]: Train loss: 1.5144, Valid loss: 1.2630\n",
      "Epoch [585/3000]: Train loss: 1.5296, Valid loss: 1.2076\n",
      "Epoch [586/3000]: Train loss: 1.5791, Valid loss: 1.1789\n",
      "Epoch [587/3000]: Train loss: 1.5875, Valid loss: 1.1956\n",
      "Epoch [588/3000]: Train loss: 1.5595, Valid loss: 1.3080\n",
      "Epoch [589/3000]: Train loss: 1.5061, Valid loss: 1.0429\n",
      "Epoch [590/3000]: Train loss: 1.4587, Valid loss: 1.1254\n",
      "Epoch [591/3000]: Train loss: 1.4668, Valid loss: 1.1301\n",
      "Epoch [592/3000]: Train loss: 1.6423, Valid loss: 1.1136\n",
      "Epoch [593/3000]: Train loss: 1.5048, Valid loss: 0.9723\n",
      "Epoch [594/3000]: Train loss: 1.5422, Valid loss: 1.0947\n",
      "Epoch [595/3000]: Train loss: 1.5233, Valid loss: 0.9799\n",
      "Epoch [596/3000]: Train loss: 1.4510, Valid loss: 1.1430\n",
      "Epoch [597/3000]: Train loss: 1.5099, Valid loss: 1.0989\n",
      "Epoch [598/3000]: Train loss: 1.4419, Valid loss: 1.1193\n",
      "Epoch [599/3000]: Train loss: 1.6213, Valid loss: 1.1226\n",
      "Epoch [600/3000]: Train loss: 1.4923, Valid loss: 1.1136\n",
      "Epoch [601/3000]: Train loss: 1.4508, Valid loss: 1.1887\n",
      "Epoch [602/3000]: Train loss: 1.4931, Valid loss: 1.0087\n",
      "Epoch [603/3000]: Train loss: 1.5109, Valid loss: 1.1182\n",
      "Epoch [604/3000]: Train loss: 1.4712, Valid loss: 1.1537\n",
      "Epoch [605/3000]: Train loss: 1.4661, Valid loss: 1.1883\n",
      "Epoch [606/3000]: Train loss: 1.5207, Valid loss: 1.0636\n",
      "Epoch [607/3000]: Train loss: 1.5422, Valid loss: 1.1297\n",
      "Epoch [608/3000]: Train loss: 1.5036, Valid loss: 1.1754\n",
      "Epoch [609/3000]: Train loss: 1.5282, Valid loss: 1.1502\n",
      "Epoch [610/3000]: Train loss: 1.4326, Valid loss: 1.0562\n",
      "Epoch [611/3000]: Train loss: 1.5036, Valid loss: 1.1461\n",
      "Epoch [612/3000]: Train loss: 1.5038, Valid loss: 1.0855\n",
      "Epoch [613/3000]: Train loss: 1.5153, Valid loss: 1.0969\n",
      "Epoch [614/3000]: Train loss: 1.5180, Valid loss: 1.0866\n",
      "Epoch [615/3000]: Train loss: 1.5593, Valid loss: 1.0843\n",
      "Epoch [616/3000]: Train loss: 1.4984, Valid loss: 1.0989\n",
      "Epoch [617/3000]: Train loss: 1.5125, Valid loss: 1.1235\n",
      "Epoch [618/3000]: Train loss: 1.5099, Valid loss: 1.0486\n",
      "Epoch [619/3000]: Train loss: 1.5372, Valid loss: 0.9455\n",
      "Saving model with loss 0.946...\n",
      "Epoch [620/3000]: Train loss: 1.5667, Valid loss: 1.0493\n",
      "Epoch [621/3000]: Train loss: 1.5054, Valid loss: 1.0749\n",
      "Epoch [622/3000]: Train loss: 1.4642, Valid loss: 1.0601\n",
      "Epoch [623/3000]: Train loss: 1.4855, Valid loss: 1.1704\n",
      "Epoch [624/3000]: Train loss: 1.4610, Valid loss: 1.0815\n",
      "Epoch [625/3000]: Train loss: 1.4793, Valid loss: 1.0787\n",
      "Epoch [626/3000]: Train loss: 1.4942, Valid loss: 1.2296\n",
      "Epoch [627/3000]: Train loss: 1.4967, Valid loss: 1.2792\n",
      "Epoch [628/3000]: Train loss: 1.5188, Valid loss: 1.1922\n",
      "Epoch [629/3000]: Train loss: 1.4768, Valid loss: 0.9885\n",
      "Epoch [630/3000]: Train loss: 1.4375, Valid loss: 1.0380\n",
      "Epoch [631/3000]: Train loss: 1.4635, Valid loss: 1.1618\n",
      "Epoch [632/3000]: Train loss: 1.5060, Valid loss: 1.0634\n",
      "Epoch [633/3000]: Train loss: 1.5071, Valid loss: 1.1050\n",
      "Epoch [634/3000]: Train loss: 1.5942, Valid loss: 1.1931\n",
      "Epoch [635/3000]: Train loss: 1.5186, Valid loss: 1.0360\n",
      "Epoch [636/3000]: Train loss: 1.4814, Valid loss: 1.0597\n",
      "Epoch [637/3000]: Train loss: 1.5323, Valid loss: 1.1113\n",
      "Epoch [638/3000]: Train loss: 1.5239, Valid loss: 1.0907\n",
      "Epoch [639/3000]: Train loss: 1.5135, Valid loss: 1.1200\n",
      "Epoch [640/3000]: Train loss: 1.5429, Valid loss: 1.0413\n",
      "Epoch [641/3000]: Train loss: 1.6099, Valid loss: 1.1079\n",
      "Epoch [642/3000]: Train loss: 1.4542, Valid loss: 1.0332\n",
      "Epoch [643/3000]: Train loss: 1.5182, Valid loss: 0.9955\n",
      "Epoch [644/3000]: Train loss: 1.5031, Valid loss: 1.1432\n",
      "Epoch [645/3000]: Train loss: 1.5265, Valid loss: 1.0691\n",
      "Epoch [646/3000]: Train loss: 1.5495, Valid loss: 1.0943\n",
      "Epoch [647/3000]: Train loss: 1.4526, Valid loss: 1.0717\n",
      "Epoch [648/3000]: Train loss: 1.5127, Valid loss: 1.1709\n",
      "Epoch [649/3000]: Train loss: 1.5294, Valid loss: 1.0559\n",
      "Epoch [650/3000]: Train loss: 1.4574, Valid loss: 1.1352\n",
      "Epoch [651/3000]: Train loss: 1.4902, Valid loss: 1.1472\n",
      "Epoch [652/3000]: Train loss: 1.5159, Valid loss: 1.0350\n",
      "Epoch [653/3000]: Train loss: 1.5345, Valid loss: 0.9938\n",
      "Epoch [654/3000]: Train loss: 1.4074, Valid loss: 1.0551\n",
      "Epoch [655/3000]: Train loss: 1.3920, Valid loss: 1.0619\n",
      "Epoch [656/3000]: Train loss: 1.4598, Valid loss: 1.0756\n",
      "Epoch [657/3000]: Train loss: 1.5040, Valid loss: 1.0690\n",
      "Epoch [658/3000]: Train loss: 1.5169, Valid loss: 1.2291\n",
      "Epoch [659/3000]: Train loss: 1.5542, Valid loss: 1.1668\n",
      "Epoch [660/3000]: Train loss: 1.4798, Valid loss: 1.0507\n",
      "Epoch [661/3000]: Train loss: 1.4895, Valid loss: 1.0487\n",
      "Epoch [662/3000]: Train loss: 1.4897, Valid loss: 1.0767\n",
      "Epoch [663/3000]: Train loss: 1.5070, Valid loss: 1.0614\n",
      "Epoch [664/3000]: Train loss: 1.5658, Valid loss: 1.1400\n",
      "Epoch [665/3000]: Train loss: 1.5152, Valid loss: 1.2239\n",
      "Epoch [666/3000]: Train loss: 1.5281, Valid loss: 1.1721\n",
      "Epoch [667/3000]: Train loss: 1.5497, Valid loss: 1.0444\n",
      "Epoch [668/3000]: Train loss: 1.5078, Valid loss: 1.1448\n",
      "Epoch [669/3000]: Train loss: 1.4955, Valid loss: 1.1077\n",
      "Epoch [670/3000]: Train loss: 1.4500, Valid loss: 1.1346\n",
      "Epoch [671/3000]: Train loss: 1.4928, Valid loss: 1.0985\n",
      "Epoch [672/3000]: Train loss: 1.5329, Valid loss: 1.1542\n",
      "Epoch [673/3000]: Train loss: 1.4797, Valid loss: 1.0680\n",
      "Epoch [674/3000]: Train loss: 1.4553, Valid loss: 1.1341\n",
      "Epoch [675/3000]: Train loss: 1.4932, Valid loss: 1.1525\n",
      "Epoch [676/3000]: Train loss: 1.4352, Valid loss: 1.1016\n",
      "Epoch [677/3000]: Train loss: 1.5039, Valid loss: 1.1397\n",
      "Epoch [678/3000]: Train loss: 1.5612, Valid loss: 1.0700\n",
      "Epoch [679/3000]: Train loss: 1.5682, Valid loss: 1.0967\n",
      "Epoch [680/3000]: Train loss: 1.4985, Valid loss: 1.1504\n",
      "Epoch [681/3000]: Train loss: 1.4700, Valid loss: 1.0949\n",
      "Epoch [682/3000]: Train loss: 1.4722, Valid loss: 1.0744\n",
      "Epoch [683/3000]: Train loss: 1.5642, Valid loss: 1.1510\n",
      "Epoch [684/3000]: Train loss: 1.5561, Valid loss: 1.1350\n",
      "Epoch [685/3000]: Train loss: 1.5436, Valid loss: 1.2130\n",
      "Epoch [686/3000]: Train loss: 1.5261, Valid loss: 1.2030\n",
      "Epoch [687/3000]: Train loss: 1.4973, Valid loss: 1.1650\n",
      "Epoch [688/3000]: Train loss: 1.5056, Valid loss: 1.1588\n",
      "Epoch [689/3000]: Train loss: 1.4214, Valid loss: 1.1578\n",
      "Epoch [690/3000]: Train loss: 1.5323, Valid loss: 1.2067\n",
      "Epoch [691/3000]: Train loss: 1.5278, Valid loss: 1.1601\n",
      "Epoch [692/3000]: Train loss: 1.4722, Valid loss: 1.0767\n",
      "Epoch [693/3000]: Train loss: 1.4812, Valid loss: 1.1036\n",
      "Epoch [694/3000]: Train loss: 1.5564, Valid loss: 1.0705\n",
      "Epoch [695/3000]: Train loss: 1.4760, Valid loss: 1.0831\n",
      "Epoch [696/3000]: Train loss: 1.4770, Valid loss: 1.0243\n",
      "Epoch [697/3000]: Train loss: 1.4901, Valid loss: 1.0278\n",
      "Epoch [698/3000]: Train loss: 1.4313, Valid loss: 1.0358\n",
      "Epoch [699/3000]: Train loss: 1.4566, Valid loss: 1.0147\n",
      "Epoch [700/3000]: Train loss: 1.4235, Valid loss: 1.1624\n",
      "Epoch [701/3000]: Train loss: 1.4988, Valid loss: 0.9893\n",
      "Epoch [702/3000]: Train loss: 1.5538, Valid loss: 1.1554\n",
      "Epoch [703/3000]: Train loss: 1.5318, Valid loss: 1.1341\n",
      "Epoch [704/3000]: Train loss: 1.4657, Valid loss: 1.1044\n",
      "Epoch [705/3000]: Train loss: 1.4278, Valid loss: 1.1598\n",
      "Epoch [706/3000]: Train loss: 1.5050, Valid loss: 1.2627\n",
      "Epoch [707/3000]: Train loss: 1.5241, Valid loss: 1.0961\n",
      "Epoch [708/3000]: Train loss: 1.4612, Valid loss: 1.0031\n",
      "Epoch [709/3000]: Train loss: 1.5176, Valid loss: 1.1105\n",
      "Epoch [710/3000]: Train loss: 1.5562, Valid loss: 1.0551\n",
      "Epoch [711/3000]: Train loss: 1.4790, Valid loss: 0.9948\n",
      "Epoch [712/3000]: Train loss: 1.4838, Valid loss: 1.0699\n",
      "Epoch [713/3000]: Train loss: 1.5366, Valid loss: 1.0609\n",
      "Epoch [714/3000]: Train loss: 1.4898, Valid loss: 1.3665\n",
      "Epoch [715/3000]: Train loss: 1.4750, Valid loss: 1.0665\n",
      "Epoch [716/3000]: Train loss: 1.4337, Valid loss: 1.1628\n",
      "Epoch [717/3000]: Train loss: 1.5340, Valid loss: 1.0169\n",
      "Epoch [718/3000]: Train loss: 1.4596, Valid loss: 1.0312\n",
      "Epoch [719/3000]: Train loss: 1.5153, Valid loss: 1.0595\n",
      "Epoch [720/3000]: Train loss: 1.4188, Valid loss: 1.1381\n",
      "Epoch [721/3000]: Train loss: 1.5282, Valid loss: 1.0506\n",
      "Epoch [722/3000]: Train loss: 1.4916, Valid loss: 1.0132\n",
      "Epoch [723/3000]: Train loss: 1.4831, Valid loss: 1.2133\n",
      "Epoch [724/3000]: Train loss: 1.5073, Valid loss: 1.0547\n",
      "Epoch [725/3000]: Train loss: 1.5520, Valid loss: 1.1390\n",
      "Epoch [726/3000]: Train loss: 1.4606, Valid loss: 1.0878\n",
      "Epoch [727/3000]: Train loss: 1.4411, Valid loss: 1.1178\n",
      "Epoch [728/3000]: Train loss: 1.4522, Valid loss: 1.2413\n",
      "Epoch [729/3000]: Train loss: 1.4395, Valid loss: 1.0494\n",
      "Epoch [730/3000]: Train loss: 1.3998, Valid loss: 1.1748\n",
      "Epoch [731/3000]: Train loss: 1.4690, Valid loss: 1.2511\n",
      "Epoch [732/3000]: Train loss: 1.5179, Valid loss: 1.1378\n",
      "Epoch [733/3000]: Train loss: 1.5005, Valid loss: 1.0853\n",
      "Epoch [734/3000]: Train loss: 1.5175, Valid loss: 1.0985\n",
      "Epoch [735/3000]: Train loss: 1.4841, Valid loss: 1.1157\n",
      "Epoch [736/3000]: Train loss: 1.5163, Valid loss: 1.3475\n",
      "Epoch [737/3000]: Train loss: 1.4559, Valid loss: 1.0685\n",
      "Epoch [738/3000]: Train loss: 1.5690, Valid loss: 1.1001\n",
      "Epoch [739/3000]: Train loss: 1.4564, Valid loss: 1.0619\n",
      "Epoch [740/3000]: Train loss: 1.5357, Valid loss: 1.0576\n",
      "Epoch [741/3000]: Train loss: 1.4316, Valid loss: 1.1016\n",
      "Epoch [742/3000]: Train loss: 1.4710, Valid loss: 1.1172\n",
      "Epoch [743/3000]: Train loss: 1.5140, Valid loss: 1.0956\n",
      "Epoch [744/3000]: Train loss: 1.4737, Valid loss: 1.1000\n",
      "Epoch [745/3000]: Train loss: 1.4426, Valid loss: 1.0754\n",
      "Epoch [746/3000]: Train loss: 1.5508, Valid loss: 1.2822\n",
      "Epoch [747/3000]: Train loss: 1.4524, Valid loss: 1.1916\n",
      "Epoch [748/3000]: Train loss: 1.4894, Valid loss: 1.0040\n",
      "Epoch [749/3000]: Train loss: 1.5159, Valid loss: 1.4302\n",
      "Epoch [750/3000]: Train loss: 1.5164, Valid loss: 1.0748\n",
      "Epoch [751/3000]: Train loss: 1.4652, Valid loss: 1.0496\n",
      "Epoch [752/3000]: Train loss: 1.5097, Valid loss: 1.0796\n",
      "Epoch [753/3000]: Train loss: 1.4421, Valid loss: 1.1009\n",
      "Epoch [754/3000]: Train loss: 1.4851, Valid loss: 1.1974\n",
      "Epoch [755/3000]: Train loss: 1.5571, Valid loss: 1.1676\n",
      "Epoch [756/3000]: Train loss: 1.4828, Valid loss: 1.2067\n",
      "Epoch [757/3000]: Train loss: 1.4720, Valid loss: 1.1552\n",
      "Epoch [758/3000]: Train loss: 1.4579, Valid loss: 1.0265\n",
      "Epoch [759/3000]: Train loss: 1.4707, Valid loss: 1.1391\n",
      "Epoch [760/3000]: Train loss: 1.5117, Valid loss: 1.0262\n",
      "Epoch [761/3000]: Train loss: 1.5141, Valid loss: 1.1824\n",
      "Epoch [762/3000]: Train loss: 1.4475, Valid loss: 1.0587\n",
      "Epoch [763/3000]: Train loss: 1.5314, Valid loss: 1.0723\n",
      "Epoch [764/3000]: Train loss: 1.4409, Valid loss: 1.1947\n",
      "Epoch [765/3000]: Train loss: 1.4549, Valid loss: 1.0745\n",
      "Epoch [766/3000]: Train loss: 1.4929, Valid loss: 1.1262\n",
      "Epoch [767/3000]: Train loss: 1.4819, Valid loss: 1.0459\n",
      "Epoch [768/3000]: Train loss: 1.4924, Valid loss: 1.1939\n",
      "Epoch [769/3000]: Train loss: 1.4372, Valid loss: 1.3610\n",
      "Epoch [770/3000]: Train loss: 1.4604, Valid loss: 1.0619\n",
      "Epoch [771/3000]: Train loss: 1.4719, Valid loss: 1.0766\n",
      "Epoch [772/3000]: Train loss: 1.4864, Valid loss: 1.1114\n",
      "Epoch [773/3000]: Train loss: 1.5022, Valid loss: 1.0962\n",
      "Epoch [774/3000]: Train loss: 1.4745, Valid loss: 1.1608\n",
      "Epoch [775/3000]: Train loss: 1.5419, Valid loss: 1.1465\n",
      "Epoch [776/3000]: Train loss: 1.5423, Valid loss: 1.2237\n",
      "Epoch [777/3000]: Train loss: 1.5082, Valid loss: 1.2043\n",
      "Epoch [778/3000]: Train loss: 1.5369, Valid loss: 1.0481\n",
      "Epoch [779/3000]: Train loss: 1.5377, Valid loss: 1.0901\n",
      "Epoch [780/3000]: Train loss: 1.5243, Valid loss: 1.0920\n",
      "Epoch [781/3000]: Train loss: 1.5506, Valid loss: 1.1856\n",
      "Epoch [782/3000]: Train loss: 1.4902, Valid loss: 1.1289\n",
      "Epoch [783/3000]: Train loss: 1.5095, Valid loss: 1.1508\n",
      "Epoch [784/3000]: Train loss: 1.4223, Valid loss: 1.1113\n",
      "Epoch [785/3000]: Train loss: 1.5298, Valid loss: 1.0671\n",
      "Epoch [786/3000]: Train loss: 1.4994, Valid loss: 1.0933\n",
      "Epoch [787/3000]: Train loss: 1.4872, Valid loss: 1.2598\n",
      "Epoch [788/3000]: Train loss: 1.4498, Valid loss: 1.0897\n",
      "Epoch [789/3000]: Train loss: 1.4832, Valid loss: 1.0815\n",
      "Epoch [790/3000]: Train loss: 1.4812, Valid loss: 1.1167\n",
      "Epoch [791/3000]: Train loss: 1.4936, Valid loss: 1.0756\n",
      "Epoch [792/3000]: Train loss: 1.5066, Valid loss: 1.1642\n",
      "Epoch [793/3000]: Train loss: 1.5126, Valid loss: 1.1190\n",
      "Epoch [794/3000]: Train loss: 1.4367, Valid loss: 1.0729\n",
      "Epoch [795/3000]: Train loss: 1.4821, Valid loss: 1.0383\n",
      "Epoch [796/3000]: Train loss: 1.4437, Valid loss: 1.0227\n",
      "Epoch [797/3000]: Train loss: 1.5702, Valid loss: 1.1261\n",
      "Epoch [798/3000]: Train loss: 1.4783, Valid loss: 1.0700\n",
      "Epoch [799/3000]: Train loss: 1.4881, Valid loss: 1.0676\n",
      "Epoch [800/3000]: Train loss: 1.4530, Valid loss: 1.0139\n",
      "Epoch [801/3000]: Train loss: 1.4516, Valid loss: 1.1016\n",
      "Epoch [802/3000]: Train loss: 1.4743, Valid loss: 1.1240\n",
      "Epoch [803/3000]: Train loss: 1.4179, Valid loss: 1.0999\n",
      "Epoch [804/3000]: Train loss: 1.4251, Valid loss: 1.2856\n",
      "Epoch [805/3000]: Train loss: 1.5007, Valid loss: 1.2899\n",
      "Epoch [806/3000]: Train loss: 1.4923, Valid loss: 1.0244\n",
      "Epoch [807/3000]: Train loss: 1.5053, Valid loss: 1.0429\n",
      "Epoch [808/3000]: Train loss: 1.4688, Valid loss: 1.1051\n",
      "Epoch [809/3000]: Train loss: 1.5348, Valid loss: 1.1116\n",
      "Epoch [810/3000]: Train loss: 1.4910, Valid loss: 1.3838\n",
      "Epoch [811/3000]: Train loss: 1.4789, Valid loss: 1.0217\n",
      "Epoch [812/3000]: Train loss: 1.4728, Valid loss: 1.0786\n",
      "Epoch [813/3000]: Train loss: 1.4372, Valid loss: 1.0431\n",
      "Epoch [814/3000]: Train loss: 1.4676, Valid loss: 1.0817\n",
      "Epoch [815/3000]: Train loss: 1.4911, Valid loss: 1.0661\n",
      "Epoch [816/3000]: Train loss: 1.4688, Valid loss: 1.0469\n",
      "Epoch [817/3000]: Train loss: 1.4223, Valid loss: 1.1524\n",
      "Epoch [818/3000]: Train loss: 1.4896, Valid loss: 1.2590\n",
      "Epoch [819/3000]: Train loss: 1.4842, Valid loss: 1.0598\n",
      "Epoch [820/3000]: Train loss: 1.5223, Valid loss: 1.1639\n",
      "Epoch [821/3000]: Train loss: 1.5149, Valid loss: 1.1451\n",
      "Epoch [822/3000]: Train loss: 1.5386, Valid loss: 1.1114\n",
      "Epoch [823/3000]: Train loss: 1.4539, Valid loss: 1.0649\n",
      "Epoch [824/3000]: Train loss: 1.4578, Valid loss: 1.0802\n",
      "Epoch [825/3000]: Train loss: 1.4439, Valid loss: 1.0809\n",
      "Epoch [826/3000]: Train loss: 1.4660, Valid loss: 1.0089\n",
      "Epoch [827/3000]: Train loss: 1.4237, Valid loss: 1.1939\n",
      "Epoch [828/3000]: Train loss: 1.4806, Valid loss: 1.1090\n",
      "Epoch [829/3000]: Train loss: 1.5630, Valid loss: 1.0129\n",
      "Epoch [830/3000]: Train loss: 1.5058, Valid loss: 1.1019\n",
      "Epoch [831/3000]: Train loss: 1.5055, Valid loss: 1.1168\n",
      "Epoch [832/3000]: Train loss: 1.3909, Valid loss: 1.0465\n",
      "Epoch [833/3000]: Train loss: 1.5510, Valid loss: 1.0934\n",
      "Epoch [834/3000]: Train loss: 1.4748, Valid loss: 1.0445\n",
      "Epoch [835/3000]: Train loss: 1.5266, Valid loss: 1.1805\n",
      "Epoch [836/3000]: Train loss: 1.4562, Valid loss: 1.0977\n",
      "Epoch [837/3000]: Train loss: 1.5604, Valid loss: 1.2775\n",
      "Epoch [838/3000]: Train loss: 1.6441, Valid loss: 1.0891\n",
      "Epoch [839/3000]: Train loss: 1.4942, Valid loss: 1.0697\n",
      "Epoch [840/3000]: Train loss: 1.4595, Valid loss: 1.2447\n",
      "Epoch [841/3000]: Train loss: 1.5250, Valid loss: 1.1866\n",
      "Epoch [842/3000]: Train loss: 1.5467, Valid loss: 1.0787\n",
      "Epoch [843/3000]: Train loss: 1.4338, Valid loss: 1.1381\n",
      "Epoch [844/3000]: Train loss: 1.4493, Valid loss: 1.2091\n",
      "Epoch [845/3000]: Train loss: 1.3659, Valid loss: 1.1377\n",
      "Epoch [846/3000]: Train loss: 1.4536, Valid loss: 1.1427\n",
      "Epoch [847/3000]: Train loss: 1.4449, Valid loss: 1.0584\n",
      "Epoch [848/3000]: Train loss: 1.4711, Valid loss: 1.0737\n",
      "Epoch [849/3000]: Train loss: 1.4206, Valid loss: 1.0904\n",
      "Epoch [850/3000]: Train loss: 1.4693, Valid loss: 1.2565\n",
      "Epoch [851/3000]: Train loss: 1.4893, Valid loss: 1.1500\n",
      "Epoch [852/3000]: Train loss: 1.5156, Valid loss: 1.1688\n",
      "Epoch [853/3000]: Train loss: 1.5009, Valid loss: 1.1287\n",
      "Epoch [854/3000]: Train loss: 1.4339, Valid loss: 1.0538\n",
      "Epoch [855/3000]: Train loss: 1.4621, Valid loss: 1.0217\n",
      "Epoch [856/3000]: Train loss: 1.4124, Valid loss: 1.1049\n",
      "Epoch [857/3000]: Train loss: 1.4422, Valid loss: 1.1453\n",
      "Epoch [858/3000]: Train loss: 1.4518, Valid loss: 0.9960\n",
      "Epoch [859/3000]: Train loss: 1.5780, Valid loss: 1.0401\n",
      "Epoch [860/3000]: Train loss: 1.4208, Valid loss: 1.2308\n",
      "Epoch [861/3000]: Train loss: 1.5352, Valid loss: 1.1069\n",
      "Epoch [862/3000]: Train loss: 1.4815, Valid loss: 1.0937\n",
      "Epoch [863/3000]: Train loss: 1.4922, Valid loss: 1.0453\n",
      "Epoch [864/3000]: Train loss: 1.4827, Valid loss: 0.9756\n",
      "Epoch [865/3000]: Train loss: 1.5897, Valid loss: 1.1506\n",
      "Epoch [866/3000]: Train loss: 1.4106, Valid loss: 1.0020\n",
      "Epoch [867/3000]: Train loss: 1.4362, Valid loss: 1.1988\n",
      "Epoch [868/3000]: Train loss: 1.5553, Valid loss: 1.2025\n",
      "Epoch [869/3000]: Train loss: 1.4269, Valid loss: 1.0554\n",
      "Epoch [870/3000]: Train loss: 1.5018, Valid loss: 1.1045\n",
      "Epoch [871/3000]: Train loss: 1.4519, Valid loss: 1.1437\n",
      "Epoch [872/3000]: Train loss: 1.5112, Valid loss: 0.9630\n",
      "Epoch [873/3000]: Train loss: 1.6208, Valid loss: 1.0701\n",
      "Epoch [874/3000]: Train loss: 1.4156, Valid loss: 1.0729\n",
      "Epoch [875/3000]: Train loss: 1.4817, Valid loss: 1.2100\n",
      "Epoch [876/3000]: Train loss: 1.6143, Valid loss: 1.1786\n",
      "Epoch [877/3000]: Train loss: 1.4438, Valid loss: 1.0760\n",
      "Epoch [878/3000]: Train loss: 1.4241, Valid loss: 1.2053\n",
      "Epoch [879/3000]: Train loss: 1.4249, Valid loss: 1.1910\n",
      "Epoch [880/3000]: Train loss: 1.4189, Valid loss: 1.0914\n",
      "Epoch [881/3000]: Train loss: 1.4727, Valid loss: 1.0917\n",
      "Epoch [882/3000]: Train loss: 1.4800, Valid loss: 1.0545\n",
      "Epoch [883/3000]: Train loss: 1.4842, Valid loss: 1.1398\n",
      "Epoch [884/3000]: Train loss: 1.4810, Valid loss: 1.0940\n",
      "Epoch [885/3000]: Train loss: 1.5166, Valid loss: 1.1904\n",
      "Epoch [886/3000]: Train loss: 1.4479, Valid loss: 1.1459\n",
      "Epoch [887/3000]: Train loss: 1.4552, Valid loss: 1.0656\n",
      "Epoch [888/3000]: Train loss: 1.4904, Valid loss: 1.1151\n",
      "Epoch [889/3000]: Train loss: 1.6002, Valid loss: 1.0456\n",
      "Epoch [890/3000]: Train loss: 1.4698, Valid loss: 1.0782\n",
      "Epoch [891/3000]: Train loss: 1.5478, Valid loss: 1.0783\n",
      "Epoch [892/3000]: Train loss: 1.5007, Valid loss: 1.1057\n",
      "Epoch [893/3000]: Train loss: 1.4931, Valid loss: 1.0694\n",
      "Epoch [894/3000]: Train loss: 1.4814, Valid loss: 1.0907\n",
      "Epoch [895/3000]: Train loss: 1.4883, Valid loss: 1.0253\n",
      "Epoch [896/3000]: Train loss: 1.4508, Valid loss: 1.0975\n",
      "Epoch [897/3000]: Train loss: 1.4718, Valid loss: 1.1728\n",
      "Epoch [898/3000]: Train loss: 1.4710, Valid loss: 1.0790\n",
      "Epoch [899/3000]: Train loss: 1.5747, Valid loss: 1.1190\n",
      "Epoch [900/3000]: Train loss: 1.4777, Valid loss: 1.0915\n",
      "Epoch [901/3000]: Train loss: 1.4524, Valid loss: 1.0658\n",
      "Epoch [902/3000]: Train loss: 1.4234, Valid loss: 1.0980\n",
      "Epoch [903/3000]: Train loss: 1.5769, Valid loss: 1.0435\n",
      "Epoch [904/3000]: Train loss: 1.4733, Valid loss: 1.1558\n",
      "Epoch [905/3000]: Train loss: 1.5350, Valid loss: 1.1302\n",
      "Epoch [906/3000]: Train loss: 1.4731, Valid loss: 1.1112\n",
      "Epoch [907/3000]: Train loss: 1.4901, Valid loss: 1.0660\n",
      "Epoch [908/3000]: Train loss: 1.5515, Valid loss: 1.2247\n",
      "Epoch [909/3000]: Train loss: 1.5385, Valid loss: 1.1190\n",
      "Epoch [910/3000]: Train loss: 1.4486, Valid loss: 1.1022\n",
      "Epoch [911/3000]: Train loss: 1.4714, Valid loss: 1.0484\n",
      "Epoch [912/3000]: Train loss: 1.4295, Valid loss: 1.1917\n",
      "Epoch [913/3000]: Train loss: 1.5250, Valid loss: 1.2023\n",
      "Epoch [914/3000]: Train loss: 1.4695, Valid loss: 1.0919\n",
      "Epoch [915/3000]: Train loss: 1.4752, Valid loss: 1.0487\n",
      "Epoch [916/3000]: Train loss: 1.4930, Valid loss: 1.0552\n",
      "Epoch [917/3000]: Train loss: 1.4594, Valid loss: 0.9908\n",
      "Epoch [918/3000]: Train loss: 1.4921, Valid loss: 1.0750\n",
      "Epoch [919/3000]: Train loss: 1.4601, Valid loss: 1.1818\n",
      "Epoch [920/3000]: Train loss: 1.3927, Valid loss: 1.0220\n",
      "Epoch [921/3000]: Train loss: 1.3884, Valid loss: 1.1243\n",
      "Epoch [922/3000]: Train loss: 1.5010, Valid loss: 1.1636\n",
      "Epoch [923/3000]: Train loss: 1.5438, Valid loss: 1.1682\n",
      "Epoch [924/3000]: Train loss: 1.4551, Valid loss: 1.0964\n",
      "Epoch [925/3000]: Train loss: 1.4483, Valid loss: 1.2145\n",
      "Epoch [926/3000]: Train loss: 1.3974, Valid loss: 1.1656\n",
      "Epoch [927/3000]: Train loss: 1.4277, Valid loss: 1.0434\n",
      "Epoch [928/3000]: Train loss: 1.4986, Valid loss: 1.0431\n",
      "Epoch [929/3000]: Train loss: 1.4249, Valid loss: 1.1668\n",
      "Epoch [930/3000]: Train loss: 1.4234, Valid loss: 1.2121\n",
      "Epoch [931/3000]: Train loss: 1.4651, Valid loss: 1.2617\n",
      "Epoch [932/3000]: Train loss: 1.3946, Valid loss: 1.0852\n",
      "Epoch [933/3000]: Train loss: 1.4350, Valid loss: 1.2552\n",
      "Epoch [934/3000]: Train loss: 1.4353, Valid loss: 1.0572\n",
      "Epoch [935/3000]: Train loss: 1.4934, Valid loss: 1.1059\n",
      "Epoch [936/3000]: Train loss: 1.4542, Valid loss: 1.1035\n",
      "Epoch [937/3000]: Train loss: 1.4932, Valid loss: 1.1391\n",
      "Epoch [938/3000]: Train loss: 1.4564, Valid loss: 1.0513\n",
      "Epoch [939/3000]: Train loss: 1.5105, Valid loss: 1.1099\n",
      "Epoch [940/3000]: Train loss: 1.5001, Valid loss: 1.2259\n",
      "Epoch [941/3000]: Train loss: 1.5154, Valid loss: 1.0229\n",
      "Epoch [942/3000]: Train loss: 1.4266, Valid loss: 1.1938\n",
      "Epoch [943/3000]: Train loss: 1.5400, Valid loss: 1.0206\n",
      "Epoch [944/3000]: Train loss: 1.4159, Valid loss: 1.1038\n",
      "Epoch [945/3000]: Train loss: 1.4428, Valid loss: 1.2189\n",
      "Epoch [946/3000]: Train loss: 1.4572, Valid loss: 1.1322\n",
      "Epoch [947/3000]: Train loss: 1.4149, Valid loss: 1.1845\n",
      "Epoch [948/3000]: Train loss: 1.5052, Valid loss: 1.1552\n",
      "Epoch [949/3000]: Train loss: 1.4780, Valid loss: 1.0641\n",
      "Epoch [950/3000]: Train loss: 1.5069, Valid loss: 1.0057\n",
      "Epoch [951/3000]: Train loss: 1.5221, Valid loss: 1.0760\n",
      "Epoch [952/3000]: Train loss: 1.4908, Valid loss: 1.0622\n",
      "Epoch [953/3000]: Train loss: 1.4626, Valid loss: 1.0896\n",
      "Epoch [954/3000]: Train loss: 1.4926, Valid loss: 1.2466\n",
      "Epoch [955/3000]: Train loss: 1.5235, Valid loss: 1.1494\n",
      "Epoch [956/3000]: Train loss: 1.4766, Valid loss: 1.1063\n",
      "Epoch [957/3000]: Train loss: 1.4235, Valid loss: 1.0380\n",
      "Epoch [958/3000]: Train loss: 1.5627, Valid loss: 1.0845\n",
      "Epoch [959/3000]: Train loss: 1.4955, Valid loss: 1.0613\n",
      "Epoch [960/3000]: Train loss: 1.5010, Valid loss: 1.3422\n",
      "Epoch [961/3000]: Train loss: 1.4790, Valid loss: 1.1324\n",
      "Epoch [962/3000]: Train loss: 1.4193, Valid loss: 1.2129\n",
      "Epoch [963/3000]: Train loss: 1.5035, Valid loss: 1.1700\n",
      "Epoch [964/3000]: Train loss: 1.4734, Valid loss: 1.1140\n",
      "Epoch [965/3000]: Train loss: 1.4491, Valid loss: 1.1140\n",
      "Epoch [966/3000]: Train loss: 1.4972, Valid loss: 1.0548\n",
      "Epoch [967/3000]: Train loss: 1.4760, Valid loss: 1.1354\n",
      "Epoch [968/3000]: Train loss: 1.5168, Valid loss: 1.0263\n",
      "Epoch [969/3000]: Train loss: 1.4761, Valid loss: 1.0504\n",
      "Epoch [970/3000]: Train loss: 1.4781, Valid loss: 1.0723\n",
      "Epoch [971/3000]: Train loss: 1.4200, Valid loss: 1.1483\n",
      "Epoch [972/3000]: Train loss: 1.4734, Valid loss: 1.0326\n",
      "Epoch [973/3000]: Train loss: 1.5152, Valid loss: 1.2070\n",
      "Epoch [974/3000]: Train loss: 1.4901, Valid loss: 1.0499\n",
      "Epoch [975/3000]: Train loss: 1.5323, Valid loss: 1.1350\n",
      "Epoch [976/3000]: Train loss: 1.4028, Valid loss: 1.2575\n",
      "Epoch [977/3000]: Train loss: 1.4777, Valid loss: 1.0295\n",
      "Epoch [978/3000]: Train loss: 1.5282, Valid loss: 0.9968\n",
      "Epoch [979/3000]: Train loss: 1.4421, Valid loss: 1.0900\n",
      "Epoch [980/3000]: Train loss: 1.4342, Valid loss: 1.1861\n",
      "Epoch [981/3000]: Train loss: 1.5161, Valid loss: 1.0715\n",
      "Epoch [982/3000]: Train loss: 1.5461, Valid loss: 1.0661\n",
      "Epoch [983/3000]: Train loss: 1.4605, Valid loss: 1.1731\n",
      "Epoch [984/3000]: Train loss: 1.5054, Valid loss: 1.1074\n",
      "Epoch [985/3000]: Train loss: 1.4577, Valid loss: 1.1291\n",
      "Epoch [986/3000]: Train loss: 1.4388, Valid loss: 1.0683\n",
      "Epoch [987/3000]: Train loss: 1.4285, Valid loss: 1.0863\n",
      "Epoch [988/3000]: Train loss: 1.4690, Valid loss: 1.0810\n",
      "Epoch [989/3000]: Train loss: 1.4861, Valid loss: 1.0848\n",
      "Epoch [990/3000]: Train loss: 1.5342, Valid loss: 1.0649\n",
      "Epoch [991/3000]: Train loss: 1.4649, Valid loss: 1.0937\n",
      "Epoch [992/3000]: Train loss: 1.4954, Valid loss: 1.0011\n",
      "Epoch [993/3000]: Train loss: 1.4314, Valid loss: 1.1465\n",
      "Epoch [994/3000]: Train loss: 1.4966, Valid loss: 1.1219\n",
      "Epoch [995/3000]: Train loss: 1.5060, Valid loss: 1.0437\n",
      "Epoch [996/3000]: Train loss: 1.5373, Valid loss: 1.1837\n",
      "Epoch [997/3000]: Train loss: 1.5423, Valid loss: 1.1672\n",
      "Epoch [998/3000]: Train loss: 1.4424, Valid loss: 1.1103\n",
      "Epoch [999/3000]: Train loss: 1.5856, Valid loss: 1.4198\n",
      "Epoch [1000/3000]: Train loss: 1.4823, Valid loss: 1.1165\n",
      "Epoch [1001/3000]: Train loss: 1.4700, Valid loss: 0.9732\n",
      "Epoch [1002/3000]: Train loss: 1.5128, Valid loss: 1.2128\n",
      "Epoch [1003/3000]: Train loss: 1.4513, Valid loss: 1.0474\n",
      "Epoch [1004/3000]: Train loss: 1.4751, Valid loss: 1.1383\n",
      "Epoch [1005/3000]: Train loss: 1.4512, Valid loss: 0.9935\n",
      "Epoch [1006/3000]: Train loss: 1.4255, Valid loss: 1.0524\n",
      "Epoch [1007/3000]: Train loss: 1.4871, Valid loss: 1.0330\n",
      "Epoch [1008/3000]: Train loss: 1.5106, Valid loss: 1.1513\n",
      "Epoch [1009/3000]: Train loss: 1.5534, Valid loss: 1.0833\n",
      "Epoch [1010/3000]: Train loss: 1.4665, Valid loss: 1.0574\n",
      "Epoch [1011/3000]: Train loss: 1.4437, Valid loss: 1.0145\n",
      "Epoch [1012/3000]: Train loss: 1.4147, Valid loss: 1.1353\n",
      "Epoch [1013/3000]: Train loss: 1.5335, Valid loss: 1.1641\n",
      "Epoch [1014/3000]: Train loss: 1.4533, Valid loss: 1.0686\n",
      "Epoch [1015/3000]: Train loss: 1.4764, Valid loss: 1.0031\n",
      "Epoch [1016/3000]: Train loss: 1.3450, Valid loss: 1.0693\n",
      "Epoch [1017/3000]: Train loss: 1.4458, Valid loss: 1.1784\n",
      "Epoch [1018/3000]: Train loss: 1.5063, Valid loss: 1.1604\n",
      "Epoch [1019/3000]: Train loss: 1.4930, Valid loss: 1.0936\n",
      "\n",
      "Model is not improving, so we halt the training session.\n"
     ]
    }
   ],
   "source": [
    "model = My_Model(input_dim=x_train.shape[1]).to(device) # put your model and data on the same computation device.\n",
    "trainer(train_loader, valid_loader, model, config, device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "Launching TensorBoard..."
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir=./runs/"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 417.78it/s]\n"
     ]
    }
   ],
   "source": [
    "def save_pred(preds, file):\n",
    "    ''' Save predictions to specified file '''\n",
    "    with open(file, 'w') as fp:\n",
    "        writer = csv.writer(fp)\n",
    "        writer.writerow(['id', 'tested_positive'])\n",
    "        for i, p in enumerate(preds):\n",
    "            writer.writerow([i, p])\n",
    "\n",
    "model = My_Model(input_dim=x_train.shape[1]).to(device)\n",
    "model.load_state_dict(torch.load(config['save_path']))\n",
    "preds = predict(test_loader, model, device)\n",
    "save_pred(preds, 'pred2.csv')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-8c0dab32",
   "language": "python",
   "display_name": "PyCharm (deeplearning)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}